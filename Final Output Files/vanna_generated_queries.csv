instance_id,question,vanna_sql
bq011,"How many distinct pseudo users had positive engagement time  in the 7-day period ending on January 7, 2021 at 23:59:59, but had no positive engagement time in the 2-day period ending on the same date (January 7, 2021 at 23:59:59) ?","The provided schema does not include any explicit details or fields that directly indicate ""pseudo users"" or their ""engagement times."" Additionally, there is no table that records engagement times or user activity in a way that fits the query description. 

To accurately address this query, additional information or another table that includes user engagement times would be necessary. Without this, the SQL query cannot be generated."
bq010,"Find the top-selling product among customers who bought 'Youtube Men’s Vintage Henley' in July 2017, excluding itself.",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
bq009,"Which traffic source has the highest total transaction revenue for the year 2017, and what is the difference in millions (rounded to two decimal places) between the highest and lowest monthly total transaction revenue for that traffic source?","The provided context is insufficient to generate a SQL query for the question regarding ""traffic source"" and ""total transaction revenue for the year 2017,"" as there are no tables containing explicit fields or data that relate to traffic sources or directly indicate a connection to online traffic or marketing channels. Additionally, the concept of ""traffic source"" is not present in any of the given table structures."
bq001,"For each visitor who made at least one transaction in February 2017, how many days elapsed between the date of their first visit in February and the date of their first transaction in February, and on what type of device did they make that first transaction?","WITH FirstVisits AS (
    SELECT 
        cookie_id,
        MIN(event_time) AS first_visit_time
    FROM shopping_cart_events
    WHERE strftime('%Y-%m', event_time) = '2017-02'
    GROUP BY cookie_id
),
FirstTransactions AS (
    SELECT 
        ct.cookie_id,
        MIN(ct.txn_date) AS first_txn_date
    FROM customer_transactions ct
    JOIN shopping_cart_users scu ON ct.customer_id = scu.user_id
    WHERE strftime('%Y-%m', ct.txn_date) = '2017-02'
    GROUP BY ct.cookie_id
),
JoinedData AS (
    SELECT 
        fv.cookie_id,
        fv.first_visit_time,
        ft.first_txn_date,
        julianday(ft.first_txn_date) - julianday(fv.first_visit_time) AS days_elapsed
    FROM FirstVisits fv
    JOIN FirstTransactions ft ON fv.cookie_id = ft.cookie_id
)
SELECT 
    jd.cookie_id,
    jd.days_elapsed,
    (SELECT register_device FROM mst_users WHERE user_id = jd.cookie_id LIMIT 1) AS first_transaction_device
FROM JoinedData jd;"
bq002,"During the first half of 2017,  focusing on hits product revenue, which traffic source generated the highest total product revenue, and what were the maximum daily, weekly, and monthly product revenues (in millions) for that top-performing source over this period?","The provided context is insufficient to generate the required SQL query. There are no tables with explicit information about traffic sources or detailed temporal breakdowns linking daily, weekly, and monthly revenues directly with traffic sources. Hence, I am unable to generate a valid SQL query based on the current database schema provided."
bq003,"Between April 1 and July 31 of 2017, using the hits product revenue data along with the totals transactions to classify sessions as purchase (transactions ≥ 1 and productRevenue not null) or non-purchase (transactions null and productRevenue null), compare the average pageviews per visitor for each group by month",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
bq004,"In July 2017, among all visitors who bought any YouTube-related product, which distinct product—excluding those containing ‘YouTube’ in the product name—had the highest total quantity purchased?",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
bq008,"In January 2017, among visitors whose campaign name contains 'Data Share' and who accessed any page starting with '/home', which page did they most commonly visit next, and what is the maximum time (in seconds) they spent on the '/home' page before moving on?","The provided context is insufficient to generate the required SQL query because there is no available information or table schema directly linking web page visits to campaign names or tracking the sequence of page visits (i.e., determining the ""next"" page visited). Additionally, there is no evident method to track the time spent on a page before moving to another page based on the provided table structures."
bq269,"Between June 1, 2017, and July 31, 2017, consider only sessions that have non-null pageviews. Classify each session as ‘purchase’ if it has at least one transaction, or ‘non_purchase’ otherwise. For each month, sum each visitor’s total pageviews under each classification, then compute the average pageviews per visitor for both purchase and non-purchase groups in each month, and present the results side by side.","WITH SessionClassifications AS (
    SELECT
        web_page_visits.client_ip,
        strftime('%Y-%m', web_page_visits.visit_time) AS month,
        CASE
            WHEN EXISTS (
                SELECT 1
                FROM bookings
                WHERE strftime('%Y-%m-%d', bookings.book_date) BETWEEN strftime('%Y-%m-%d', web_page_visits.visit_time) AND strftime('%Y-%m-%d', web_page_visits.visit_time, '+1 day')
                AND bookings.total_amount > 0
            ) THEN 'purchase'
            ELSE 'non_purchase'
        END AS session_type,
        COUNT(*) AS total_pageviews
    FROM
        web_page_visits
    WHERE
        web_page_visits.visit_time BETWEEN '2017-06-01' AND '2017-07-31'
        AND web_page_visits.page_no IS NOT NULL
    GROUP BY
        web_page_visits.client_ip, month, session_type
)

SELECT
    month,
    session_type,
    AVG(total_pageviews) AS avg_pageviews_per_visitor
FROM
    SessionClassifications
GROUP BY
    month, session_type;"
bq268,"Identify the longest number of days between the first visit and the last recorded event (either the last visit or the first transaction) for a user, where the last recorded event is associated with a mobile device. The last recorded event could either be the last visit or the first transaction, and you should focus on users whose last recorded event occurred on a mobile device.",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
bq270,"What were the monthly add-to-cart and purchase conversion rates, calculated as a percentage of pageviews on product details, from January to March 2017?","The provided context is insufficient to generate the requested SQL query. There are no tables that contain information about pageviews, add-to-cart actions, or purchase conversions. Additional data sources that include these metrics are required to perform the calculations."
bq275,Which visitor IDs belong to users whose first transaction occurred on a device explicitly labeled as 'mobile' on a later date than their first visit?,The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
bq374,"Calculates the percentage of new users who, between August 1, 2016, and April 30, 2017, both stayed on the site for more than 5 minutes during their initial visit and made a purchase on a subsequent visit at any later time, relative to the total number of new users in the same period.","WITH NewUsers AS (
    SELECT user_id
    FROM mst_users
    WHERE register_date BETWEEN '2016-08-01' AND '2017-04-30'
),

InitialVisits AS (
    SELECT e.user_id
    FROM shopping_cart_events e
    JOIN shopping_cart_users u ON e.cookie_id = u.cookie_id
    WHERE u.start_date BETWEEN '2016-08-01' AND '2017-04-30'
    GROUP BY e.user_id, e.visit_id
    HAVING MAX(e.event_time) - MIN(e.event_time) > interval '5 minutes'
),

SubsequentPurchases AS (
    SELECT DISTINCT user_id
    FROM purchase_log
    WHERE stamp > (
        SELECT MAX(start_date)
        FROM shopping_cart_users
        WHERE user_id = purchase_log.user_id
    )
)

SELECT COUNT(DISTINCT CASE WHEN p.user_id IS NOT NULL THEN n.user_id END)::numeric / COUNT(n.user_id)::numeric AS ConversionRate
FROM NewUsers n
LEFT JOIN InitialVisits i ON n.user_id = i.user_id
LEFT JOIN SubsequentPurchases p ON i.user_id = p.user_id;"
sf_bq029,"Get the average number of inventors per patent and the total count of patent publications in Canada (CA) for each 5-year period from 1960 to 2020, based on publication dates. Only include patents that have at least one inventor listed, and group results by 5-year intervals (1960-1964, 1965-1969, etc.).","The provided context is insufficient to generate the requested SQL query because there is no table structure related to patents, inventors, or their relationships provided in the context. The existing tables focus on different domains such as movies, products, hardware sales, and interest metrics, none of which contain relevant information about patents or inventors."
sf_bq026,"For the assignee who has been the most active in the patent category 'A61', I'd like to know the five patent jurisdictions code where they filed the most patents during their busiest year, separated by commas.","The provided context is insufficient to generate the requested SQL query, as there are no tables or data fields related to patents, patent categories, assignees, or patent jurisdictions in the database schema provided."
sf_bq091,In which year did the assignee with the most applications in the patent category 'A61' file the most?,"The provided context does not contain any tables or fields related to patents, patent categories, assignees, or applications. Therefore, I cannot generate a SQL query based on the given tables as they do not contain the necessary data related to patents or categories like 'A61'."
sf_bq099,"For patent class A01B3, I want to analyze the information of the top 3 assignees based on the total number of applications. Please provide the following five pieces of information: the name of this assignee,  total number of applications, the year with the most applications, the number of applications in that year, and the country code with the most applications during that year.","The provided context is insufficient to generate the SQL query because there is no table related to patents, assignees, or patent applications in the provided schema. Additionally, there is no table with fields that could relate to ""patent class,"" ""assignee name,"" or details about ""applications"" such as quantity or year. Please provide the appropriate table and field details to proceed with the query generation."
sf_bq033,"How many U.S. publications related to IoT (where the abstract includes the phrase 'internet of things') were filed each month from 2008 to 2022, including months with no filings?","The provided context is insufficient to generate a SQL query for querying U.S. publications related to IoT including the phrase 'internet of things' in their abstracts from 2008 to 2022. There is no table in the given schema that contains information about publications, their abstracts, or specifics about their filing dates and locations. To proceed, a relevant dataset containing publication records would be required, which should include details like publication abstracts, filing dates, and geographical information indicating the publication is from the U.S."
sf_bq209,"Can you calculate the number of utility patents that were granted in 2010 and have exactly one forward citation within a 10-year window following their application/filing date? For this analysis, forward citations should be counted as distinct citing application numbers that cited the patent within 10 years after the patent's own filing date.","The provided context and table structures do not include any data or tables related to patents, their filing dates, grant dates, or citations. Therefore, it's impossible to generate a SQL query to answer the question about utility patents granted in 2010 with forward citations. Please provide the relevant table structures or data about patents to proceed with generating the necessary SQL query."
sf_bq027,"For patents granted between 2010 and 2018, provide the publication number of each patent and the number of backward citations it has received in the SEA category.","The provided context does not include any tables or data related to patents, publication numbers, or citation categories such as SEA. Thus, it is not possible to generate a SQL query to answer the question based on the current database schema."
sf_bq210,How many US B2 patents granted between 2008 and 2018 contain claims that do not include the word 'claim'?,"The provided context does not include any data related to patents, their types (such as US B2), or specific details about patent claims and their textual content. Therefore, I'm unable to generate a SQL query based on the current database schema provided as it does not contain the necessary information or tables related to patents."
sf_bq211,"Among patents granted between 2010 and 2023 in CN, how many of them belong to families that have a total of over one distinct applications?","The provided context does not include any tables or data related to patents, their families, or the number of applications associated with each patent family. Therefore, it is not possible to generate a SQL query based on the given information."
sf_bq213,What is the most common 4-digit IPC code among US B2 utility patents granted from June to August in 2022?,"The provided context is insufficient to generate the requested SQL query because there is no table or information related to patents, IPC codes, or specific dates of patent grants. Additionally, there is no data specifically mentioning US B2 utility patents in the provided tables. To answer the question, a table containing patent information with fields for IPC codes, grant dates, and patent types would be necessary."
sf_bq212,"For United States utility patents under the B2 classification granted between June and September of 2022, identify the most frequent 4-digit IPC code for each patent. Then, list the publication numbers and IPC4 codes of patents where this code appears 10 or more times.","The provided context is insufficient to generate the SQL query because there are no tables or data schemas related to patents, IPC codes, or any specific details about patents' classifications and dates in the given database structure. To proceed, information and database schema relevant to patents, such as tables containing patent classifications, IPC codes, grant dates, and other patent-related details would be required."
sf_bq214,"For United States utility patents under the B2 classification granted between 2010 and 2014, find the one with the most forward citations within a month of its filing date, and identify the most similar patent from the same filing year, regardless of its type.","The provided context is insufficient to generate the SQL query because there is no table or data schema related to patents, patent classifications, citation counts, or filing dates. Further information about the relevant database schema or additional data sources would be necessary to proceed with this request."
sf_bq216,Identify the top five patents filed in the same year as `US-9741766-B2` that are most similar to it based on technological similarities. Please provide the publication numbers.,"The provided context is insufficient to generate the SQL query because there is no table or data related to patents, their publication numbers, or technological similarities mentioned in the schema provided. To execute this query, a database containing information about patents, their filing years, and a method to evaluate technological similarities would be necessary."
sf_bq247,"From the publications dataset, first identify the top six families with the most publications whose family_id is not '-1'. Then, using the abs_and_emb table (joined on publication_number), provide each of those families’ IDs alongside every non-empty abstract associated with their publications.","The provided context is insufficient to generate the required SQL query as there are no details about the ""publications"" dataset, the structure of a table named ""abs_and_emb"" or the specific columns to be used for joining and filtering. Additionally, the schema for the table that contains ""family_id"" is not provided, making it impossible to identify the top six families or retrieve the corresponding abstracts."
sf_bq127,"For each publication family whose earliest publication was first published in January 2015, please provide the earliest publication date, the distinct publication numbers, their country codes, the distinct CPC and IPC codes, distinct families (namely, the ids) that cite and are cited by this publication family. Please present all lists as comma-separated values, sorted alphabetically","The provided context is insufficient to generate the requested SQL query because there are no tables in the context that contain information about publication families, publication dates, publication numbers, country codes, CPC and IPC codes, or citation relationships between publications. A database schema that includes details about publications, their classification codes, and citation data is necessary to fulfill this request."
sf_bq215,Which US patent (with a B2 kind code and a grant date between 2015 and 2018) has the highest originality score calculated as 1 - (the sum of squared occurrences of distinct 4-digit IPC codes in its backward citations divided by the square of the total occurrences of these 4-digit IPC codes)?,"The provided context is insufficient to generate the requested SQL query because there is no database schema related to patents, IPC codes, or citation data provided in the context. I cannot access or query data about US patents, their kind codes, grant dates, or information about backward citations and their IPC codes from the given database schemas."
sf_bq222,"Find the CPC technology areas in Germany that had the highest exponential moving average (smoothing factor 0.1) of patent filings per year, specifically for patents granted in December 2016. For each CPC group at level 4, show the full title, CPC group, and the year with the highest exponential moving average of patent filings.","The provided context is insufficient to generate the requested SQL query. There are no tables or data fields related to CPC technology areas, patent filings, or any specific data concerning patents, such as titles or CPC groups. Additional data sources containing these details are necessary to fulfill the query requirements."
sf_bq221,"Identify the CPC technology areas with the highest exponential moving average of patent filings each year (with a smoothing factor of 0.2), considering only the first CPC code for each patent that has a valid filing date and a non-empty application number, and report the full CPC title along with the best year associated with the highest exponential moving average for each CPC group at level 5.","The context provided does not include any information about a table containing patent filings, CPC codes, filing dates, or CPC titles. Thus, I am unable to generate the required SQL query as the database schema given in the prompt lacks the necessary information to fulfill the requirements of the question asked."
sf_bq223,"Which assignees, excluding DENSO CORP itself, have cited patents assigned to DENSO CORP, and what are the titles of the primary CPC subclasses associated with these citations? Provide the name of each citing assignee (excluding DENSO CORP), the full title of the primary CPC subclass (based on the first CPC code), and the count of citations grouped by the citing assignee and the CPC subclass title. Ensure that only citations of patents with valid filing dates are considered, and focus on the first CPC code for each citing patent. The results should specifically exclude DENSO CORP as a citing assignee.","The provided tables and context do not include any information about patents, assignees, CPC codes, or related details necessary to generate the requested SQL query. Therefore, it's not possible to create the SQL query based on the current database schema provided."
sf_bq420,"Can you identify the top 5 patents that were initially rejected under section 101 with no allowed claims, based on the length of their granted claims? The patents should have been granted in the US between 2010 and 2023. Additionally, ensure to select the first office action date for each application. Please include their first publication numbers, along with their first publication dates, length of the filed claims and grant dates.","The provided context is insufficient to generate the requested SQL query. The database schema given does not contain any tables or columns related to patents, their claims, rejection under section 101, or any related patent statuses such as first office action dates, publication numbers, and publication dates. Therefore, it is not possible to construct a query based on the given database schema to answer the question about patents."
sf_bq207,"Could you provide the earliest publication numbers, corresponding application numbers, claim numbers, and word counts for the top 100 independent patent claims, based on the highest word count, retrieved from claims stats within uspto_oce_claims (filtered by ind_flg='1'), matched with their publication numbers from uspto_oce_claims match, and further joined with patents publications to ensure only the earliest publication for each application is included, ordered by descending word count, and limited to the top 100 results?","The context provided is insufficient to generate the SQL query because there are no tables or schema details related to ""uspto_oce_claims"", ""uspto_oce_claims match"", or ""patents publications"" mentioned in the tables provided in the prompt. The required tables and fields to connect patent claims with publications and their application data are not defined in the input context. Therefore, it's not possible to create an accurate SQL query without additional information about the database schema concerning USPTO data."
sf_bq128,"Retrieve the following information for U.S. patents filed between January 1, 2014, and February 1, 2014. The patent title and abstract. The publication date of the patent. The number of backward citations for each patent (i.e., the number of patents cited by the current patent before its filing date). The number of forward citations for each patent within the first 5 years of its publication (i.e., the number of patents that cited the current patent within 5 years after its publication). For each patent, ensure the forward citations are counted only for citations within 5 years after the publication date, and backward citations are counted for citations before the filing date.","The provided context is insufficient to generate the requested SQL query. There is no table in the provided schema that contains information on patents, patent titles, abstracts, publication dates, or patent citations (either backward or forward). A suitable database schema including tables related to patents and their citations is required to fulfill this request."
sf_bq246,"Retrieve U.S. patents with the number of forward citations within the first 3 years after the patent application date (i.e., patents citing the current patent within 3 years). Only include patents with both backward citations within 1 year before the application date and forward citations within 1 year after the application date. The query should focus on specific CPC categories, sort results by backward citations in descending order, and return the patent with the most backward citations, limiting to one result.","The provided context is insufficient to generate the requested SQL query because there are no tables related to patents, their citations, application dates, or CPC categories in the provided schema. To create the requested query, access to a database containing patent information and their citation details would be necessary."
sf_bq052,"Retrieve the following information for U.S. patents: The patent ID, title, and application date. The number of backward citations within 1 month before the application date (i.e., patents that cited the current patent before its application). The number of forward citations within 1 month after the application date (i.e., patents that cited the current patent after its application). The abstract text of the patent. Only include patents that belong to specific CPC categories, such as subsection 'C05' or group 'A01G'. The query should filter patents to include only those that have at least one backward citation or one forward citation in the 1-month period specified. Sort the results by application date and return all matching records.","The provided context does not contain any tables or schema related to patents, patent citations, or classification of patents such as CPC categories. Therefore, I cannot generate a SQL query based on the current database schema provided. Please provide the relevant table structure or additional context about the patent data to proceed with generating an appropriate SQL query."
sf_bq036,What was the average number of GitHub commits made per month in 2016 for repositories containing Python code?,"The provided context is insufficient to generate the SQL query as there is no table containing information about GitHub commits, repositories, programming languages, or any related data for the year 2016 or otherwise."
sf_bq100,"How can we identify the top 10 most frequently used packages in GitHub repository contents by looking for import statements enclosed in parentheses, splitting any multi-line imports by newlines, extracting package names that appear within double quotes, counting how often these packages appear, ignoring any null results, and finally ordering them in descending order of their frequency? The final answer should remove the quotation marks.","The provided context is insufficient to generate a SQL query as it does not include any table that contains data on GitHub repository contents or any related information about programming code, import statements, or any text analysis of such content. SQL operations are typically performed on structured data stored in databases, and the operations described (extracting package names from import statements in code) would require text processing that is not typical or efficient in SQL without specific extensions or pre-processed data.

For analyzing GitHub repository content, one would typically use other tools designed for processing and analyzing text data or code, such as Python with libraries like PyDriller, or specialized software for code analysis. If this data were to be analyzed using SQL, the raw code data would first need to be extracted and pre-processed into a structured format suitable for SQL operations. This pre-processing could involve extracting import statements and storing them in a SQL-accessible database, which is not outlined in the current database schema provided."
sf_bq101,"From GitHub Repos contents, how can we identify the top 10 most frequently imported package names in Java source files by splitting each file's content into lines, filtering for valid import statements, extracting only the package portion using a suitable regex, grouping by these extracted package names, counting their occurrences, and finally returning the 10 packages that appear most often in descending order of frequency?","The provided context is insufficient to generate a SQL query for the requested operation since there is no table or dataset information related to GitHub Repos or Java source files content. The request involves processing file contents with operations such as regex matching and text splitting, which are typically not handled directly by SQL databases without specific extensions or custom functions. This task is better suited for programming environments that can handle text processing and file I/O, such as Python with libraries like Pandas for data manipulation."
sf_bq182,"Which primary programming languages, determined by the highest number of bytes in each repository, had at least 100 PullRequestEvents on January 18, 2023 across all their repositories?","The provided context and table schemas do not contain any information related to repositories, programming languages, or PullRequestEvents. Thus, it's impossible to generate a SQL query to answer the question based on the available tables."
sf_bq217,"On January 18, 2023, how many pull request creation events occurred in GitHub repositories that include JavaScript as one of their programming languages? Use data from the githubarchive  table for the events and the languages table for repository language information.",The provided context is insufficient to generate the SQL query because there are no tables named `githubarchive` or a table that contains repository language information such as one that would include JavaScript. The tables provided do not contain data related to GitHub repositories or programming languages associated with repositories.
sf_bq191,"From the 2017 GitHub WatchEvent data, find the top two repositories that have more than 300 distinct watchers, ensuring the results are joined with the 'sample_files' table so that we return each repository's name along with its distinct watcher count, and limit the output to the two repositories with the highest watcher counts.","The provided tables and schema information does not include any details or references to GitHub WatchEvent data or a 'sample_files' table. Therefore, I cannot generate a SQL query based on the given context."
sf_bq224,"Which repository with an approved license in `licenses.md` had the highest combined total of forks, issues, and watches in April 2022?","The provided context is insufficient to generate a SQL query as there are no tables related to repositories, licenses, forks, issues, and watches available in the table definitions. Therefore, I'm unable to generate the SQL query you're asking for."
sf_bq192,"Find the most active Python repository on GitHub based on watcher count, issues, and forks. The query should select repositories with specific open-source licenses (`artistic-2.0`, `isc`, `mit`, `apache-2.0`), count distinct watchers, issue events, and forks for each repository in April 2022, and include only those with `.py` files on the `master` branch. Join the license data with watch counts, issue events, and fork counts, then sort by a combined metric of forks, issues, and watches, returning the name and count of the most active repository.","The provided context is insufficient to generate the requested SQL query because there are no tables or database schema details related to GitHub repositories, licenses, watchers, issue events, forks, or file types in the provided database schema information. To perform the analysis described, information about GitHub repositories and related metrics would be necessary in the database schema."
sf_bq225,"From the GitHub repository files in 'github_repos.sample_files' joined with 'github_repos.sample_contents', which 10 programming languages occur most frequently (based on recognized file extensions) in files that have non-empty content, ordered by their file counts in descending order?",The context provided does not include any tables named 'github_repos.sample_files' or 'github_repos.sample_contents'. Please provide the schema for these tables or correct table names to proceed with generating the SQL query.
sf_bq180,"Get the top 5 most frequently used module names from Python (`.py`) and R (`.r`) scripts, counting occurrences of modules in `import` and `from` statements for Python, and `library()` calls for R. The query should consider only Python and R files, group by module name, and return the top 5 modules ordered by frequency.","The current database schema provided does not contain any tables or fields that store information about script files, programming languages, or details of code such as modules used in Python or R scripts. Due to this lack of relevant data in the tables, it is not possible to generate a SQL query to retrieve the most frequently used module names from Python and R scripts."
sf_bq233,"Can you analyze the joined data from github repos files and github_repos contents, focusing only on files ending with '.py' or '.r', then extract Python modules from 'import' or 'from ... import' lines and R libraries from 'library(...)' lines, count their occurrences, and finally list the results sorted by language and by the number of occurrences in descending order?","The provided context is insufficient to generate the requested SQL query because there are no tables or columns described in the context that relate to GitHub repositories, their files, or the contents of these files. The necessary tables for storing information about GitHub repositories and their file contents are not defined in the provided schema. To proceed with such an analysis, tables containing repository file paths, file extensions, and file contents would be required."
sf_bq248,"Among all repositories that do not use any programming language whose name (case-insensitively) includes the substring ""python,"" what is the proportion of files whose paths include ""readme.md"" and whose contents contain the phrase ""Copyright (c)""?","The provided context is insufficient to generate a SQL query because there is no information or table schema related to repositories, programming languages, file paths, or file contents. Thus, it's not possible to determine the necessary relationships or the relevant data points needed to construct the query."
sf_bq193,"Retrieve all non-empty, non-commented lines from `README.md` files in GitHub repositories, excluding lines that are comments (either starting with `#` for Markdown or `//` for code comments). For each line, calculate how often each unique line appears across all repositories and return a comma-separated list of the programming languages used in each repository containing that line, sorted alphabetically, with the results ordered by the frequency of occurrence in descending order.","The provided context is insufficient to generate the requested SQL query because there are no tables or datasets described that contain information about GitHub repositories, `README.md` files, their contents, or associated programming languages. Additional data sources would be needed to fulfill this request."
sf_bq295,"Using the 2017 GitHub Archive data for watch events, which three repositories that include at least one Python file (with a .py extension) smaller than 15,000 bytes and containing the substring ""def "" in its content have the highest total number of watch events for that year?","The provided context is insufficient to generate the SQL query because there are no tables or schema details related to GitHub Archive data, repositories, file contents, or watch events in the given schema. Additional information or access to the relevant database structure relating to GitHub data is required to create the requested query."
sf_bq249,"Please provide a report on the number of occurrences of specific line types across files from the GitHub repository. Categorize a line as 'trailing' if it ends with a blank character, as 'Space' if it starts with a space, and as 'Other' if it meets neither condition. The report should include the total number of occurrences for each category, considering all lines across all files.","The provided context is insufficient to generate the requested SQL query because there is no table or data structure in the provided schema that relates to GitHub repositories, files within a repository, or content of files (such as lines of text). To process and classify lines from files in a GitHub repository using SQL, the data must first be extracted and stored in a database table that includes information about each line. This table is not present in the schema provided."
sf_bq375,"Determine which file type among Python (.py), C (.c), Jupyter Notebook (.ipynb), Java (.java), and JavaScript (.js) in the GitHub codebase has the most files with a directory depth greater than 10, and provide the file count.","The provided context is insufficient to generate a SQL query as there is no table or database schema provided that includes information about file types, directory depth, or a GitHub codebase structure. To generate such a query, details about the relevant tables storing file information, including file extension and directory depth, would be necessary."
sf_bq255,"How many commit messages are there in repositories that use the 'Shell' programming language and 'apache-2.0' license, where the length of the commit message is more than 5 characters but less than 10,000 characters, and the messages do not start with the word 'merge', 'update' or 'test'?","The provided context is insufficient to generate the SQL query as there are no tables or columns described that relate to repositories, programming languages, licenses, or commit messages. Further database schema details regarding these aspects are required to construct an appropriate SQL query."
sf_bq194,"Among all Python (*.py), R (*.r, *.R, *.Rmd, *.rmd), and IPython notebook (*.ipynb) files in the GitHub sample dataset, which library or module is identified as the second most frequently imported or loaded based on the extracted import statements?","The provided tables do not contain information about GitHub repositories, file types, or import statements from code files. Therefore, it is not possible to generate a SQL query to answer the question based on the current dataset."
sf_bq377,Extract and count the frequency of all package names listed in the require section of JSON-formatted content,The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
sf_bq359,List the repository names and commit counts for the top two GitHub repositories with JavaScript as the primary language and the highest number of commits.,"The provided context does not contain any tables or data related to GitHub repositories, programming languages, or commit counts. Therefore, I cannot generate a SQL query based on the information you're asking for. Please provide the relevant table structure or additional context if you have any."
sf_bq252,"Could you please find the name of the repository that contains the most copied non-binary Swift file in the dataset, ensuring each file is uniquely identified by its ID?","The provided context is insufficient to generate the required SQL query as there is no table or column information related to repositories, Swift files, or a method to identify non-binary files and their copy frequency."
sf_bq251,"I want to know the GitHub project URLs for the top 3 most downloaded PyPI packages based on download count. First, extract PyPI package metadata including name, version, and project URLs. Filter these URLs to only include those that link to GitHub repositories. Use a regular expression to clean the GitHub URLs by removing unnecessary parts like 'issues', 'pull', 'blob', and 'tree' paths, keeping only the main repository URL. For packages with multiple versions, use only the most recent version based on upload time. Join this data with download metrics to determine the most downloaded packages. Return only the cleaned GitHub repository URLs (without quotation marks) for the top 3 packages by total download count, ensuring that only packages with valid GitHub URLs are included in the results.","The context provided is insufficient because the tables and database schema required to answer your query about PyPI packages, their download counts, project URLs, and metadata like upload times are not present in the given table structures. The existing tables do not contain any information related to software packages, their download metrics, or URLs linking to repositories such as GitHub. Therefore, I'm unable to generate a valid SQL query based on the current database schema provided."
bq019,"In the 2014 CMS Medicare inpatient charges data, which DRG definition has the highest total number of discharges, and among the top three cities with the most discharges for that DRG definition, what are their respective weighted average total payments (weighted by total discharges)","The provided context is insufficient to generate the SQL query as there is no table or data related to CMS Medicare, inpatient charges, DRG definitions, discharges, or total payments in the schema descriptions provided."
bq234,What is the most prescribed medication in each state in 2014?,"The provided context is insufficient to generate a SQL query for the question about the most prescribed medication in each state in 2014. The tables described do not contain any data related to medications or prescriptions. Therefore, I cannot provide a SQL query without the relevant data or table schema."
bq235,Can you tell me which healthcare provider incurs the highest combined average costs for both outpatient and inpatient services in 2014?,"The provided context is insufficient to generate a SQL query for your question because there are no tables related to healthcare providers, inpatient services, outpatient services, or any data from the year 2014. Please provide the relevant table structures or data sources that contain the necessary information to address this query."
bq172,"For the drug with the highest total number of prescriptions in New York State during 2014, could you list the top five states with the highest total claim counts for this drug? Please also include their total claim counts and total drug costs. ","The provided context is insufficient to generate the required SQL query. There is no information or table structure related to prescriptions, drug costs, or claim counts available in the context provided. Further details about the relevant dataset or tables containing this specific information would be necessary to proceed."
bq177,"For the provider whose total inpatient Medicare cost from 2011 through 2015 is the highest (computed as the sum of average_medicare_payments multiplied by total_discharges), please list that provider’s yearly average inpatient cost and yearly average outpatient cost for each calendar year in this period, where the inpatient cost is calculated as the average of (average_medicare_payments × total_discharges) and the outpatient cost is calculated as the average of (average_total_payments × outpatient_services).","The provided context is insufficient to generate the SQL query because there are no tables described in the provided schema that contain information related to Medicare, inpatient costs, outpatient services, or related healthcare data such as ""average_medicare_payments"", ""total_discharges"", ""average_total_payments"", or ""outpatient_services"". These specific healthcare-related details are essential to answer the question, and without them, the query cannot be formulated."
bq354,"Could you provide the percentage of participants for standard acne, atopic dermatitis, psoriasis, and vitiligo as defined by the International Classification of Diseases 10-CM (ICD-10-CM), including their subcategories? Please include all related concepts mapped to the standard ICD-10-CM codes (L70 for acne, L20 for atopic dermatitis, L40 for psoriasis, and L80 for vitiligo) by utilizing concept relationships, including descendant concepts. The percentage should be calculated based on the total number of participants, considering only the standard concepts and their related descendants.","The provided context is insufficient to generate the requested SQL query because there is no information or data structure related to medical conditions, ICD-10-CM codes, concept relationships, or participant data in the provided table schemas. The existing tables do not contain information on medical diagnoses, participant counts, or any mappings to standard medical coding systems such as ICD-10-CM. Therefore, I am unable to generate the SQL query without the necessary data structures and relationships being defined."
bq355,Please tell me the percentage of participants not using quinapril and related medications(Quinapril RxCUI: 35208).,"The provided context is insufficient to generate the SQL query as there is no information or table structure related to medication usage, participant details, or any specific identifier like ""Quinapril RxCUI: 35208"" in the existing database schema."
bq032,Can you provide the latitude of the final coordinates for the hurricane that traveled the second longest distance in the North Atlantic during 2020?,"The provided context is insufficient to generate a SQL query for the requested information, as there is no table containing data about hurricanes, their paths, distances traveled, or related geographical and temporal details."
bq119,"Please show information about the hurricane with the third longest total travel distance in the North Atlantic during 2020, including its travel coordinates, the cumulative travel distance (in kilometers) at each point, and the maximum sustained wind speed at those times.","The provided context is insufficient to generate the requested SQL query because there are no tables containing information about hurricanes, their travel coordinates, travel distances, or wind speeds. Additionally, there is no data specifically related to weather events or geographic tracking data that could be used to infer or calculate the requested information about hurricanes."
sf_bq117,"What is the total number of severe storm events that occurred in the most affected month over the past 15 years according to NOAA records, considering only the top 100 storm events with the highest property damage?","The provided context is insufficient to generate the SQL query because there is no table or column provided that directly refers to NOAA records, storm events, or property damage associated with such events. To answer this query, a table containing details about storm events, including occurrence dates, severity, and associated damages, would be necessary. Please provide the relevant table structure or data source that contains this information."
bq419,"Which 5 states had the most storm events from 1980 to 1995, considering only the top 1000 states with the highest event counts each year? Please use state abbreviations.","The provided tables do not contain information regarding storm events or related data. Without a table that captures information about storm events by state and year, it is impossible to generate a SQL query to answer the question regarding storm event counts from 1980 to 1995. Please provide the relevant table or dataset that includes storm event details to proceed with generating the required SQL query."
sf_bq071,"Can you provide the count of hurricanes and a list of hurricane names (sorted alphabetically and separated by commas) for each city and its associated zip code, where the hurricanes fall within the boundaries of the zip codes? Please exclude any unnamed hurricanes, and sort the results by the count of hurricanes in descending order. The output should include the following columns: city, zip code, state, count of hurricanes, and the list of hurricanes.","The provided context is insufficient to generate the requested SQL query because there is no table containing data about hurricanes, their names, and their geographical locations or boundaries that match with zip codes, cities, or states. Thus, I cannot create a precise SQL query to fetch the count of hurricanes, their names, or associate them with specific cities and zip codes without the relevant hurricane data in the database schema."
sf_bq236,What are the top 5 zip codes of the areas in the United States that have experienced the most hail storm events in the past 10 years? Don't use data from hail reports table.,The provided context is insufficient to generate the requested SQL query because there is no table containing information or historical data about hail storm events in the provided database schema.
bq356,"Among all NOAA GSOD weather stations that recorded valid daily temperature data (non-missing temp, max, min) in 2019 and whose period of record began on or before January 1, 2000, and continued through at least June 30, 2019, how many of these stations achieved 90% or more of the maximum possible number of valid temperature-record days in 2019?","The context provided does not contain any information regarding NOAA GSOD weather stations or related weather data such as daily temperatures, periods of record, or specific criteria for valid temperature-record days. Therefore, I cannot generate the required SQL query based on the existing database schema provided. Further information or access to the relevant database tables containing this weather data is necessary to proceed with formulating an appropriate SQL query."
bq042,"Can you help me retrieve the average temperature, average wind speed, and precipitation for LaGuardia Airport in NYC on June 12 for each year from 2011 through 2020, specifically using the station ID 725030?","The provided tables do not contain data related to weather conditions such as temperature, wind speed, or precipitation. Therefore, I am unable to generate a SQL query to retrieve the requested weather information for LaGuardia Airport or any other location. Please ensure that the database includes the relevant weather data tables or provide access to a suitable data source for generating the requested query."
bq394,"What are the top 3 months between 2010 and 2014 with the smallest sum of absolute differences between the average air temperature, wet bulb temperature, dew point temperature, and sea surface temperature, including respective years and sum of differences? Please present the year and month in numerical format.","The provided context is insufficient to generate a SQL query for the question regarding temperatures as there are no tables containing information related to air temperature, wet bulb temperature, dew point temperature, or sea surface temperature."
bq357,"What are the latitude and longitude coordinates and dates between 2005 and 2015 with the top 5 highest daily average wind speeds, excluding records with missing wind speed values? Using data from tables start with prefix ""icoads_core"".","The context provided is insufficient to generate a SQL query because there are no tables starting with the prefix ""icoads_core"" described in the given schema. Please provide the relevant table schema or specify another existing table to query from."
bq181,"What percentage of weather stations recorded valid temperature data (with no missing or invalid values) for at least 90% of the days in 2022, where the temperature, maximum, and minimum values are neither NULL nor equal to 9999.9, and the station has a valid identifier (USAF code not equal to '999999'), out of all available stations in the NOAA GSOD database?","The provided context is insufficient to generate the SQL query because there is no table or schema information related to weather stations, temperature data, or any related attributes such as ""USAF code"" provided in the table structures available. This specific query requires access to a table or dataset that includes daily temperature records and station identifiers which is not present in the provided schema."
bq045,Which weather stations in Washington State recorded more than 150 rainy days in 2023 but fewer rainy days compared to 2022? Defining a “rainy day” as one having precipitation greater than zero millimeters and not equal to 99.99. Only include stations with valid precipitation data.,"The provided context is insufficient to generate the SQL query because there is no table with data about weather stations, precipitation measurements, or specific daily weather records for the years 2022 and 2023."
sf_bq358,"Can you tell me which bike trip in New York City on July 15, 2015, started and ended in ZIP Code areas with the highest average temperature for that day, as recorded by the Central Park weather station (WBAN '94728')? If there's more than one trip that meets these criteria, I'd like to know about the one that starts in the smallest ZIP Code and ends in the largest ZIP Code. Please return the starting and ending ZIP Codes of this trip.","The provided context is insufficient to generate the requested SQL query because there are no tables containing data on bike trips, daily temperature records, or specific weather station data (such as WBAN numbers or Central Park weather observations). To proceed, a dataset linking bike trips with date, start and end ZIP Codes, and another linking ZIP Codes with temperature records from the specified weather station (WBAN '94728') on July 15, 2015, would be necessary."
bq290,"Can you calculate the difference in maximum temperature, minimum temperature, and average temperature between US and UK weather stations for each day in October 2023, using the date field, and excluding records with missing or invalid temperature values?","The provided context is insufficient to generate the SQL query as there is no table containing information about weather stations, temperatures, or related data for either the US or the UK. Therefore, I cannot calculate the temperature differences without access to the appropriate weather data in the database schema."
bq031,"Provide the daily weather data for Rochester from January 1 to March 31, 2019, including temperature (in Celsius), precipitation (in centimeters), and wind speed (in meters per second). For each variable, calculate the 8-day moving average (including the current day and the previous 7 days). Also, calculate the difference between the moving average on each day and the moving averages for the previous 1 to 8 days (i.e., lag1 to lag8). The result should include: The daily values for temperature, precipitation, and wind speed.The 8-day moving averages for each variable. The differences between the moving averages for each of the previous 1 to 8 days (e.g., the difference between today's moving average and the moving average from 1 day ago, from 2 days ago, and so on). Round all values to one decimal place. The data should be ordered by date, starting from January 9, 2019.","The provided context is insufficient to generate the requested SQL query as there is no table containing weather data such as temperature, precipitation, or wind speed for any location, including Rochester. The necessary data to perform the analysis and calculations as described is not available in the provided database schema."
bq392,"What are the top 3 dates in October 2009 with the highest average temperature for station number 723758, in the format YYYY-MM-DD?","The provided context is insufficient to generate a SQL query for finding the top 3 dates with the highest average temperature for a specific station in October 2009, as there is no table or column information related to weather, temperature, or stations provided in the database schema."
sf_bq050,"I want to analyze bike trips in New York City for 2014 by linking trip data with weather information to understand how weather conditions (temperature, wind speed, and precipitation) affect bike trips between neighborhoods. For each combination of starting and ending neighborhoods, I need the following: 1. Total number of bike trips between the neighborhoods. 2. Average trip duration in minutes (rounded to 1 decimal). 3. Average temperature at the start of the trip (rounded to 1 decimal). 4. Average wind speed at the start (in meters per second, rounded to 1 decimal). 5. Average precipitation at the start (in centimeters, rounded to 1 decimal). 6. The month with the most trips (e.g., `4` for April). The data should be grouped by the starting and ending neighborhoods, with:`zip_codes` in `geo_us_boundaries` used to map the bike trip locations based on latitude and longitude. `zip_codes` in `cyclistic` used to obtain the borough and neighborhood names. Using weather data from the Central Park station for the trip date, covering all trips in 2014.","The provided context is insufficient to generate the required SQL query. The tables needed for analyzing bike trips, such as a trips table containing information about each bike trip (start and end locations, trip duration, start time), a weather table with detailed weather conditions (temperature, wind speed, and precipitation) for specific dates and locations, and tables linking geographic locations or zip codes to neighborhoods, are not described in the provided schema. Additionally, there is no mention of a Central Park weather station table or a cyclistic table that includes borough and neighborhood names based on zip codes. Therefore, without these tables, the query cannot be generated."
sf_bq426,"What user type recorded the highest average temperature for trips starting and ending in New York City's zip code 10019 during 2018? Include average precipitation, wind speed, and temperature for that user type based on weather data from the New York Central Park station.","The context provided is insufficient to generate a SQL query as there are no tables related to trips, user types, or weather data, including temperature, precipitation, or wind speed, especially linked to specific years or locations like New York City or Central Park. Please provide the relevant table schemas or data descriptions to proceed."
sf_bq291,"Can you provide a daily weather summary for July 2019 within a 5 km radius of latitude 26.75 and longitude 51.5? I need the maximum, minimum, and average temperatures; total precipitation; average cloud cover between 10 AM and 5 PM; total snowfall (when average temperature is below 32°F); and total rainfall (when average temperature is 32°F or above) for each forecast date. The data should correspond to forecasts created in July 2019 for the following day.","The provided tables do not include any weather-specific data such as temperature, precipitation, cloud cover, snowfall, or rainfall. Therefore, it is not possible to generate a SQL query to provide a daily weather summary based on the context given. Additional data sources containing weather-related information would be required to fulfill this request."
bq208,"Can you provide weather stations within a 20-mile radius of Chappaqua, New York (Latitude: 41.197, Longitude: -73.764), and tell me the number of valid temperature observations they have recorded from 2011 to 2020, excluding any invalid or missing temperature data?","The provided context is insufficient. The tables listed do not contain any data about weather stations or temperature observations, which are necessary to answer the query. Therefore, I cannot generate a valid SQL query based on the existing tables. Please provide the appropriate dataset or table schema containing information about weather stations and temperature observations."
bq047,"Could you analyze the relationship between each complaint type and daily temperature in New York City, specifically using temperature data from LaGuardia (STN=725030) and JFK (STN=744860) airports for the 10 years starting in 2008, and then determine, for each complaint type that has more than 5000 total occurrences and shows a strong correlation (absolute value > 0.5) with temperature, the total number of complaints, the total number of days with valid temperature records, and the Pearson correlation coefficients (rounded to four decimals) between temperature and both the daily complaint count as well as the daily percentage of total complaints, excluding any days with missing or invalid temperature data (such as 9999.9)?","The provided context is insufficient to generate the requested SQL query because:

1. There is no table provided that contains data related to complaint types or daily complaint counts.
2. There is no table provided that contains temperature data for LaGuardia and JFK airports or any other location-specific temperature data.
3. There is no indication of how or where the Pearson correlation coefficients should be calculated within the database, as this typically requires statistical functions or external processing not described in the provided schema.

To proceed, tables containing the relevant complaint data and temperature data would be necessary, along with a means to calculate Pearson correlation coefficients within or external to the SQL environment."
bq048,"Which complaint types with more than 3000 total requests from 2011 to 2020 show the strongest positive and negative Pearson correlations with the daily average wind speed measured at station 744860 (JFK Airport), based on daily complaint proportions (the ratio of type-specific complaints to total daily complaints)? Please provide the complaint types and their correlation coefficients, rounded to four decimal places.","The provided context does not contain any relevant information about complaint types, wind speed measurements, or any data that directly ties to complaint handling or environmental conditions. Thus, it is impossible to generate a valid SQL query based on the existing tables to answer this question."
bq293,"I want to analyze New York City yellow taxi trip data specifically for January 1, 2015, using the bigquery-public-data.new_york.tlc_yellow_trips_2015 dataset. I need to join this with the geo_us_boundaries.zip_codes table to map each trip to its respective NYC zip code based on the pickup coordinates. The analysis should create a complete hour-by-hour breakdown for all zip codes by cross-joining distinct zip codes with distinct hours, including hours with zero trips. For each zip code and hour combination, calculate the total number of trips and the following time-based metrics: count of trips from 1 hour ago, 1 day (24 hours) ago, 7 days (168 hours) ago, and 14 days (336 hours) ago. Additionally, compute the 14-day and 21-day moving averages and standard deviations of trip counts, excluding the current hour. Only include trips with valid latitude and longitude coordinates. The final results should be sorted by the highest trip counts, showing only the top 5 groups with the most trips.","The context provided does not include information or access to the `bigquery-public-data.new_york.tlc_yellow_trips_2015` dataset or the `geo_us_boundaries.zip_codes` table. Additionally, the task involves complex operations such as calculating moving averages and standard deviations, which require specific functions and capabilities of BigQuery SQL. These are not possible to implement without access to the actual data schema and environment of BigQuery.

To proceed, you would need to have access to the BigQuery platform and the specific datasets mentioned."
sf_bq017,"What are the five longest types of highways within the multipolygon boundary of Denmark (as defined by Wikidata ID 'Q35') by total length, analyzed through planet features?","The provided context is insufficient to generate the requested SQL query because there is no table or database schema related to geographic or multipolygon boundaries, nor is there a table related to Wikidata IDs or planet features. Additionally, no table includes information about highway types or their lengths within specific geographic boundaries. More specific geographic or mapping data is required to fulfill this query."
sf_bq131,"What is the number of bus stops for the bus network with the most stops within the multipolygon boundary of Denmark (as defined by Wikidata ID 'Q35'), analyzed through planet features?","The provided context does not include any tables or data related to bus networks, bus stops, planet features, or geographic boundaries such as those of Denmark. Therefore, I cannot generate a SQL query to answer the question about the number of bus stops for the bus network with the most stops within Denmark. Further data containing geographic locations of bus stops and their network association would be required to proceed."
sf_bq349,"Which OpenStreetMap ID from the planet features table corresponds to an administrative boundary, represented as multipolygons, whose total number of 'amenity'-tagged Points of Interest (POIs), as derived from the planet nodes table, is closest to the median count among all such boundaries?","The provided context is insufficient to generate the SQL query because there are no tables named ""planet features"" or ""planet nodes"" in the given schema. These tables are necessary to identify OpenStreetMap IDs, administrative boundaries represented as multipolygons, and Points of Interest (POIs) with 'amenity' tags. Please provide the relevant table structures or specify how these features map to the existing tables provided in the schema."
sf_bq007,"Identify the top 10 U.S. states with the highest vulnerable population, calculated based on a weighted sum of employment sectors using 2017 ACS 5-Year data, and determine their average median income change from 2015 to 2018 using zip code data. ","The provided context is insufficient to generate the SQL query because:
1. There is no table provided that contains detailed information on employment sectors by state or zip code, especially for the year 2017 or any other specific years.
2. There is no table containing historical median income data for the years 2015 to 2018 that can be linked to states or zip codes for calculating the average median income change.

Additional data sources containing the required employment sector information for 2017 and historical median income data for the specified years, linked by state or zip code, would be necessary to fulfill this query request."
sf_bq429,"Which are the top five states with the greatest average difference in median income between 2015 and 2018 at the ZIP code level, and what is the corresponding average number of vulnerable employees across wholesale trade, natural resources and construction, arts and entertainment, information, and retail trade industries in 2017 according to the ACS Five-Year Estimates and ZIP code boundaries data?","The provided context is insufficient to generate the requested SQL query because:

1. There is no data or tables provided that directly contain information on median income by year (specifically for the years 2015 and 2018) at the ZIP code level that would allow a year-to-year comparison.
2. There is no information or data provided on the number of vulnerable employees across the specified industries.
3. The ACS Five-Year Estimates data appears to be referenced, but there is no specific table provided containing such data or anything similar that could be used to calculate the average number of vulnerable employees.
4. No data is provided regarding ZIP code boundaries beyond basic geolocation and ZIP code data, and there is no linkage to specific years or detailed industry information in the provided tables.

To proceed, additional data containing yearly median income per ZIP code, detailed employment statistics by industry per ZIP code, and a method to categorize employees as vulnerable would be needed."
sf_bq073,"Using data on ZIP-level median income differences between 2015 and 2018, along with the 2017 ACS employment figures, list each state in descending order of total vulnerable workers, where “vulnerable” is defined as 38% of wholesale trade employees and 41% of manufacturing employees in 2017. Your results should include the state name, the number of vulnerable wholesale trade workers, the number of vulnerable manufacturing workers, and the combined total of these vulnerable workers.","The context provided does not contain any tables or data regarding employment figures by industry (such as wholesale trade or manufacturing) for any year, including 2017. The necessary data to compute the number of vulnerable workers as defined is not available in the given table schemas.

Therefore, it is not possible to generate a SQL query to fulfill the request based on the current database schema provided."
sf_bq410,"Find the top 3 states with the smallest adjusted non-labor force population using 2017 ACS tract-level data. Calculate the adjusted non-labor force population as (unemployed_pop + not_in_labor_force - group_quarters), clamping any negative values to zero. For each of these states, display the state abbreviation, the total median income change between 2015 and 2018 across all tracts, the total adjusted non-labor force population, and the average population-adjusted proportion (calculated as the ratio of adjusted non-labor force to total population at the tract level). Join tract data to states using FIPS code prefixes, exclude any tracts with null income differences, and sort results by the adjusted non-labor force population in ascending order.","The provided context and table structures do not contain the necessary data (such as ACS tract-level data, FIPS codes, unemployment data, group quarters information, and others) to generate the SQL query to find the top 3 states with the smallest adjusted non-labor force population using 2017 ACS tract-level data as described. Therefore, it is not possible to provide a valid SQL query based on the current information."
sf_bq348,"Within the rectangular area defined by the geogpoints (31.1798246, 18.4519921), (54.3798246, 18.4519921), (54.3798246, 33.6519921), and (31.1798246, 33.6519921), which are the top three usernames responsible for the highest number of historical nodes, originally tagged with the amenities ‘hospital’, ‘clinic’, or ‘doctors’, that do not appear anymore in the current planet_nodes dataset?","The provided context is insufficient to generate the required SQL query because there are no tables or schema details provided related to ""historical nodes"", ""planet_nodes dataset"", or ""usernames"" associated with these nodes. Additionally, there is no information about the ""amenities"" attribute within any of the provided tables. To proceed, the schema and data for these specific tables and attributes would be necessary."
sf_bq253,"Find the name of the OpenStreetMap relation that encompasses the most features within the same geographic area as the multipolygon tagged with the Wikidata item 'Q1095'. The relation should have a specified name and no 'wikidata' tag, and at least one of its included features must have a 'wikidata' tag. The analysis should be conducted using the planet_features table. Return the name of this relation.",The provided context is insufficient to generate the SQL query as there is no description or schema for a table named `planet_features` in the provided database schema. This table is necessary to analyze OpenStreetMap relations and features as described in the query request. Please provide the schema for the `planet_features` table or any relevant table that contains the necessary data for OpenStreetMap features and relations.
sf_bq254,"Among all multipolygons located within the same geographic area as the multipolygon associated with Wikidata item Q191, but lacking a 'wikidata' tag themselves, which two rank highest by the number of points that lie within their boundaries, and what are their names?","The provided context is insufficient to generate a SQL query because there are no tables in the schema that contain geographic data related to multipolygons or any data linked to Wikidata items. Additionally, there is no information on tags or points within geographic boundaries. A different data source or additional table schema information would be required to answer this query."
sf_bq056,"How many different pairs of roads classified as motorway, trunk, primary, secondary, or residential in California overlap each other without sharing nodes and do not have a bridge tag, where these roads are tagged with 'highway', analyzed through planet ways","The provided context is insufficient to generate the requested SQL query because the database schema given does not include a table or dataset that clearly relates to road classifications, tagging, or attributes like ""planet ways"" or ""bridge tag"" typically found in OpenStreetMap or similar geographical datasets. There is no mention in the schema of any table containing road types, tags, or spatial features necessary for determining overlaps without sharing nodes. Further access to a relevant dataset or table structure is required to accurately address this query."
sf_bq289,"Can you find the shortest distance between any two amenities (either a library, place of worship, or community center) located within Philadelphia, analyzed through pennsylvania table and planet features points?","The provided context is insufficient to generate the required SQL query as there are no tables named ""pennsylvania"" or ""planet features points"" and no information regarding amenities like libraries, places of worship, or community centers in the given table structures. The necessary data to perform this analysis is not available in the provided context."
sf_bq250,"Based on the most recent 1km population grid data in Singapore before January 2023, using ST_CONVEXHULL to aggregate all population grid centroids into a bounding region and ST_INTERSECTS to identify hospitals from OpenStreetMap’s planet layer (layer_code in (2110, 2120)) that fall within this region, then calculating the distance from each grid cell to its nearest hospital, what is the total population of the grid cell that is farthest from any hospital?","The provided context is insufficient to generate a SQL query for the following reasons:

1. **Absence of Population Grid Data**: There's no table provided that includes the 1km population grid data for Singapore or any other location.
  
2. **Lack of GIS Functions**: The database schema does not indicate support for GIS functions such as `ST_CONVEXHULL` or `ST_INTERSECTS`, which are crucial for processing spatial queries as described.

3. **Missing OpenStreetMap Data**: The schema does not include any OpenStreetMap data or references to a `layer_code` that would represent different types of geographical features, such as hospitals.

4. **No Spatial Coordinates in Tables**: There are no tables provided with spatial coordinates that would allow for the calculation of distances or intersections of geographic features.

To accurately respond to this query, the database would need tables containing spatial data of population grids and hospital locations, and the database system itself would need to support spatial SQL functions."
sf_bq083,"Can you calculate the daily change in the market value of USDC tokens (address `0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48`) for 2023, based on Ethereum transactions? The change should be computed from minting (input pattern `0x40c10f19%`) and burning (input pattern `0x42966c68%`) operations. For each transaction, minting should be positive and burning negative. Extract the relevant amount from the 'input' field as a hexadecimal, convert it to millions, express it in USD format. Group the results by date and order them in descending order.","The context provided is insufficient for generating the SQL query because there is no table described in the database schema that contains information about Ethereum transactions, transaction inputs, or specific operations like minting and burning of USDC tokens. Additionally, details on how to extract and convert the hexadecimal values from the 'input' field are not provided in the schema. To proceed, the schema would need to include a table with Ethereum transaction data that specifically records token addresses, input patterns, and transaction dates."
sf_bq184,"Using only the traces, can you calculate daily cumulative counts of smart contracts created by external addresses (where the trace_address is NULL) versus those created by other contracts (where the trace_address is NOT NULL) for each date from 2017-01-01 through 2021-12-31, ensuring that all dates in this range are included even if no new contracts were created on some days, and showing monotonically increasing cumulative totals for both categories?","The provided context is insufficient to generate the requested SQL query as there is no table or column information related to ""smart contracts,"" ""external addresses,"" or ""trace addresses"" in the database schema descriptions provided. Please provide the relevant table schema or additional information regarding where and how smart contracts and trace addresses are stored in the database to proceed with crafting the appropriate SQL query."
sf_bq195,"What are the top 10 Ethereum addresses by balance, considering both value transactions and gas fees, before September 1, 2021? Only keep successful transactions with no call type or where the call type is 'call'.","The provided context is insufficient to generate the SQL query as there is no table containing Ethereum transaction data, Ethereum addresses, or any related blockchain data such as balances, gas fees, or call types. Please provide the necessary table structure or data source to proceed with generating the SQL query."
sf_bq256,"Determine the final Ether balance of the Ethereum address that initiated the highest number of successful transactions prior to September 1, 2021 (UTC), excluding calls of type delegatecall, callcode, or staticcall and including all relevant incoming and outgoing transfers, miner rewards, and gas fee deductions, with the final balance presented in Ether after converting from the native unit.","The provided context is insufficient to generate the requested SQL query because there are no tables or data related to Ethereum transactions, Ethereum addresses, transaction types like delegatecall, callcode, or staticcall, miner rewards, or gas fee deductions. Further, there is no table containing data about balances that could be converted from native units to Ether. Therefore, it's not possible to construct a SQL query based on the given database schema to satisfy the query requirements."
sf_bq080,"Using only the Ethereum traces table, can you provide a daily cumulative count of smart contracts created by external users (where trace_address is null) versus contracts created by other contracts (where trace_address is not null) between August 30, 2018, and September 30, 2018? Ensure results include every date in this range, even if no new contracts were created, and show strictly increasing cumulative totals.","The provided context is insufficient to generate the requested SQL query because there is no ""Ethereum traces"" table described in the given schema information. Please provide the schema for the relevant table or any additional details that might help in generating the required SQL query."
sf_bq342,"What is the difference between the average hourly changes in transaction values for the Ethereum token 0x68e54af74b22acaccffa04ccaad13be16ed14eac, specifically considering only transactions where the address 0x8babf0ba311aab914c00e8fda7e8558a8b66de5d was the sender or the address 0xfbd6c6b112214d949dcdfb1217153bc0a742862f was the receiver, between January 1, 2019, and December 31, 2020, when comparing 2019 to 2020?","The provided context is insufficient to generate the requested SQL query. The database tables provided do not include any table related specifically to Ethereum transactions, nor do they contain any fields that would directly relate to Ethereum token addresses or the sender/receiver addresses as specified in the question. To proceed, detailed data regarding Ethereum transactions would be required, which is not available in the current dataset."
sf_bq341,"Which Ethereum address has the top 3 smallest positive balance from transactions involving the token at address ""0xa92a861fc11b99b24296af880011b47f9cafb5ab""?","The provided context is insufficient to generate a SQL query for the given question because there are no tables or database schema details related to Ethereum transactions, Ethereum addresses, or token balances. Please provide the relevant database schema or table details in order to proceed with creating an appropriate SQL query."
sf_bq444,"Can you pull the blockchain timestamp, block number, and transaction hash for the first five mint and burn events from Ethereum logs for the address '0x8ad599c3a0ff1de082011efddc58f1908eb6e6d8'? Please include mint events identified by the topic '0x7a53080ba414158be7ec69b987b5fb7d07dee101fe85488f0853ae16239d0bde' and burn events by '0x0c396cd989a39f4459b5fa1aed6a9a8dcdbc45908acfd67e028cd568da98982c', and order them by block timestamp from the oldest to the newest.","The provided database context does not contain any tables related to blockchain data such as Ethereum logs, block numbers, transaction hashes, or specific blockchain events like mint and burn. Therefore, I cannot generate the requested SQL query. You would need access to a database that stores Ethereum blockchain events and related attributes to perform this query."
sf_bq340,"Which six Ethereum addresses, excluding '0x0000000000000000000000000000000000000000', have the largest absolute differences between their previous and current balances from the tokens at addresses '0x0d8775f648430679a709e98d2b0cb6250d2887ef0' and '0x1e15c05cbad367f044cbfbafda3d9a1510db5513'?","The provided context is insufficient for generating the requested SQL query as there are no tables or data structures mentioned in the schema related to Ethereum, blockchain addresses, or token balances. Further information or a relevant database schema is necessary to proceed with the SQL query creation."
sf_bq005,"Calculate the daily average Bitcoin block interval (in seconds) for 2023 by joining consecutive blocks via row-numbered self-joins (including cross-day intervals), excluding the genesis block, and list the first 10 dates with their unadjusted averages.","The provided context does not contain any relevant tables or data that pertain to Bitcoin, blockchain, or any time-based block generation details. Therefore, it is impossible to generate the requested SQL query based on the available table structures and data descriptions."
sf_bq334,"Calculate the annual differences in Bitcoin output value averages between two methods: Merged input/output records: Combine the inputs and outputs tables, filter to only output records, and calculate yearly averages. Transactions table: Directly use the output_value field from the transactions table for yearly averages. Show the difference (merged outputs average minus transactions average) only for years with data in both methods.","The provided context is insufficient to generate the requested SQL query. There is no table with a detailed structure for Bitcoin input or output transactions, nor is there a table with an `output_value` field in the transactions table. Additionally, no specific table connects directly to the Bitcoin input/output records as described in the request. Hence, I can't generate the SQL query to calculate the annual differences in Bitcoin output value averages between the two methods mentioned."
sf_bq335,"Among all Bitcoin addresses that have at least one transaction in October 2017 (combining both inputs and outputs), which address conducted its final transaction on the latest date in that month, and, among any addresses sharing that same latest date, which one has the highest sum of transaction values?","The provided context is insufficient to generate the required SQL query. There is no information or table schema provided that includes Bitcoin addresses or a way to associate transactions directly with Bitcoin addresses. The ""bitcoin_transactions"" table mentioned does not contain fields for Bitcoin addresses or a direct way to distinguish inputs from outputs, which are essential to answer the question. Further data or table schema adjustments are required to proceed."
sf_bq057,"Which month (e.g., 3 for March) in 2021 witnessed the highest percentage of Bitcoin transaction volume occurring in CoinJoin transactions (defined as transactions with >2 outputs, output value ≤ input value, and having multiple equal-value outputs)? Also provide the percentage of all Bitcoin transactions that were CoinJoins, the percentage of UTXOs involved in CoinJoin transactions (average of input and output percentages), and the percentage of total Bitcoin volume that occurred in CoinJoin transactions for that month. Round all percentages to 1 decimal place.","The provided context does not contain detailed information about Bitcoin transactions such as the number of outputs, input values, output values, or details about UTXOs (Unspent Transaction Outputs) necessary to identify CoinJoin transactions or compute the required percentages accurately. Therefore, I cannot generate a SQL query to fulfill the request based on the available tables and data structure.

To perform this analysis, we would need a table or dataset with transaction details that include the number of outputs per transaction, the values of these outputs, and matching input values."
sf_bq068,"Using double-entry bookkeeping principles by treating transaction inputs as debits (negative values) and outputs as credits (positive values) for all Bitcoin Cash transactions between 2014-03-01 and 2014-04-01, how can we calculate the maximum and minimum final balances grouped by address type from these transactions?","The provided context is insufficient to generate the requested SQL query since there are no tables related to Bitcoin Cash transactions or any tables containing cryptocurrency transaction data, including address types and their respective transaction amounts (debits and credits). Additionally, details about how to identify or classify 'address types' are not provided. Please provide the relevant table structures or data sources that include these specific details for cryptocurrency transactions."
sf_bq092,"In April 2023, what are the highest and lowest balances across all Dash addresses when calculating the net balance for each address using double-entry bookkeeping (where inputs are treated as debits/negative values and outputs as credits/positive values)? Consider all transactions filtered by block_timestamp_month='2023-04-01', and when an address appears as an array in the data, concatenate the array elements into a comma-separated string. For each address and type combination, sum all the values to determine the balance.","The provided context is insufficient to generate a SQL query for the following reasons:

1. **Lack of Blockchain Transaction Table**: The request involves analyzing blockchain transactions (specifically for Dash addresses), but no table containing blockchain transaction data (like addresses, inputs, outputs, and block timestamps) is provided in the context.

2. **Unknown Column Names and Table Structure**: The specific structure, column names, and types (like `block_timestamp_month`, `addresses`, `inputs`, and `outputs`) required to perform the analysis on blockchain transactions are not described in any of the given table structures.

3. **Array Handling in SQL**: The request to concatenate array elements into a comma-separated string suggests that transactions involving multiple addresses may be stored in an array format. However, there's no indication of how arrays are managed or stored in the provided database schema.

To proceed, a table containing detailed blockchain transaction data is essential, and information on how arrays are stored and manipulated in the database is needed."
sf_bq093,"What were the maximum and minimum net balance changes for Ethereum Classic addresses on October 14, 2016? Calculate these by summing all transactions where addresses received funds (debits), sent funds (credits), and paid or received gas fees. Only include successful status transactions and exclude internal calls of types. For gas fees, consider both the fees paid by transaction senders and received by miners, calculated as multiplied by the gas price for both miners and senders","The provided context is insufficient to generate a query for the requested data analysis. The requirements mention Ethereum Classic addresses and transaction details that involve debits, credits, gas fees, and specific transaction statuses on a particular date. However, none of the tables described in the schema provided includes data related to Ethereum Classic transactions, addresses, or blockchain-specific elements such as gas fees. Therefore, it is not possible to generate the requested SQL query based on the current data structure provided."
sf_bq292,"Analyze Bitcoin transactions since July 2023 to determine monthly percentages of: (1)Transactions classified as CoinJoins (defined by >2 outputs, output value ≤ input value, and multiple identical-value outputs), (2) UTXOs involved in CoinJoins (calculated as the average of CoinJoin input/output ratios against total network UTXOs), (3) Transaction volume (based on input value) attributed to CoinJoins. Provide results in a table with monthly metrics for transactions, UTXOs, and volume.","The provided context is insufficient to generate the SQL query because the necessary details such as input values, output values, number of outputs, and total network UTXOs are not available in the provided table structure. The table `bitcoin_transactions` does not contain fields that would directly indicate input values, output values, or the specifics of outputs needed to classify transactions as CoinJoins. Additionally, there is no data about the total network UTXOs, which is necessary for the calculations specified in the request."
sf_bq135,Which date before 2022 had the highest total transaction amount in the Zilliqa blockchain data?,"The provided context lacks a specific table or data directly referencing the Zilliqa blockchain or blockchain transactions in general. Therefore, I can't generate the requested SQL query."
sf_bq136,"Find all exactly 2-hop transaction paths on Zilliqa blockchain between the source address 'zil1jrpjd8pjuv50cfkfr7eu6yrm3rn5u8rulqhqpz' and destination address 'zil19nmxkh020jnequql9kvqkf3pkwm0j0spqtd26e', considering both regular transactions and contract transitions. A 2-hop path means there must be an intermediate address between source and destination. Exclude paths where any intermediate address has more than 50 outgoing transactions to filter out exchanges and high-activity wallets. Ensure transactions in each path follow chronological order (earlier transaction timestamps first). Display results in the format: '<source> --(tx ABCDE..)--> <intermediate> --(tx FGHIJ..)--> <destination>' where the transaction IDs are truncated to the first 5 characters. Include only confirmed on-chain transactions in both steps of the path.","The provided context is insufficient to generate a SQL query because there are no tables or data schema details related to the Zilliqa blockchain transactions, addresses, or any relevant blockchain activity. Information about how transactions, addresses, and their interactions are stored is necessary to structure an appropriate query. Please provide the relevant table schema or data structure that includes transaction records, addresses, timestamps, and their relationships."
sf_bq065,"From the oracle requests table, retrieve the 10 most recent oracle requests with script ID 3. For each request, extract all symbol-rate pairs by matching each symbol in the ""symbols"" array with its corresponding rate at the same position in the ""rates"" array from the decoded result. Adjust each rate by dividing it by the request's multiplier value. Return the block timestamp, oracle request ID, symbol, and the adjusted rate for each symbol-rate pair. Sort the results in chronological order with the newest records first.","The provided context is insufficient to generate the requested SQL query because there is no table with the name ""oracle requests"" or any other table described in the schema that contains columns such as ""script ID"", ""symbols"" array, ""rates"" array, ""multiplier"", or ""block timestamp"". A different database schema or additional details about the relevant tables and their structure are needed to proceed with constructing a SQL query that can fulfill the query requirements."
sf_bq037,"About the refined human genetic variations collected in phase 3 on 2015-02-20, I want to know the minimum and maximum start positions as well as the proportions of these two respectively for reference bases 'AT' and 'TA'.","The provided context is insufficient to generate the SQL query because there is no table in the given schema that contains information about genetic variations, reference bases, or start positions related to genetic data. To accurately respond to your request, information about a table containing genetic variation data including fields for date, reference bases, and start positions would be necessary."
sf_bq012,"Calculate the average balance (in quadrillions, 10^15) of the top 10 Ethereum addresses by net balance, including incoming and outgoing transfers from traces (only successful transactions and excluding call types like delegatecall, callcode, and staticcall), miner rewards (sum of gas fees per block), and sender gas fee deductions. Exclude null addresses and round the result to two decimal places.","The provided context is insufficient to generate the SQL query as there are no tables related to Ethereum addresses, incoming and outgoing transfers from traces, miner rewards, or sender gas fee deductions in the provided schema information. Thus, it's not possible to proceed with creating a SQL query based on the given databases."
sf_bq187,"Calculate the total circulating supply of 'BNB' tokens (in units divided by 10^18) by summing balances of all non-zero addresses, where each address’s balance equals its total received BNB minus sent BNB. Exclude transactions involving the zero address (0x000...) for both senders and receivers.","The provided context does not have any table containing information about BNB tokens, blockchain addresses, or any related transaction details involving cryptocurrency balances or transfers. Therefore, it is not possible to generate a SQL query based on the given database schemas to calculate the total circulating supply of 'BNB' tokens. Please provide the relevant table schema and data context for cryptocurrency transactions and balances to generate the required SQL query."
sf_bq450,"Generate a comprehensive report of all Ethereum addresses active before January 1, 2017, calculating their net balances (adjusted for transaction fees and excluding delegatecall/callcode/staticcall transactions), hourly activity patterns, active days, incoming/outgoing transaction metrics (counts, unique counterparties, average ETH transfers), ERC20 token interactions (in/out counts, unique tokens, counterparties), mining rewards, contract creation frequency, failed transaction counts, and contract bytecode sizes, with all ETH values converted to standard units (divided by 10^18) and excluding addresses with no transaction history.","The provided context is insufficient to generate the requested SQL query. The tables and their descriptions necessary to retrieve information about Ethereum addresses, transactions, mining rewards, ERC20 token interactions, and contract bytecode are not present in the provided schema. Therefore, I cannot create a SQL query that fulfills the requirements mentioned without access to the relevant blockchain data structured in a relational database format."
bq034,"I want to know the IDs, names of weather stations within a 50 km straight-line distance from the center of Chicago (41.8319°N, 87.6847°W)","The provided context is insufficient to generate a SQL query because there is no table that includes information on weather stations, their names, IDs, and geographical coordinates (latitude and longitude). Additionally, there is no data related to the city of Chicago or any geographical data that can be used to calculate distances to specific coordinates. A table with weather station details and coordinates is necessary to proceed with this request."
bq383,"Could you provide the highest recorded precipitation, minimum temperature, and maximum temperature from the last 15 days of each year from 2013 to 2016 at weather station USW00094846? Ensure each value represents the peak measurement for that period, with precipitation in millimeters and temperatures in degrees Celsius, using only validated data (non-null values and no quality flags)","The provided context is insufficient to generate the requested SQL query because there is no table or schema related to weather data, such as precipitation or temperature, in the provided schema description. Additionally, there is no information on how quality flags or units of measurement are stored or represented in the database. Therefore, I cannot create a valid SQL query to retrieve the highest recorded precipitation, minimum temperature, and maximum temperature for the specified conditions and years. Please provide the relevant table schema or additional information related to weather data, including how to handle quality flags and units of conversion."
bq051,"Calculate the average daily number of Citibike trips in New York City during 2016, categorizing days as rainy if the total precipitation exceeds 5 millimeters (obtained by dividing the raw precipitation value by 10), and non-rainy otherwise. Use data from the nearest GHCN station located within 50 km of (40.7128, -74.0060) that has valid, unflagged measurements, then compare the resulting average Citibike trips on rainy days versus non-rainy days.","The provided context is insufficient to generate the requested SQL query because there are no tables containing information about Citibike trips, weather data, precipitation, or GHCN stations. Additional data sources containing these specific datasets would be required to fulfill the query request."
bq038,"Identify the top 10 Citibike stations by highest proportion of group rides, defined as trips starting and ending at the same station where multiple riders departed/arrived within the same 2-minute time window. Calculate the proportion as the number of trips that are part of a group divided by the total number of trips ending at that station.","The context provided is insufficient to generate the requested SQL query because there are no tables related to Citibike, trips, stations, or time windows of trips in the given schema. The existing tables focus on racing events, order processing, and baseball statistics, among others, which do not contain relevant information about bike sharing or group rides. To proceed, a dataset containing records of bike trips including station identifiers, trip start and end times, and possibly user or bike identifiers would be required."
bq053,"Calculate the change in the number of living trees of each fall color in New York City from 1995 to 2015 by computing, for each tree species, the difference between the number of trees not marked as dead in 1995 and the number of trees alive in 2015, matching species by the uppercase form of their scientific names from the tree_species table. Then, group the species by their fall color and sum these differences to determine the total change in the number of trees for each fall color.","The provided context does not mention or define a `tree_species` table, which is necessary to determine the fall colors associated with each tree species. Additionally, the `trees` table does not contain columns that directly indicate the year of the data entry or the fall color of trees. Thus, we cannot generate a SQL query for the requested analysis without these critical pieces of information.

If the `tree_species` table and the relevant year data in the `trees` table were available, it would be possible to proceed with the SQL query."
bq054,"Please provide the top 10 tree species in New York, using their uppercase Latin names where the Latin name is not empty and including their common names, showing the total number of trees, the counts of alive and dead trees for each year, and the corresponding growth in these counts from 1995 to 2015, then order by the difference in total tree counts between these years.","SELECT 
    UPPER(t.spc_latin) AS Latin_Name,
    t.spc_common AS Common_Name,
    COUNT(*) AS Total_Trees,
    SUM(CASE WHEN t.status = 'Alive' THEN 1 ELSE 0 END) AS Alive_Trees,
    SUM(CASE WHEN t.status = 'Dead' THEN 1 ELSE 0 END) AS Dead_Trees,
    COUNT(*) FILTER (WHERE t.boroname = 'New York' AND EXTRACT(YEAR FROM t.process_date) = 1995) AS Trees_1995,
    COUNT(*) FILTER (WHERE t.boroname = 'New York' AND EXTRACT(YEAR FROM t.process_date) = 2015) AS Trees_2015,
    (COUNT(*) FILTER (WHERE t.boroname = 'New York' AND EXTRACT(YEAR FROM t.process_date) = 2015) -
     COUNT(*) FILTER (WHERE t.boroname = 'New York' AND EXTRACT(YEAR FROM t.process_date) = 1995)) AS Growth_1995_2015
FROM 
    trees AS t
WHERE 
    t.spc_latin IS NOT NULL AND t.boroname = 'New York'
GROUP BY 
    t.spc_latin, t.spc_common
ORDER BY 
    Growth_1995_2015 DESC
LIMIT 
    10;"
bq021,"For the top 20 Citi Bike routes in 2016, which route is faster than yellow taxis and among those, which one has the longest average bike duration? Please provide the start station name of this route. The coordinates are rounded to three decimals.","The context provided is insufficient as there is no information or data tables related to ""Citi Bike"" or ""yellow taxis"" including their routes, durations, or start station names. The existing tables pertain to racing, pit stops, lap times, results, and a few other unrelated categories, none of which include data on bikes or taxis. Therefore, I cannot generate the requested SQL query."
bq202,"For the station that had the highest number of Citibike trips starting there in 2018, which numeric day of the week and which hour of the day had the greatest number of trips based on the start time of those trips?","The provided context is insufficient to generate an SQL query to answer the question as there is no table related to Citibike trips or any details about stations, trip start times, or specific data related to bike sharing services."
bq185,"What is the average trip duration in minutes for all valid Yellow taxi trips that took place between February 1, 2016, and February 7, 2016 (inclusive), with a positive trip duration, more than three passengers, and a trip distance of at least ten miles, where both the pickup and dropoff locations are in Brooklyn?","The provided context is insufficient to generate the requested SQL query because there is no table or column information related to ""Yellow taxi trips,"" passenger count, trip duration, trip distance, or location specifics such as pickup and dropoff locations being in Brooklyn. Additional details about the relevant table structures and column names are necessary to formulate this query."
bq040,"For NYC yellow taxi trips between January 1 and January 7, 2016, excluding any trips picked up in ‘EWR’ or ‘Staten Island,’ determine the proportion of rides that fall into each tip category in each pickup borough. Only include trips where the dropoff time is after the pickup time, the passenger count is greater than zero, and trip_distance, tip_amount, tolls_amount, mta_tax, fare_amount, and total_amount are all non-negative. Classify the tip percentage as follows: 0% (no tip), up to 5%, 5% to 10%, 10% to 15%, 15% to 20%, 20% to 25%, and more than 25%.","The provided context is insufficient to generate the requested SQL query because there is no table schema or data related to NYC yellow taxi trips. The necessary table containing data fields like pickup and dropoff times, pickup borough, passenger count, trip distance, tip amount, and other fare-related details is not provided in the context. Without this information, it is impossible to create the SQL query to answer the question about NYC yellow taxi trips."
bq098,"For NYC yellow taxi trips where both the pickup and dropoff occurred between January 1 and 7, 2016, inclusive, calculate the percentage of trips with no tip in each pickup borough, ensuring that only trips where the dropoff occurs after the pickup are included, the passenger count is greater than zero, and the trip distance, tip amount, tolls amount, MTA tax, fare amount, and total amount are non-negative; define ""no tip"" trips as those where the tip rate is zero, with the tip rate calculated as (tip_amount × 100) divided by total_amount (and considered zero when total_amount is zero).","The provided context is insufficient to generate the required SQL query because there is no table related to NYC yellow taxi trips, nor any tables containing fields related to passenger count, pickup and dropoff dates, boroughs, or financial details of the trips such as tip amount, tolls amount, MTA tax, fare amount, and total amount. The tables provided do not match the needed context for taxi trip data analysis."
bq039,"Find the top 10 taxi trips in New York City between July 1 and July 7, 2016 (ensuring both pickup and dropoff times fall within these dates) where the passenger count is greater than five, the trip distance is at least ten miles, and there are no negative fare-related amounts (including tip, tolls, mta tax, fare, and total costs). Exclude any trips where the dropoff time is not strictly after the pickup time, then sort the results by total fare amount in descending order. Finally, display each trip’s pickup zone, dropoff zone, trip duration in seconds, driving speed in miles per hour, and tip rate as a percentage of the total fare amount.","The provided context is insufficient to generate the requested SQL query as it does not include any table structure or description pertaining to taxi trips, fare details, or geographical zones in New York City. Therefore, I can't generate the query without information on the relevant table(s) and their columns."
bq203,"For each New York City borough, how many subway stations are there in total, how many have at least one entrance that is marked both as an actual entry and as ADA-compliant, and what percentage of the total stations in each borough does this represent, listing boroughs from the highest to the lowest percentage?","The provided context is insufficient to generate the required SQL query as there are no tables containing information about subway stations, their ADA compliance, entry status, or their association with specific boroughs of New York City. To proceed with this analysis, data regarding subway stations, including their locations by borough and details on ADA compliance and entrance statuses, would be necessary."
bq035,What is the total distance traveled by each bike in the San Francisco Bikeshare program? Use data from bikeshare trips and stations to calculate this.,"The provided context is insufficient to generate the SQL query because there are no tables or schema details related to a ""Bikeshare program,"" ""bikeshare trips,"" or ""stations"" in the provided database schema information. Without these specific tables, it is not possible to calculate the total distance traveled by each bike in the San Francisco Bikeshare program."
bq186,"Please find, for each year-month combination (in the format YYYYMM) derived from the start date of bike share trips in San Francisco, the first trip duration in minutes, the last trip duration in minutes, the highest trip duration in minutes, and the lowest trip duration in minutes, where ‘first’ and ‘last’ are determined by the chronological order of the trip start date, then group your results by this year-month and sort them by the same year-month key.","The provided context is insufficient to generate the requested SQL query. The database schema provided does not include any table related to ""bike share trips"" or fields directly related to ""trip duration"" or ""trip start date."" Please provide the relevant table and field information or confirm the presence of such data in the existing database schema."
bq081,"Find the latest ride data for each region between 2014 and 2017. I want to know the name of each region, the trip ID of this ride, the ride duration, the start time, the starting station, and the gender of the rider.","The provided context is insufficient to generate the SQL query as there is no table that explicitly contains information about rides, trip IDs, ride durations, start times, starting stations, or rider gender. Please provide details on the relevant table structure or confirm the correct table and column names that include this information."
sf_bq294,"Could you provide the details of the top 5 longest bike share trips that started between July 1, 2017, and December 31, 2017, including the trip ID, duration in seconds, start date, start station name, route (derived from start station name to end station name), bike number, subscriber type, member's birth year, the member's current age (calculated using the current year), an age classification based on whether the member is younger than 40, between 40 and 60, or older than 60, the member's gender, and the name of the region of the start station? Please exclude any trips where the start station name, member's birth year, or member's gender is not specified.","The context provided does not contain information or any table schema related to bike share trips, trip duration, stations, bike numbers, subscriber types, or member's birth year and gender. Therefore, I cannot generate a SQL query based on the details provided."
bq339,"Which month in 2017 had the largest absolute difference between cumulative bike usage minutes for customers and subscribers?	Which month (in number) in 2017 had the largest absolute difference between cumulative bike usage minutes (in thousands) for customers and subscribers, based on the trip end dates in the San Francisco bikeshare data?","The provided context is insufficient to generate the required SQL query as there is no table or data related to ""San Francisco bikeshare data"", ""trip end dates"", ""bike usage minutes"", ""customers"", and ""subscribers"" provided in the schema details. Please provide the relevant table structures or data descriptions to proceed with the SQL query formulation."
bq400,"For trips where 'Clay St & Drumm St' occurs before 'Sacramento St & Davis St' in the stop sequence (one direction only), what are the earliest departure times from 'Clay St & Drumm St' and the latest arrival times at 'Sacramento St & Davis St' in the format HH:MM:SS? Please provide the trip headsign for each route.","The context provided is insufficient to generate the SQL query as there are no tables or columns provided that relate to trip information, stop sequences, or stop names such as 'Clay St & Drumm St' and 'Sacramento St & Davis St'. Additional details about the database schema including tables that contain trip details, stop sequences, and timing information are necessary to formulate the query."
bq059,"What is the highest average speed (rounded to 1 decimal, in metric m/s) for bike trips in Berkeley with trip distance greater than 1000 meters?","The provided context does not include any table or column names that are related to bike trips, trip distances, average speeds, or any specific location like Berkeley. Therefore, I cannot generate a SQL query based on the given table schemas as they do not relate to the information required to answer your question about bike trips. Please provide the relevant table and column names or adjust the question to match the available data context."
bq376,"For each neighborhood in San Francisco where at least one bike share station and at least one crime incident are located, provide the neighborhood name along with the total count of bike share stations and the total number of crime incidents in that neighborhood.","The provided context is insufficient to generate the requested SQL query because there is no table in the context that explicitly contains information about neighborhoods, bike share stations, or crime incidents. Further details about these specific datasets or tables containing the required fields would be necessary to proceed with an accurate SQL query."
sf_bq014,Can you help me figure out the revenue for the product category that has the highest number of customers making a purchase in their first non-cancelled and non-returned order?,The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
sf_bq188,"Among all product categories in the dataset, identify the category with the highest total purchase quantity (based on order_items table), and for that specific category, what is the average time in minutes that users spend on each product page visit? The average time should be calculated as the difference between the timestamp when a user views a product page and the timestamp of the next event within the same session","The provided context is insufficient to generate the requested SQL query accurately for the following reasons:

1. There is no ""order_items"" table mentioned or described in the provided schema to determine the total purchase quantity by product category.
2. The ""shopping_cart_events"" table does not explicitly link to product categories or contain a direct way to calculate the time spent on each page (as timestamps for subsequent events are required but not clearly structured for this calculation).

Due to these issues, it's not possible to accurately generate the SQL query that fulfills the request. Further clarification on the table structures and additional data linking product categories to orders or shopping cart events would be necessary."
sf_bq258,"Generate a monthly report for each product category , where each row corresponds to orders that have a status of 'Complete' and were delivered before the year 2022, grouping by the month and year of delivery. For each category, calculate the total revenue (the sum of sale_price), the total number of completed orders, and compute the month-over-month percentage growth for both revenue and orders by comparing each month’s totals to the previous month’s. Then, for the same orders, aggregate and show the total cost (from product costs), total profit (revenue minus total cost), and finally the profit-to-cost ratio for each month.",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
sf_bq259,"Using data up to the end of 2022 and organized by the month of each user's first purchase, can you provide the percentage of users who made a purchase in each of the first, second, third, and fourth months since their initial purchase, where the ""first month"" refers to the month of their initial purchase?","WITH FirstPurchase AS (
    SELECT 
        user_id, 
        MIN(time_id) AS first_purchase_date
    FROM sales
    WHERE time_id <= '2022-12-31'
    GROUP BY user_id
),
MonthlyPurchases AS (
    SELECT 
        fp.user_id,
        fp.first_purchase_date,
        strftime('%Y-%m', s.time_id) AS purchase_month,
        CAST(strftime('%m', s.time_id) AS INTEGER) - CAST(strftime('%m', fp.first_purchase_date) AS INTEGER) 
        + 12 * (CAST(strftime('%Y', s.time_id) AS INTEGER) - CAST(strftime('%Y', fp.first_purchase_date) AS INTEGER)) 
        AS month_since_first
    FROM sales s
    JOIN FirstPurchase fp ON s.user_id = fp.user_id
    WHERE s.time_id <= '2022-12-31'
),
PurchaseCounts AS (
    SELECT 
        purchase_month,
        month_since_first,
        COUNT(DISTINCT user_id) AS users
    FROM MonthlyPurchases
    WHERE month_since_first BETWEEN 0 AND 3
    GROUP BY purchase_month, month_since_first
),
TotalUsers AS (
    SELECT 
        purchase_month,
        COUNT(DISTINCT user_id) AS total_users
    FROM MonthlyPurchases
    WHERE month_since_first = 0
    GROUP BY purchase_month
)
SELECT 
    pc.purchase_month,
    pc.month_since_first,
    pc.users,
    tu.total_users,
    ROUND((pc.users * 100.0 / tu.total_users), 2) AS percentage
FROM PurchaseCounts pc
JOIN TotalUsers tu ON pc.purchase_month = tu.purchase_month
ORDER BY pc.purchase_month, pc.month_since_first;"
sf_bq189,"Based solely on completed orders, calculate the average monthly percentage growth rate in the number of unique orders (counting distinct order IDs) for each product category by comparing each month's count to the previous month within the same category. Identify the product category with the highest average of these monthly order growth rates. Then, for that specific product category, compute the average monthly revenue growth rate by calculating the percentage change in total revenue (sum of sale prices) from month to month and averaging these values over the entire period.","WITH MonthlyOrderData AS (
    SELECT 
        p.categoryid,
        strftime('%Y-%m', o.order_moment_created) AS month,
        COUNT(DISTINCT o.order_id) AS order_count
    FROM 
        orders o
    JOIN 
        orderlines ol ON o.order_id = ol.order_id
    JOIN 
        products p ON ol.product_id = p.productid
    WHERE 
        o.order_status = 'Completed'
    GROUP BY 
        p.categoryid, month
),
MonthlyOrderGrowth AS (
    SELECT
        MOD1.categoryid,
        MOD1.month,
        ((MOD1.order_count - COALESCE(MOD2.order_count, 0)) / (COALESCE(MOD2.order_count, 1))) * 100 AS growth_rate
    FROM 
        MonthlyOrderData MOD1
    LEFT JOIN 
        MonthlyOrderData MOD2 ON MOD1.categoryid = MOD2.categoryid AND MOD2.month = strftime('%Y-%m', date(MOD1.month || '-01', '-1 month'))
),
AvgMonthlyGrowthByCategory AS (
    SELECT
        categoryid,
        AVG(growth_rate) AS avg_growth_rate
    FROM 
        MonthlyOrderGrowth
    GROUP BY 
        categoryid
),
MaxGrowthCategory AS (
    SELECT
        categoryid
    FROM 
        AvgMonthlyGrowthByCategory
    ORDER BY 
        avg_growth_rate DESC
    LIMIT 1
),
MonthlyRevenueData AS (
    SELECT
        p.categoryid,
        strftime('%Y-%m', o.order_moment_created) AS month,
        SUM(ol.amount) AS total_revenue
    FROM
        orders o
    JOIN
        orderlines ol ON o.order_id = ol.order_id
    JOIN
        products p ON ol.product_id = p.productid
    WHERE
        o.order_status = 'Completed'
    GROUP BY
        p.categoryid, month
),
MonthlyRevenueGrowth AS (
    SELECT
        MRD1.categoryid,
        MRD1.month,
        ((MRD1.total_revenue - COALESCE(MRD2.total_revenue, 0)) / (COALESCE(MRD2.total_revenue, 1))) * 100 AS revenue_growth_rate
    FROM 
        MonthlyRevenueData MRD1
    LEFT JOIN 
        MonthlyRevenueData MRD2 ON MRD1.categoryid = MRD2.categoryid AND MRD2.month = strftime('%Y-%m', date(MRD1.month || '-01', '-1 month'))
),
AvgMonthlyRevenueGrowthForMaxCategory AS (
    SELECT
        AVG(revenue_growth_rate) AS avg_revenue_growth_rate
    FROM 
        MonthlyRevenueGrowth
    WHERE 
        categoryid = (SELECT categoryid FROM MaxGrowthCategory)
)
SELECT * FROM AvgMonthlyRevenueGrowthForMaxCategory;"
sf_bq260,"From January 1, 2019, to April 30, 2022, how many users are at the youngest age and how many users are at the oldest age for each gender in the e-commerce platform, counting both youngest and oldest users separately for each gender?","WITH AgeData AS (
    SELECT 
        sex,
        MIN(DATE(birth_date)) AS MinBirthDate,
        MAX(DATE(birth_date)) AS MaxBirthDate
    FROM mst_users
    WHERE DATE(register_date) BETWEEN '2019-01-01' AND '2022-04-30'
    GROUP BY sex
)

SELECT
    a.sex,
    'Youngest' AS AgeCategory,
    COUNT(DISTINCT u.user_id) AS UserCount
FROM 
    mst_users u
JOIN 
    AgeData a ON DATE(u.birth_date) = a.MinBirthDate AND u.sex = a.sex
WHERE 
    DATE(u.register_date) BETWEEN '2019-01-01' AND '2022-04-30'
GROUP BY 
    a.sex
UNION ALL
SELECT
    a.sex,
    'Oldest' AS AgeCategory,
    COUNT(DISTINCT u.user_id) AS UserCount
FROM 
    mst_users u
JOIN 
    AgeData a ON DATE(u.birth_date) = a.MaxBirthDate AND u.sex = a.sex
WHERE 
    DATE(u.register_date) BETWEEN '2019-01-01' AND '2022-04-30'
GROUP BY 
    a.sex"
sf_bq261,"For each month prior to January 2024, identify the product that achieved the highest total profit (calculated as the sum of sale_price minus the product’s cost) across all order items, then report the total cost and total profit for that top product per month, including all order items regardless of their status, and present the results chronologically by month.","SELECT 
    olist_order_items.order_id,
    strftime('%Y-%m', olist_order_items.shipping_limit_date) AS month,
    olist_order_items.product_id,
    SUM(olist_order_items.price - product.standardcost) AS total_profit,
    SUM(product.standardcost) AS total_cost
FROM 
    olist_order_items
JOIN 
    product ON olist_order_items.product_id = product.productid
WHERE
    strftime('%Y-%m', olist_order_items.shipping_limit_date) < '2024-01'
GROUP BY 
    month, olist_order_items.product_id
ORDER BY 
    month ASC, total_profit DESC;"
sf_bq262,"Generate a monthly analysis report for e-commerce sales from June 2019 to December 2019 that includes, for each product category and each month, the total number of orders, total revenue, and total profit, along with their month-over-month growth rates using the data from June 2019 as the basis for calculating growth starting from July 2019. Ensure that all orders are included regardless of their status, and present the results sorted in ascending order by month (formatted as ""2019-07"") and then by product category. Omitting June 2019 from the final output but using it for the growth calculations.","WITH MonthlyData AS (
    SELECT
        p.category_id,
        strftime('%Y-%m', o.order_moment_created) AS month,
        COUNT(DISTINCT o.order_id) AS total_orders,
        SUM(o.order_amount) AS total_revenue,
        SUM(o.order_amount - o.order_delivery_cost) AS total_profit
    FROM
        orders o
    JOIN
        orderlines ol ON o.order_id = ol.order_id
    JOIN
        products p ON ol.product_id = p.id
    WHERE
        strftime('%Y-%m', o.order_moment_created) BETWEEN '2019-06' AND '2019-12'
    GROUP BY
        p.category_id, month
),
BaseRates AS (
    SELECT
        category_id,
        total_orders AS base_orders,
        total_revenue AS base_revenue,
        total_profit AS base_profit
    FROM
        MonthlyData
    WHERE
        month = '2019-06'
),
GrowthRates AS (
    SELECT
        m.category_id,
        m.month,
        m.total_orders,
        m.total_revenue,
        m.total_profit,
        (m.total_orders - b.base_orders) / CAST(b.base_orders AS REAL) AS orders_growth_rate,
        (m.total_revenue - b.base_revenue) / CAST(b.base_revenue AS REAL) AS revenue_growth_rate,
        (m.total_profit - b.base_profit) / CAST(b.base_profit AS REAL) AS profit_growth_rate
    FROM
        MonthlyData m
    JOIN
        BaseRates b ON m.category_id = b.category_id
    WHERE
        m.month > '2019-06'
)
SELECT
    category_id,
    month,
    total_orders,
    total_revenue,
    total_profit,
    orders_growth_rate,
    revenue_growth_rate,
    profit_growth_rate
FROM
    GrowthRates
ORDER BY
    month ASC, category_id ASC;"
sf_bq190,"Determine the number of users who are the youngest and oldest for each gender (male and female) separately, among those who signed up between January 1, 2019, and April 30, 2022. For each gender, identify the minimum and maximum ages within this date range, and count how many users fall into these respective age groups.","WITH FilteredUsers AS (
    SELECT 
        user_id, 
        sex,
        birth_date,
        register_date
    FROM 
        mst_users
    WHERE 
        register_date >= '2019-01-01' AND register_date <= '2022-04-30'
),

AgeData AS (
    SELECT 
        sex,
        MIN(birth_date) AS MinBirthDate,
        MAX(birth_date) AS MaxBirthDate
    FROM 
        FilteredUsers
    GROUP BY 
        sex
)

SELECT 
    f.sex,
    'Youngest' AS AgeGroup,
    COUNT(*) AS UserCount
FROM 
    FilteredUsers f
JOIN 
    AgeData a ON f.sex = a.sex AND f.birth_date = a.MinBirthDate
GROUP BY 
    f.sex

UNION ALL

SELECT 
    f.sex,
    'Oldest' AS AgeGroup,
    COUNT(*) AS UserCount
FROM 
    FilteredUsers f
JOIN 
    AgeData a ON f.sex = a.sex AND f.birth_date = a.MaxBirthDate
GROUP BY 
    f.sex;"
sf_bq263,"Please create a month-by-month report for the year 2023 that focuses on the 'Sleep & Lounge' category, showing for each month the total sales, total cost, number of complete orders, total profit, and the profit-to-cost ratio, ensuring that the order is marked as 'Complete,' the creation date is between January 1, 2023, and December 31, 2023, and the cost data is accurately associated with the corresponding product through the order items. ","The provided context is insufficient to generate the requested SQL query because there are no tables linking orders to specific product categories such as 'Sleep & Lounge'. Additionally, tables containing order items or linking orders to product details (with categories) are necessary to determine the total sales, total cost, and profit calculations for specific product categories. Without such linking or relational tables, it is not possible to accurately associate orders with the 'Sleep & Lounge' category or calculate the required financial metrics."
sf_bq264,"Identify the difference in the number of the oldest and youngest users registered between January 1, 2019, and April 30, 2022, from our e-commerce platform data.","WITH AgeData AS (
    SELECT
        MIN(strftime('%Y', birth_date)) AS MinBirthYear,
        MAX(strftime('%Y', birth_date)) AS MaxBirthYear
    FROM
        mst_users
    WHERE
        register_date BETWEEN '2019-01-01' AND '2022-04-30'
)

SELECT
    (SELECT COUNT(*) FROM mst_users WHERE strftime('%Y', birth_date) = AgeData.MaxBirthYear AND register_date BETWEEN '2019-01-01' AND '2022-04-30') AS YoungestUserCount,
    (SELECT COUNT(*) FROM mst_users WHERE strftime('%Y', birth_date) = AgeData.MinBirthYear AND register_date BETWEEN '2019-01-01' AND '2022-04-30') AS OldestUserCount,
    ABS((SELECT COUNT(*) FROM mst_users WHERE strftime('%Y', birth_date) = AgeData.MinBirthYear AND register_date BETWEEN '2019-01-01' AND '2022-04-30')
        - (SELECT COUNT(*) FROM mst_users WHERE strftime('%Y', birth_date) = AgeData.MaxBirthYear AND register_date BETWEEN '2019-01-01' AND '2022-04-30')) AS Difference
FROM
    AgeData;"
sf_bq197,"For each month prior to July 2024, identify the single best-selling product (determined by highest sales volume, with total revenue as a tiebreaker) among all orders with a 'Complete' status and products with non-null brands. Return a report showing the month, product name, brand, category, total sales, rounded total revenue, and order status for these monthly top performers.","The provided context is insufficient to generate the required SQL query. There are several reasons for this:

1. No table or column directly indicates the 'brand' of a product. The available tables do not provide a column that explicitly represents a product's brand.

2. There is no table or column explicitly indicating the 'status' of an order (like 'Complete'). Without this, we cannot filter orders based on their completion status.

3. The request to include the product's 'category' also lacks a direct reference in the provided tables. While there is a 'categoryid' in the 'products' table, there is no corresponding table or column to translate this ID into a readable category name.

4. The requirement to filter data up to ""July 2024"" suggests needing a date or datetime field in the relevant tables, but the connection to this specific timeframe isn't clear, especially with the absence of an order table that includes explicit date information tied to each order.

To proceed, these issues would need resolutions either by modifying the database schema to include these missing pieces or by clarifying the existing data structure to identify how these elements can be derived from the current tables."
sf_bq265,"Can you list the email addresses of the top 10 users who registered in 2019 and made purchases in 2019, ranking them by their highest average order value, where average order value is calculated by multiplying the number of items in each order by the sale price, summing this total across all orders for each user, and then dividing by the total number of orders?","The provided context does not contain any tables or columns related to user registration details, email addresses, or specifics about the number of items directly related to users and their orders in a way that would allow for the calculation of average order value as described. Additional data is required, such as user registration information including email addresses and details linking users to their orders with timestamps and item counts per order."
sf_bq266,"Please provide the names of the products that had sales in each month of 2020 and had the lowest profit, calculated as the difference between their retail price and cost from the products data. Exclude any months where this data isn't available. Please list the products in chronological order based on the month.","WITH MonthlySalesData AS (
    SELECT 
        p.productname,
        s.time_id,
        SUM(s.amount_sold) AS total_revenue,
        SUM(s.quantity_sold * p.unitprice) AS total_cost
    FROM 
        sales s
    JOIN 
        products p ON s.prod_id = p.productid
    WHERE
        strftime('%Y', s.time_id) = '2020'
    GROUP BY 
        s.time_id, p.productid
),
ProfitCalculation AS (
    SELECT 
        productname,
        time_id,
        total_revenue - total_cost AS profit
    FROM 
        MonthlySalesData
),
LowestProfitPerMonth AS (
    SELECT 
        time_id,
        MIN(profit) AS min_profit
    FROM 
        ProfitCalculation
    GROUP BY 
        time_id
),
Result AS (
    SELECT 
        pc.productname,
        lp.time_id
    FROM 
        ProfitCalculation pc
    JOIN 
        LowestProfitPerMonth lp ON pc.time_id = lp.time_id AND pc.profit = lp.min_profit
    ORDER BY 
        pc.time_id
)
SELECT DISTINCT 
    productname,
    time_id
FROM 
    Result
ORDER BY 
    time_id;"
sf_bq333,"Which three browsers have the shortest average session duration—calculated by the difference in seconds between the earliest and latest timestamps for each user’s session—while only including browsers that have more than 10 total sessions, and what are their respective average session durations?","The provided context is insufficient to generate a SQL query for the question. There is no information or table structure provided that includes data about user sessions, timestamps, or browser usage. To answer the question, one would need access to a table that logs user sessions with timestamps and browser identifiers."
sf_bq361,"For the user cohort with a first purchase date in January 2020, what proportion of users returned in the subsequent months of 2020?","WITH FirstPurchase AS (
    SELECT 
        user_id,
        MIN(stamp) AS first_purchase_date
    FROM purchase_log
    GROUP BY user_id
    HAVING strftime('%Y-%m', MIN(stamp)) = '2020-01'
),

-- Next, count the total number of users who made their first purchase in January 2020
TotalFirstUsers AS (
    SELECT 
        COUNT(*) AS total_users
    FROM FirstPurchase
),

-- Then, find all subsequent purchases for these users within 2020
SubsequentPurchases AS (
    SELECT 
        p.user_id,
        strftime('%m', p.stamp) AS purchase_month
    FROM purchase_log p
    INNER JOIN FirstPurchase fp ON p.user_id = fp.user_id
    WHERE 
        p.stamp > fp.first_purchase_date
        AND strftime('%Y', p.stamp) = '2020'
),

-- Calculate the distinct users returning in each month of 2020
MonthlyReturningUsers AS (
    SELECT 
        purchase_month,
        COUNT(DISTINCT user_id) AS returning_users
    FROM SubsequentPurchases
    GROUP BY purchase_month
),

-- Finally, calculate the proportion of returning users for each month
ProportionOfReturningUsers AS (
    SELECT 
        mru.purchase_month,
        mru.returning_users,
        tfu.total_users,
        CAST(mru.returning_users AS REAL) / CAST(tfu.total_users AS REAL) AS return_proportion
    FROM MonthlyReturningUsers mru
    CROSS JOIN TotalFirstUsers tfu
)

SELECT *
FROM ProportionOfReturningUsers
ORDER BY purchase_month;"
sf_bq271,"Please generate a report that, for each month in 2021, provides the number of orders, the number of unique purchasers, and the profit (calculated as the sum of product retail prices minus the sum of product costs), where the orders were placed during 2021 by users who registered in 2021 for inventory items created in 2021, and group the results by the users' country, product department, and product category.","The provided context is insufficient to generate the requested SQL query. The necessary tables for user registration dates, user countries, product departments, and product categories are not available in the provided schema. Additionally, there is a lack of a direct relationship or data point to calculate the ""product costs"" for deriving profit."
sf_bq272,"Please provide the names of the top three most profitable products for each month from January 2019 through August 2022, excluding any products associated with orders that were canceled or returned. For each product in each month, the profit should be calculated as the sum of the sale prices of all order items minus the sum of the costs of those sold items in that month.",SELECT DISTINCT status FROM orders;
sf_bq273,"Can you list the top 5 months from August 2022 to November 2023 where the profit from Facebook-sourced completed orders showed the largest month-over-month increase? Calculate profit as sales minus costs, group by delivery month, and include only orders created between August 2022 and November 2023. Compare each month's profit to its previous month to find the largest increases.","WITH MonthlyProfits AS (
    SELECT
        strftime('%Y-%m', order_moment_created) AS delivery_month,
        SUM(order_amount - order_delivery_cost) AS profit
    FROM
        orders
    WHERE
        channel_id = (SELECT id FROM channels WHERE channel_name = 'Facebook')
        AND order_status = 'completed'
        AND order_moment_created BETWEEN '2022-08-01' AND '2023-11-30'
    GROUP BY
        delivery_month
),
MonthlyProfitChanges AS (
    SELECT
        p1.delivery_month,
        p1.profit - COALESCE(p2.profit, 0) AS profit_change
    FROM
        MonthlyProfits p1
    LEFT JOIN
        MonthlyProfits p2 ON p1.delivery_month = strftime('%Y-%m', date(p2.delivery_month || '-01', '+1 month'))
)

SELECT
    delivery_month,
    profit_change
FROM
    MonthlyProfitChanges
ORDER BY
    profit_change DESC
LIMIT 5;"
sf_bq020,What is the name of the reference sequence with the highest variant density in the given cannabis genome dataset?,"The provided context is insufficient to generate a SQL query that answers the question about the reference sequence with the highest variant density in a cannabis genome dataset. None of the tables listed in the schema contain information related to genetic sequences, variants, or any dataset specifically related to cannabis genomes."
sf_bq107,What is the variant density of the cannabis reference with the longest reference length? Pay attention that a variant is present if there is at least one variant call with a genotype greater than 0.,"The provided context is insufficient to generate the requested SQL query as there are no tables related to cannabis references, reference lengths, or variant calls in the provided schema descriptions. Please provide the relevant table structure or data related to cannabis genetics to proceed with an accurate SQL query generation."
bq025,"Provide a list of the top 10 countries for the year 2020, ordered by the highest percentage of their population under 20 years old. For each country, include the total population under 20 years old, the total midyear population, and the percentage of the population that is under 20 years old.","The provided context is insufficient to generate the requested SQL query. There are no tables in the provided schema containing detailed age-specific demographic data or a breakdown of populations by age groups for each country, which are necessary to calculate the total population under 20 years old and its percentage of the total midyear population for the year 2020."
bq115,Which country has the highest percentage of population under the age of 25 in 2017?,"The context provided does not contain any tables or fields directly related to population age distribution or specific data for the year 2017. Therefore, I cannot generate a SQL query to answer the question about the highest percentage of population under the age of 25 in 2017. Please provide tables that include demographic data by age or a similar dataset for a query to be constructed."
bq030,"As of May 10, 2020, among all countries that had more than 50,000 confirmed COVID-19 cases, which three countries had the highest recovery rates based on the total number of recovered cases relative to their total confirmed cases, and what were their respective recovery rates expressed as percentages?",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
bq018,Which day in March and April had the highest COVID-19 confirmed case growth rate in the United States? The format is MM-DD.,"WITH DailyGrowth AS (
    SELECT 
        strftime('%m-%d', date) AS day,
        SUM(total_cases) AS cases_on_day,
        LAG(SUM(total_cases)) OVER (ORDER BY date) AS cases_previous_day
    FROM statistics
    WHERE strftime('%m', date) IN ('03', '04') AND state = 'United States'
    GROUP BY date
),
GrowthRates AS (
    SELECT 
        day, 
        (cases_on_day - cases_previous_day) / cases_previous_day * 100 AS growth_rate
    FROM DailyGrowth
    WHERE cases_previous_day IS NOT NULL
)
SELECT day
FROM GrowthRates
WHERE growth_rate = (
    SELECT MAX(growth_rate)
    FROM GrowthRates
);"
bq086,"You need to calculate the percentage of each country's population that had been confirmed with COVID-19 by June 30, 2020. The population data for 2018 can be found in the World Bank dataset, and the cumulative COVID-19 confirmed cases data is available in the COVID-19 Open Data dataset. Calculate the percentage of each country's population, that was cumulatively confirmed to have COVID-19","The provided context is insufficient to generate the required SQL query. The tables described do not include population data or a specific cumulative dataset for COVID-19 confirmed cases as of June 30, 2020. Additionally, there are no links to external datasets like the World Bank dataset for population or the COVID-19 Open Data dataset. To proceed, you would need to have tables containing the relevant population data and specific COVID-19 cases data up to the date mentioned."
bq085,"Could you provide, for the United States, France, China, Italy, Spain, Germany, and Iran, the total number of confirmed COVID-19 cases as of April 20, 2020, along with the number of cases per 100,000 people based on their total 2020 populations calculated by summing all relevant population entries from the World Bank data","SELECT 
    s.state,
    SUM(s.total_cases) AS total_confirmed_cases,
    (SUM(s.total_cases) / MAX(c.population) * 100000) AS cases_per_100k_population
FROM 
    statistics s
JOIN 
    cities_countries cc ON s.state = cc.country_name
JOIN 
    countries c ON cc.country_code_2 = c.country_iso_code
WHERE 
    s.date = '2020-04-20'
    AND s.state IN ('United States', 'France', 'China', 'Italy', 'Spain', 'Germany', 'Iran')
GROUP BY 
    s.state;"
bq130,"Analyze daily new COVID-19 case counts from March to May 2020, identifying the top five states by daily increases. Please compile a ranking based on how often each state appears in these daily top fives. Then, examine the state that ranks fourth overall and identify its top five counties based on their frequency of appearing in the daily top five new case counts.","WITH daily_new_cases AS (
    SELECT 
        date,
        state,
        total_cases - LAG(total_cases, 1, 0) OVER (PARTITION BY state ORDER BY date) AS new_cases
    FROM statistics
    WHERE date BETWEEN '2020-03-01' AND '2020-05-31'
    AND total_cases IS NOT NULL
),

-- Table to find top 5 states by new cases per day
daily_top_five AS (
    SELECT 
        date,
        state,
        RANK() OVER (PARTITION BY date ORDER BY new_cases DESC) AS rank
    FROM daily_new_cases
),
top_five_states AS (
    SELECT 
        state,
        COUNT(*) AS frequency
    FROM daily_top_five
    WHERE rank <= 5
    GROUP BY state
    ORDER BY frequency DESC
),

-- Find the state that ranks fourth
fourth_state AS (
    SELECT state
    FROM top_five_states
    LIMIT 1
    OFFSET 3
),

-- Intermediate SQL to create the daily new cases table per county
county_daily_new_cases AS (
    SELECT 
        date,
        state,
        county_city_location AS county,
        total_cases - LAG(total_cases, 1, 0) OVER (PARTITION BY state, county_city_location ORDER BY date) AS new_cases
    FROM statistics
    WHERE date BETWEEN '2020-03-01' AND '2020-05-31'
    AND total_cases IS NOT NULL
    AND state = (SELECT state FROM fourth_state)
),

-- Table to find top 5 counties by new cases per day
daily_county_top_five AS (
    SELECT 
        date,
        county,
        RANK() OVER (PARTITION BY date ORDER BY new_cases DESC) AS rank
    FROM county_daily_new_cases
),
top_five_counties AS (
    SELECT 
        county,
        COUNT(*) AS frequency
    FROM daily_county_top_five
    WHERE rank <= 5
    GROUP BY county
    ORDER BY frequency DESC
    LIMIT 5
)

SELECT * FROM top_five_counties;"
bq087,"Please calculate the overall percentage change in the average weekly search frequency for the symptom 'Anosmia' across the five New York City counties—Bronx County, Queens County, Kings County, New York County, and Richmond County—by comparing the combined data from January 1, 2019, through December 31, 2019, with the combined data from January 1, 2020, through December 31, 2020.","The provided context is insufficient to generate the requested SQL query because there is no table or dataset specified that includes search frequency data for symptoms, including 'Anosmia', nor is there any table that explicitly provides information related to search frequencies or dates. Please provide or specify the relevant table and data structure that includes the necessary details to perform this analysis."
bq088,"Please calculate the average levels of anxiety and depression symptoms from the weekly country data for the United States during the periods from January 1, 2019, to January 1, 2020, and from January 1, 2020, to January 1, 2021. Then, compute the percentage increase in these average symptom levels from the 2019 period to the 2020 period.",The provided context is insufficient to generate the requested SQL query as there is no table containing information related to levels of anxiety and depression symptoms. Please provide the relevant table or dataset that includes this specific information.
bq089,"Given the latest population estimates from the 2018 five-year American Community Survey, what is the number of vaccine sites per 1000 people for counties in California?",The provided context is insufficient to generate a SQL query to answer the question about the number of vaccine sites per 1000 people for counties in California. There is no table or data related to vaccine sites or the latest population estimates from the 2018 five-year American Community Survey in the given schema.
bq407,"Find the top three counties with populations over 50,000, using the 2020 5-year census data, that had the highest COVID-19 case fatality rates on August 27, 2020. For these counties, provide the name, state, median age, total population, number of confirmed COVID-19 cases per 100,000 people, number of deaths per 100,000 people, and the case fatality rate as a percentage","The provided context is insufficient to generate the desired SQL query because there is no table with the required 2020 5-year census data containing median age, total population, and there is no direct relation between these demographic details and the COVID-19 statistics in the provided tables. Additionally, the data necessary to calculate the rate of confirmed COVID-19 cases per 100,000 people and the number of deaths per 100,000 people is not available in the provided schema. Further, there is no table containing historical COVID-19 data linked to specific dates such as August 27, 2020, to calculate daily case or death rates."
bq137,"Please find all zip code areas located within 10 kilometers of the coordinates (-122.3321, 47.6062) by joining the 2010 census population data (summing only male and female populations with no age constraints) and the zip code area information, and return each area’s polygon, land and water area in meters, latitude and longitude, state code, state name, city, county, and total population.","The provided context is insufficient to generate the requested SQL query. The necessary table containing 2010 census population data including population details by gender and no age constraints, as well as the table containing zip code area polygon, land and water area in meters, is not provided in the context. Additionally, there is no table mentioned which includes all the specific details such as state code, state name, county, and total population linked with zip codes."
bq060,Which top 3 countries had the highest net migration in 2017 among those with an area greater than 500 square kilometers? And what are their migration rates?,"The provided tables and context do not contain any information regarding net migration, migration rates, or the area of countries for any year, including 2017. Therefore, it is not possible to generate a SQL query to answer the question based on the current database schema."
bq338,"Can you find the census tracts in the 36047 area that are among the top 20 for the largest percentage increases in population from 2011 to 2018, are also among the top 20 for the largest absolute increases in median income during the same period, and had over 1,000 residents in each of those years?","The provided context is insufficient to generate the SQL query because there is no table containing data about population changes by census tract over the years, or specific data for the years 2011 and 2018. Additionally, the table structure provided does not include any information about census tracts or detailed yearly population and income records. Please provide the necessary table structures or data details to proceed with generating the required SQL query."
bq061,Which census tract has witnessed the largest increase in median income between 2015 and 2018 in California? Tell me the tract code.,"The provided context is insufficient to generate the requested SQL query because there is no table containing data specific to census tracts or data that tracks median income changes over the years, specifically for the years 2015 and 2018. Additionally, no table contains state-specific tract-level data, which is necessary to filter for California."
bq064,"Using the 2017 U.S. Census Tract data from the BigQuery public datasets, you need to proportionally allocate each tract's population and income to the zip codes based on the overlapping area between their geographic boundaries. Then, filter the results to include only those zip codes located within a 5-mile radius of a specific point in Washington State, with coordinates at latitude 47.685833°N and longitude -122.191667°W. Finally, calculate the total population and the average individual income for each zip code (rounded to one decimal place) and sort the results by the average individual income in descending order.","The provided context is insufficient to generate the requested SQL query for the following reasons:

1. **Lack of Geographic Data:** The SQL tables provided do not contain any geographic boundary data or spatial data functions necessary to calculate overlaps between census tracts and zip codes. The tables also lack population data by tract.

2. **BigQuery Public Datasets:** The question references data and operations (spatial calculations and specific census data) typically handled in a BigQuery environment with specific datasets not provided in the table structures given.

3. **Geographic Calculation Requirements:** The task requires performing geographic proximity calculations and spatial joins which are not feasible with the provided tables as they lack the necessary geographic coordinates or boundary definitions for tracts and zip codes, as well as spatial functions.

4. **Specific Census Data:** The question asks for 2017 U.S. Census Tract data which is not included in any of the provided tables.

To proceed with such a query, access to a spatial database with census tract boundaries, population, income data, and spatial functions to calculate overlaps and distances would be required. Additionally, integration with a platform supporting GIS queries (such as PostgreSQL with PostGIS or BigQuery GIS) would be necessary."
bq461,"Please provide a chronological summary of all scoring plays from the 2014 season game where the Wildcats were the home team and the Fighting Irish were the away team. Include for each scoring event the game clock, cumulative scores for both teams (Wildcats and Fighting Irish), the team that scored, and a description of the event.","The provided context is insufficient to generate the requested SQL query. There is no table containing detailed event or play-by-play information, such as scoring events, game clocks, or descriptions of events for specific games involving teams named ""Wildcats"" and ""Fighting Irish."" Additionally, none of the tables contain team names or detailed match event data necessary to fulfill the request."
bq198,"List the top 5 universities with the most seasons where they achieved the maximum wins in their respective NCAA basketball seasons between 1900-2000, showing each team's total number of such peak-performance seasons, while excluding entries with missing team names.",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
bq462,"Please generate a table from the NCAA basketball dataset that lists the top five records in each of these four categories: (1) Top Venues - the largest venues by seating capacity with Date shown as 'N/A'; (2) Biggest Championship Margins - National Championship games since the 2016 season (season > 2015) with the biggest point margin victories; (3) Highest Scoring Games - games since the 2011 season (season > 2010) with the highest total points scored by both teams combined; and (4) Total Threes - games since the 2011 season (season > 2010) with the highest total three-pointers made by both teams combined. The final table should be organized with columns for Category, Date, Matchup or Venue, and Key Metric, with each category's 5 records presented in descending order of their key metric.","The provided context does not include any information about the NCAA basketball dataset, nor does it contain details about venue capacities, point margins in championship games, total points scored in games, or three-pointers made in games. The existing tables do not cover basketball-specific statistics or details about venues and their capacities. Therefore, it's not possible to generate the requested SQL query with the current dataset."
bq427,"Could you determine, for each shot type, the average x and y coordinates (adjusted to ensure consistency regarding the left or right basket), the average number of shot attempts, and the average number of successful shots, considering only shots taken before March 15, 2018, excluding those with null shot types or coordinates, ensuring the shots are on the correct side of the court based on the team's basket.","The provided context is insufficient to generate the SQL query as requested because there is no table in the provided schema that contains details about basketball shots, such as shot types, x and y coordinates, or the success of the shots. Additionally, there is no information about the team's basket or how to determine the correct side of the court for shots. More specific table and column details regarding basketball game statistics are necessary to fulfill this request."
bq428,"For the top five team markets with the highest number of distinct players who scored at least 15 points during the second period of games between 2010 and 2018, provide details of each game they played in NCAA basketball historical tournament matches during the same period, as specified in the data model document.","The provided context and the tables described do not include any data related to basketball games, player scoring, periods of games, or NCAA basketball historical tournament matches. Therefore, it is impossible to generate a valid SQL query based on the current database schema provided, as it pertains to baseball and does not contain the necessary data for basketball game statistics."
bq144,"Create a dataset by combining NCAA men's basketball tournament game outcomes from the 2014 season onwards, including both the historical tournament games and the 2018 tournament results, with the corresponding pace and efficiency performance metrics for each team and their opponents from the feature_engineering data. The dataset should include the season, game outcome labels (win or loss), team and opponent seeds, school names, pace and efficiency rankings, statistical values, and the differences between the team's and the opponent's metrics to enable a comprehensive analysis of team and opponent dynamics.","The provided context is insufficient to generate the requested SQL query. There are no tables related to NCAA men's basketball, nor are there any tables containing data on pace, efficiency performance metrics, team seeds, or school names. Additionally, there is no table named `feature_engineering` mentioned in the schema provided. Therefore, I cannot create the SQL query as requested without additional information or access to the relevant data tables."
bq113,"Which county in Utah experienced the greatest percentage increase in construction employment from 2000 to 2018, calculated by averaging the employment levels during the third month of each quarter in those years? What is the corresponding percentage increase?","The provided context is insufficient to generate a SQL query because there are no tables containing information about employment levels in different counties in Utah, construction employment, or data that specifically records monthly or quarterly employment statistics for the years 2000 to 2018."
bq112,"Between 1998 and 2017, for Allegheny County in the Pittsburgh area, did the average annual wages for all industries keep pace with the inflation of all consumer items, and what were the respective percentage growth rates (to two decimal places) for wages and the CPI over that period?","The provided context is insufficient to generate a SQL query to answer the question about average annual wages for all industries and the inflation of all consumer items in Allegheny County between 1998 and 2017. The tables mentioned do not include any data on wages by geographic location or consumer price index (CPI) data. Further, none of the tables contain specific regional economic data or inflation indices required to calculate percentage growth rates for wages and CPI."
bq055,"Can you provide the top three races with the largest percentage differences between Google's 2021 overall hiring data from dar non intersectional hiring and the average percentages in the 2021 BLS data for the technology sectors specifically defined as 'Internet publishing and broadcasting and web search portals,' 'Software publishers,' 'Data processing, hosting, and related services,' or the industry group 'Computer systems design and related services,' along with their respective differences?","The provided context and the existing database schema do not contain information related to hiring data, Google's hiring practices, or average percentages from the BLS (Bureau of Labor Statistics) data for specific technology sectors. Therefore, it's not possible to generate an SQL query to fulfill the request based on the current database structure provided."
bq075,"Could you provide a combined 2021 report comparing racial (Asian, Black, Hispanic/Latinx, White) and gender (U.S. Women, U.S. Men) distributions across Google’s overall workforce hiring, Google’s overall workforce representation, and the BLS data specifically for the technology sectors defined as Internet publishing and broadcasting and web search portals or Computer systems design and related services?","The provided context is insufficient to generate the requested SQL query. There is no data available in the tables that contain information about workforce hiring, workforce representation, or specific employment statistics from the Bureau of Labor Statistics (BLS) for Google or any technology sectors. Therefore, I'm unable to create a SQL query based on the given tables to fulfill this specific request."
bq406,"Please calculate the growth rates for Asians, Black people, Latinx people, Native Americans, White people, US women, US men, global women, and global men from 2014 to 2024 concerning the overall workforce.","The provided context is insufficient to generate a SQL query for calculating growth rates for specific demographic groups in the workforce from 2014 to 2024. The given tables do not contain any data related to demographics, workforce composition, or time-series data tracking changes in workforce demographics over the specified years."
sf_bq084,"For each month in the year 2023, how many total transactions occurred (counting all transaction records without removing duplicates of transaction hashes), and how many transactions per second were processed each month, where the transactions-per-second value is calculated by dividing the monthly total count by the exact number of seconds in that month, including the correct leap-year logic if applicable based on the extracted year from the transaction timestamp? Show the monthly transaction count, the computed transactions per second, the year, and the month, and present the rows in descending order of the monthly transaction count.","WITH MonthSeconds AS (
    SELECT
        calendar_month_number AS month,
        CASE
            WHEN calendar_month_number IN (1, 3, 5, 7, 8, 10, 12) THEN 31 * 24 * 60 * 60
            WHEN calendar_month_number IN (4, 6, 9, 11) THEN 30 * 24 * 60 * 60
            WHEN calendar_month_number = 2 AND (calendar_year % 4 = 0 AND (calendar_year % 100 != 0 OR calendar_year % 400 = 0)) THEN 29 * 24 * 60 * 60 -- Leap year February
            ELSE 28 * 24 * 60 * 60 -- Non-leap year February
        END AS seconds_in_month,
        calendar_year
    FROM times
    WHERE calendar_year = 2023
    GROUP BY calendar_month_number, calendar_year
),
TransactionCounts AS (
    SELECT
        strftime('%m', txn_date) AS txn_month,
        strftime('%Y', txn_date) AS txn_year,
        COUNT(*) AS transaction_count
    FROM bitcoin_transactions
    WHERE strftime('%Y', txn_date) = '2023'
    GROUP BY txn_month, txn_year
)
SELECT
    tc.txn_month AS month,
    tc.txn_year AS year,
    tc.transaction_count AS total_transactions,
    CAST(tc.transaction_count AS FLOAT) / ms.seconds_in_month AS transactions_per_second
FROM TransactionCounts tc
JOIN MonthSeconds ms ON tc.txn_month = ms.month AND tc.txn_year = ms.calendar_year
ORDER BY tc.transaction_count DESC;"
sf_bq058,"Retrieve all finalized deposits into Optimism at block 29815485 using the Optimism Standard Bridge, including transaction hash, an Etherscan link (the complete URL), L1 and L2 token addresses, sender and receiver addresses (with leading zeroes stripped), and the deposited amount (converted from hex to decimal). Ensure data is properly formatted and parsed according to Optimism's address and token standards, and remove the prefix '0x' except transaction hash. Note that, the keccak-256 hash of the Ethereum event signature for DepositFinalized is ""0x3303facd24627943a92e9dc87cfbb34b15c49b726eec3ad3487c16be9ab8efe8"".","The provided context and tables do not contain information about blockchain transactions, Optimism Standard Bridge data, Ethereum event signatures, or any blockchain-related data such as block numbers, transaction hashes, or addresses in hex format. Therefore, it is not possible to generate a SQL query to retrieve the requested information about finalized deposits into Optimism at a specific block. Additional data containing blockchain transaction details and relationships to Ethereum event signatures would be required."
sf_bq416,"Could you retrieve the top three largest USDT transfers on the TRON blockchain by listing the block numbers, source addresses, destination addresses (in TronLink format), and transfer amounts, using the USDT contract address '0xa614f803b6fd780986a42c78ec9c7f77e6ded13c' and the transfer event signature '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef', dividing the raw transfer value by 1,000,000 to convert it into the final USDT amount, and then ordering the results by the largest transferred amounts first?","The provided context is insufficient to generate the requested SQL query as there are no tables or columns available in the schema related to blockchain transactions, block numbers, source addresses, destination addresses, or contract addresses. The schema provided does not include any data related to the TRON blockchain or specific cryptocurrency transactions as described. To proceed, access to a relevant database that includes this specific blockchain transaction data is required."
sf_bq226,"Which sender address, represented as a complete URL on https://cronoscan.com, has been used most frequently on the Cronos blockchain in transactions to non-null 'to_address' fields, within blocks larger than 4096 bytes, since January 1, 2023?","The provided context is insufficient to generate the required SQL query because there are no tables or fields in the schema related to blockchain transactions, 'to_address' fields, block sizes, or any specific blockchain data like those on the Cronos blockchain. A relevant table containing transaction data with columns for sender address, 'to_address', block size, and transaction date is necessary to construct the query."
sf_bq016,"Considering only the highest release versions of NPM packages, which dependency (package and its version) appears most frequently among the dependencies of these packages?","The provided context does not contain any information regarding NPM packages, their versions, or their dependencies. Therefore, it's impossible to generate a SQL query based on the current database schema provided. To answer the question about NPM packages and their dependencies, a table containing this specific data would be required."
sf_bq062,What is the most frequently used license by packages in each system?,The provided context is insufficient to generate the SQL query as it does not contain any information about licenses or systems associated with the packages. There are no tables described that include data about licenses or their relationships with systems or packages. Please provide the necessary table structure or details to proceed with generating the query.
sf_bq063,"Find the GitHub URL (with link label 'SOURCE_REPO') of the latest released version of the NPM package that has the highest number of dependencies in its latest released version, excluding packages whose names contain the character '@' and only considering URLs where the link label is 'SOURCE_REPO' and the URL contains 'github.com'.","The provided tables do not contain any information regarding NPM packages, their versions, dependencies, or associated URLs. Therefore, it's not possible to generate a SQL query to answer the question based on the given context."
sf_bq028,"Considering only the latest release versions of NPM package, which packages are the top 8 most popular based on the Github star number, as well as their versions?","The provided context is insufficient to generate a SQL query for the requested data. There are no tables mentioned in the context that contain information about NPM packages, their versions, or Github star numbers. This data is necessary to fulfill the query requirements."
bq022,"Calculate the minimum and maximum trip duration in minutes (rounded to the nearest whole number), total number of trips, and average fare for each of six equal quantile groups based on trip duration, considering only trips between 0 and 60 minutes.","WITH TripDurations AS (
    SELECT
        flight_id,
        EXTRACT(EPOCH FROM (actual_arrival - actual_departure)) / 60 AS duration_minutes,
        amount
    FROM
        flights
    JOIN
        ticket_flights USING (flight_id)
    WHERE
        actual_departure IS NOT NULL
        AND actual_arrival IS NOT NULL
        AND EXTRACT(EPOCH FROM (actual_arrival - actual_departure)) / 60 BETWEEN 0 AND 60
),
Quantiles AS (
    SELECT
        duration_minutes,
        amount,
        NTILE(6) OVER (ORDER BY duration_minutes) AS quantile_group
    FROM
        TripDurations
)
SELECT
    quantile_group,
    ROUND(MIN(duration_minutes)) AS min_duration,
    ROUND(MAX(duration_minutes)) AS max_duration,
    COUNT(*) AS total_trips,
    ROUND(AVG(amount), 2) AS average_fare
FROM
    Quantiles
GROUP BY
    quantile_group
ORDER BY
    quantile_group;"
bq362,Which three companies had the largest increase in trip numbers between two consecutive months in 2018?,The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
bq363,"Calculate the total number of trips and average fare (formatted to two decimal places) for ten equal-sized quantile groups. Each quantile group should contain approximately the same number of trips based on their rounded trip duration in minutes (between 1-50 minutes). Display each group's time range formatted as ""XXm to XXm"" (where the numbers are zero-padded to two digits), the total trips count, and the average fare. The time ranges should represent the minimum and maximum duration values within each quantile. Sort the results chronologically by time range. Use NTILE(10) to create the quantiles from the ordered trip durations.","WITH DurationRounded AS (
    SELECT
        ticket_no,
        FLOOR(EXTRACT(EPOCH FROM (actual_arrival - actual_departure)) / 60) AS duration_minutes,
        amount
    FROM
        flights
    JOIN
        ticket_flights ON flights.flight_id = ticket_flights.flight_id
    WHERE
        actual_departure IS NOT NULL AND actual_arrival IS NOT NULL
        AND FLOOR(EXTRACT(EPOCH FROM (actual_arrival - actual_departure)) / 60) BETWEEN 1 AND 50
),
QuantileGroups AS (
    SELECT
        duration_minutes,
        amount,
        NTILE(10) OVER (ORDER BY duration_minutes) AS quantile
    FROM
        DurationRounded
),
AggregatedData AS (
    SELECT
        quantile,
        MIN(duration_minutes) AS min_duration,
        MAX(duration_minutes) AS max_duration,
        COUNT(*) AS total_trips,
        ROUND(AVG(amount), 2) AS avg_fare
    FROM
        QuantileGroups
    GROUP BY
        quantile
)
SELECT
    LPAD(min_duration::TEXT, 2, '0') || 'm to ' || LPAD(max_duration::TEXT, 2, '0') AS time_range,
    total_trips,
    avg_fare
FROM
    AggregatedData
ORDER BY
    min_duration;"
bq076,What is the highest number of motor vehicle theft incidents that occurred in any single month during 2016?,"The provided tables do not contain any explicit information or a direct indication of data related to motor vehicle theft incidents. Therefore, I cannot generate a SQL query to answer the question about the highest number of motor vehicle theft incidents in any single month during 2016 without additional context or a relevant table containing data on theft incidents."
bq077,"For each year from 2010 to 2016, what is the highest number of motor thefts in one month?","The provided context does not include any tables that contain data related to ""motor thefts"" or any general crime statistics including thefts. Therefore, I'm unable to generate a SQL query to answer the question regarding the highest number of motor thefts in one month for each year from 2010 to 2016.

To proceed, a table containing records of motor thefts with fields indicating the date (or month and year) and incident count is required. Without such data, it is not possible to create the requested SQL query."
bq350,"For the detailed molecule data, Please display the drug id, drug type and withdrawal status for approved drugs with a black box warning and known drug type among 'Keytruda', 'Vioxx', 'Premarin', and 'Humira'","The provided context does not include any tables related to drugs, drug types, or withdrawal statuses. Therefore, it's impossible to generate a valid SQL query based on the current database schema information provided. Please provide the relevant table schema or details to proceed with this request."
bq379,Which target approved symbol has the overall association score closest to the mean score for psoriasis?,"The provided context is insufficient to generate a SQL query as there are no tables or columns related to ""target approved symbol"", ""overall association score"", or ""psoriasis"" in the provided table schemas. You may need a specific dataset that contains such medical or biochemical data to answer this question."
bq078,Retrieve the approved symbol of target genes with the highest overall score that are associated with the disease 'EFO_0000676' from the data source 'IMPC'.,"The provided context is insufficient to generate the requested SQL query because there are no tables described in the schema related to genes, gene scores, diseases, or data sources like 'IMPC'. Please provide the relevant table structures or data schema to proceed with the query."
bq095,"Generate a list of drugs from the table containing molecular details that have completed clinical trials for pancreatic endocrine carcinoma, disease ID EFO_0007416. Please include each drug's name, the target approved symbol, and links to the relevant clinical trials.","The provided context is insufficient to generate the requested SQL query. There are no tables described in the details that contain information about drugs, molecular details, clinical trials, or specific diseases such as pancreatic endocrine carcinoma with disease ID EFO_0007416."
bq109,"Find the average, variance, max-min difference, and the QTL source(right study) of the maximum log2(h4/h3) for data where right gene id is ""ENSG00000169174"", h4 > 0.8, h3 < 0.02, reported trait includes ""lesterol levels"", right biological feature is ""IPSC"", and the variant is '1_55029009_C_T'.","The provided context is insufficient to generate the requested SQL query because there is no information about a table or tables that contain fields such as `log2(h4/h3)`, `right gene id`, `h4`, `h3`, `reported trait`, `right biological feature`, and `variant`. These specific data fields are not referenced in any of the provided table schemas. Therefore, it is not possible to write an accurate SQL query without additional information about the relevant database schema that includes these fields."
sf_bq325,"Please identify the top 10 genes with the strongest associations across all studies by first selecting, for each gene within each study, the variant with the lowest p-value, and then ranking all such gene–variant pairs to return the 10 genes with the smallest p-values overall.","The provided context does not contain any information about genes, variants, or p-values in the described tables. Therefore, it's not possible to generate the requested SQL query based on the current database schema. Please provide the relevant table structure or details that include genes, variants, and p-values to proceed with the SQL query."
bq090,How much higher the average intrinsic value is for trades using the feeling-lucky strategy compared to those using the momentum strategy under long-side trades?,"The provided context is insufficient to generate a SQL query as there is no information about any table or column related to trading strategies, intrinsic values, or trade types (like ""feeling-lucky"" or ""momentum"" strategies, or ""long-side"" trades). These details are essential to create a relevant SQL query."
bq442,Please collect the information of the top 6 trade report with the highest closing prices. Refer to the document for all the information I want.,"SELECT *
FROM bitcoin_prices
ORDER BY price DESC
LIMIT 6;"
bq079,"Considering only the latest evaluation group per state for the 'EXPCURR' evaluation type, determine which state has the highest total acreage of timberland and which has the highest total acreage of forestland. For timberland, include plots where the condition status code is 1, the reserved status code is 0, and the site productivity class code is between 1 and 6. For forestland, include plots where the condition status code is 1. Calculate the total acres by summing the adjusted expansion factors for macroplots and subplots, using their respective proportion bases ('MACR' for macroplots and 'SUBP' for subplots) and adjustment factors when greater than zero. For each category (timberland and forestland), provide the state code, evaluation group, state name, and the total acres for the state with the highest total acreage, considering only the latest evaluation group per state.","The provided context is insufficient to generate the requested SQL query. There are no tables in the provided schema related to evaluation groups, evaluation types such as 'EXPCURR', timberland, forestland, condition status codes, reserved status codes, site productivity class codes, adjusted expansion factors, macroplots, subplots, or any related attributes such as state codes or state names directly tied to these criteria.

Additional information about the relevant database schema including the specific tables and columns that store these details is required to create an appropriate SQL query."
bq024,"For the year 2012, which top 10 evaluation groups have the largest subplot acres when considering only the condition with the largest subplot acres within each group? Please include the evaluation group, evaluation type, condition status code, evaluation description, state code, macroplot acres, and subplot acres.","The provided context is insufficient to generate the requested SQL query. The tables described do not contain any information about evaluation groups, evaluation types, condition status codes, evaluation descriptions, state codes, macroplot acres, or subplot acres. Therefore, it's not possible to answer the question with the current database schema. Please provide the appropriate table structure or additional context that includes these details."
bq220,"Based on the condition, plot_tree, and population tables in bigquery-public-data.usfs_fia, for the evaluation_type set to 'EXPCURR' and condition_status_code equal to 1, which states had the largest average subplot size and the largest average macroplot size, respectively, for each of the years 2015, 2016, and 2017? Please include the type of plot (subplot or macroplot), the specific year, the state, and the corresponding average size in your results.","The provided context does not contain any information or structure about tables named `plot_tree`, `condition`, or `population` from the bigquery-public-data.usfs_fia dataset. Additionally, there's no indication of the schema for these tables or the relevant columns such as `evaluation_type`, `condition_status_code`, `subplot size`, and `macroplot size`. Without this information, it's not possible to generate a valid SQL query. Please provide the schema and relevant details for these tables, or access to the dataset might be needed to retrieve this information."
bq096,"Determine which year had the earliest date after January on which more than 10 sightings of Sterna paradisaea were recorded north of 40 degrees latitude. For each year, find the first day after January with over 10 sightings of this species in that region, and identify the year whose earliest such date is the earliest among all years.","The provided context is insufficient to generate the requested SQL query as there is no table containing information about species sightings, dates, quantities, and geographical coordinates such as latitude. A table with this specific data is essential to query for sightings of ""Sterna paradisaea"" north of 40 degrees latitude and to calculate counts and dates as described."
sf_bq276,"Can you provide a comprehensive list of all ports in region number 6585 that lie within U.S. state boundaries and have been affected by named storms in the North Atlantic basin with wind speeds of at least 35 knots and a Saffir-Simpson classification of at least minimal tropical storm strength, including for each port its name, the state name, the distinct years in which storms occurred, the total count of distinct storms, the distinct storm names, the average storm category, the average wind speed, and the respective geometries for both the port and the tropical storm areas?","The provided context does not contain any tables or information regarding ports, named storms, storm categories, wind speeds, or geometries for ports and storm areas. Therefore, it is not possible to generate a SQL query for the request, as the necessary data and tables are not available in the given schema."
bq277,"Which single port, listed under region number '6585', is located within a U.S. state boundary and appears most frequently inside the geographic areas of named tropical storms with wind speeds of at least 35 knots in the North Atlantic basin, excluding those labeled 'NOT_NAMED'?","The provided context is insufficient to generate a SQL query. The tables provided do not contain information about ports, tropical storms, wind speeds, or specifics about geographic areas within the North Atlantic basin. Therefore, I cannot generate a query without additional data that links ports to storm occurrences and their specific characteristics (like wind speed and naming)."
bq278,"Please provide a detailed comparison of the solar potential for each state, distinguishing between postal code and census tract levels. For each state, include the total number of buildings available for solar installations, the average percentage of Google Maps area covered by Project Sunroof, the average percentage of that coverage which is suitable for solar, the total potential panel count, the total kilowatt capacity, the energy generation potential, the carbon dioxide offset, the current number of buildings with solar panels, and the gap in potential installations calculated by adjusting the total qualified buildings with the coverage and suitability percentages and subtracting the current installations.","The provided context is insufficient to generate the SQL query requested. The tables described do not contain information on solar potential, Google Maps coverage, Project Sunroof data, energy generation potential, carbon dioxide offset, or buildings with solar panels. Additional data sources containing these specific details would be required to construct the query."
bq102,"Identify which start positions are associated with missense variants in the BRCA1 gene on chromosome 17, where the reference base is 'C' and the alternate base is 'T'. Using data from the gnomAD v2.1.1 version.","The provided context and table structures do not include any information about genetic data, genomic variants, or specific databases like gnomAD v2.1.1. Therefore, I am unable to generate a SQL query based on the current tables as they do not contain the necessary information about genomic data or variants related to the BRCA1 gene on chromosome 17."
bq445,"Using the gnomAD v2.1.1 genomes data for chromosome 17, determine the smallest start position and largest end position of any variant whose nested VEP annotations contain the symbol 'BRCA1'. Then, for all variants whose positions fall within that gene region, retrieve the 'Protein_position' values only if the 'Consequence' includes 'missense_variant', sort them in ascending order by 'Protein_position', and finally output the first such result.","The provided context is insufficient to generate the SQL query for the following reasons:

1. There is no table or schema information related to gnomAD v2.1.1 genomes data or VEP annotations in the provided context.
2. The database schema and fields required to access genomic data, such as start position, end position, and nested annotations for VEP (including 'Consequence', 'Protein_position', and 'BRCA1'), are not specified in the given tables.
3. Since genomic data and its specific annotations are not part of the provided database schema, it's not possible to construct a query to fulfill the request based on the current information.

Additional information about the relevant database schema including tables and columns that contain genomic data and VEP annotations would be necessary to generate the requested SQL query."
bq103,"Generate summary statistics on genetic variants in the region between positions 55039447 and 55064852 on chromosome 1. This includes the number of variants, the total allele count, the total number of alleles, and distinct gene symbols (using Variant Effect Predictor, VEP, for gene annotation). Additionally, compute the density of mutations by dividing the length of the region by the number of variants.  Using data from the gnomAD v3 version.","The provided context is insufficient to generate the requested SQL query because there is no table or dataset information related to genetic variants, gnomAD data, or any genomic data provided in the context. To execute such a query, access to a specific genomic database containing chromosome data, gene annotations, and variant information would be necessary."
sf_bq104,"Based on the most recent refresh date, identify the top-ranked rising search term for the week that is exactly one year prior to the latest available week in the dataset.",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
sf_bq411,"Please retrieve the top three Google Trends search terms (ranks 1, 2, and 3) from top_terms for each weekday (Monday through Friday) between September 1, 2024, and September 14, 2024, grouped by the refresh_date column and ordered in descending order of refresh_date.","The provided context is insufficient to generate the requested SQL query as there is no information about a table named ""top_terms"" or columns such as ""refresh_date"" and ""search_terms"". Please provide the schema for the relevant table or details about the columns to proceed."
bq105,"According to the 2015 and 2016 accident and driver distraction, and excluding cases where the driver’s distraction status is recorded as 'Not Distracted,' 'Unknown if Distracted,' or 'Not Reported,' how many traffic accidents per 100,000 people were caused by driver distraction in each U.S. state for those two years, based on 2010 census population data, and which five states each year had the highest rates?",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
bq108,"Within the 2015 dataset for accidents that occurred from January through August and involved more than one distinct person, what percentage of these accidents had more than one individual with a severe injury (injury severity = 4)","WITH RelevantCases AS (
    SELECT c.case_id
    FROM collisions c
    WHERE strftime('%Y', c.collision_date) = '2015'
      AND strftime('%m', c.collision_date) BETWEEN '01' AND '08'
      AND c.party_count > 1
),
SevereInjuryCases AS (
    SELECT v.case_id
    FROM victims v
    JOIN RelevantCases rc ON v.case_id = rc.case_id
    WHERE v.victim_degree_of_injury = '4'
    GROUP BY v.case_id
    HAVING COUNT(DISTINCT v.id) > 1
)
SELECT 
    (COUNT(DISTINCT sic.case_id) * 100.0) / COUNT(DISTINCT rc.case_id) AS percentage_with_multiple_severe_injuries
FROM RelevantCases rc
LEFT JOIN SevereInjuryCases sic ON sic.case_id = rc.case_id;"
bq067,"I want to create a labeled dataset from the National Highway Traffic Safety Administration traffic fatality data that predicts whether a traffic accident involving more than one distinct person results in more than one fatality, where the label is 1 if an accident has more than one person with an injury severity code of 4 (fatal injury) and 0 otherwise. For each accident, include the numeric predictors: state_number, the vehicle body_type, the number_of_drunk_drivers, the day_of_week, the hour_of_crash, and a binary indicator for whether the accident occurred in a work zone (1 if it is not “None,” otherwise 0). Also, engineer a feature for the average absolute difference between travel_speed and speed_limit per accident, only considering travel speeds up to 151 mph (excluding codes 997, 998, 999) and speed limits up to 80 mph (excluding codes 98, 99), and categorize this average speed difference into levels from 0 to 4 in 20 mph increments with lower bounds inclusive and upper bounds exclusive. Finally, only include accidents that involve more than one distinct person.","The provided context and tables do not include specific data or fields related to the National Highway Traffic Safety Administration (NHTSA) traffic fatality data requirements mentioned in the query, such as ""state_number"", ""vehicle body_type"", ""number_of_drunk_drivers"", ""day_of_week"", ""hour_of_crash"", or specifics about ""travel_speed"" and ""speed_limit"". The tables are primarily structured around different datasets that do not align with the specific traffic fatality data details provided in your request. Additionally, no table contains fields directly correlating to injury severity codes or work zone indicators as described.

Given the lack of relevant fields and tables, it's not possible to construct a SQL query that would fulfill the requirements of your request based on the provided context. More appropriate source data from NHTSA or a similar dataset that specifically includes these fields would be necessary to proceed with such a query."
bq396,Which top 3 states had the largest differences in the number of traffic accidents between rainy and clear weather during weekends in 2016? Please also provide the respective differences for each state.,"SELECT 
    c.caltrans_county AS state,
    ABS(SUM(CASE WHEN c.weather_1 = 'Rain' THEN 1 ELSE 0 END) - SUM(CASE WHEN c.weather_1 = 'Clear' THEN 1 ELSE 0 END)) AS difference_in_accidents
FROM 
    collisions c
WHERE 
    strftime('%Y', c.collision_date) = '2016' AND
    strftime('%w', c.collision_date) IN ('0', '6') -- '0' is Sunday and '6' is Saturday in SQLite
GROUP BY 
    c.caltrans_county
ORDER BY 
    difference_in_accidents DESC
LIMIT 3;"
bq441,"Please help me compile the critical details on traffic accidents in 2015, as listed in the info document.","SELECT 
  ""collision_date"",
  ""collision_time"",
  ""collision_severity"",
  ""killed_victims"",
  ""injured_victims"",
  ""weather_1"",
  ""type_of_collision"",
  ""primary_collision_factor"",
  ""alcohol_involved""
FROM 
  ""collisions""
WHERE 
  strftime('%Y', ""collision_date"") = '2015';"
bq097,"What is the increasing amount of the average earnings per job between the years 2012 and 2017 for each geographic region in Massachusetts (indicated by ""MA"" at the end of GeoName)?","The context provided does not contain information about the earnings per job over years, or any data that directly relates to geographic regions in Massachusetts distinguished by ""GeoName"". Therefore, it's not possible to generate a valid SQL query based on the available tables to answer the question regarding the increasing amount of average earnings per job between the years 2012 and 2017 for each geographic region in Massachusetts."
bq120,"Identify the top 10 regions (counties) with the highest total number of SNAP-participating households, using the 2017 5-year ACS county-level data and SNAP enrollment data from January 1, 2017, excluding regions where the total SNAP participation is zero. For each of these regions, calculate the ratio of households earning under $20,000 to the total number of SNAP-participating households.","The provided context is insufficient to generate the SQL query as requested. The tables necessary for retrieving data about SNAP-participating households, their income levels, and specific ACS county-level data for 2017 are not defined or mentioned in the provided schema. Therefore, it's not possible to generate the SQL query without the relevant table structures and data."
bq110,What is the change in the number of homeless veterans between 2012 and 2018 for each CoC region in New York that has data available in both years?,"The provided context is insufficient to generate the SQL query because there is no table or data description related to homeless veterans, CoC (Continuum of Care) regions, or specific years such as 2012 and 2018."
bq395,"Calculate the percentage change in the total number of unsheltered homeless people from 2015 to 2018 for each state by summing the counts over all Continuums of Care (CoCs) within each state. Then, determine the national average of these state percentage changes. Identify the five states whose percentage change is closest to this national average percentage change. Please provide the state abbreviations.","The provided context is insufficient to generate the requested SQL query. The necessary data regarding homeless counts by Continuums of Care (CoCs) over the years 2015 and 2018, or any related homelessness data, is not available in the described tables. Therefore, I am unable to fulfill the request based on the current table structures provided."
bq352,Please list the average number of prenatal weeks in 2018 for counties in Wisconsin where more than 5% of the employed population had commutes of 45-59 minutes in 2017.,"The provided context is insufficient to generate a valid SQL query. The database schema given does not include tables or data related to prenatal weeks, employment or commuting details, or specific geographical data about counties in Wisconsin. These details are necessary to construct a query matching your specifications."
bq074,"Count the number of counties that experienced an increase in unemployment from 2015 to 2018, using 5-year ACS data, and a decrease in dual-eligible enrollee counts between December 1, 2015, and December 1, 2018.","The provided context does not include any tables or data related to unemployment or dual-eligible enrollee counts for any years, including 2015 and 2018. Therefore, it is not possible to generate a SQL query to answer the question based on the available table schemas and descriptions."
bq066,"Could you assess the relationship between the poverty rates from the previous year's census data and the percentage of births without maternal morbidity for the years 2016 to 2018? Use only data for births where no maternal morbidity was reported and for each year, use the 5-year census data from the year before to compute the Pearson correlation coefficient","The provided context is insufficient to generate the SQL query as there is no table or column related to poverty rates, births data, maternal morbidity, or any relevant census data provided in the schema descriptions. Additional data containing these specific metrics is required to proceed with the analysis and computation of the Pearson correlation coefficient."
bq114,"Which three cities have the largest difference between their 1990 EPA PM2.5 measurements (using units_of_measure = 'Micrograms/cubic meter (LC)' and parameter_name = 'Acceptable PM2.5 AQI & Speciation Mass') and their 2020 OpenAQ PM2.5 measurements (where pollutant = 'pm25' based on the year extracted from the timestamp), with both datasets matched by latitude and longitude rounded to two decimals, and the difference ordered from greatest to least?","The provided context is insufficient to generate the SQL query as requested. There are no tables or data fields mentioned in the context that relate to EPA PM2.5 measurements for 1990, OpenAQ PM2.5 measurements for 2020, units of measure, parameter names, pollutants, or timestamps from which the year can be extracted. Additional information or access to relevant database schema containing these specific environmental data measurements is necessary to fulfill this query."
bq116,"Which U.S. state reported the highest total annual revenue in billions of dollars during fiscal year 2016, considering companies that provided four quarters of data and reported measure tags in ('Revenues','SalesRevenueNet','SalesRevenueGoodsNet'), excluding any entries where the state field (stprba) is null or empty?","The provided context is insufficient to generate the requested SQL query. There are no tables described in the schema with fields that directly relate to companies' annual revenue, fiscal years, or measure tags like 'Revenues', 'SalesRevenueNet', and 'SalesRevenueGoodsNet'. The provided tables focus on different aspects such as hardware facts, sales statistics, movies, and web sales representatives, but none of these include detailed financial reporting data for companies by state as described in your question."
bq015,"Identify and rank the top 10 tags from Stack Overflow questions that were referenced in Hacker News comments on or after 2014 by counting how many times each question was mentioned, then splitting the questions’ tag strings by the '|' delimiter, grouping by tag","The provided context is insufficient to generate a SQL query for the specific task of identifying and ranking the top 10 tags from Stack Overflow questions mentioned in Hacker News comments. The databases and tables mentioned in the context do not include any information or data related to Stack Overflow or Hacker News. Therefore, without relevant tables containing data about Stack Overflow questions, their tags, and references to these in Hacker News comments, it is not possible to write an SQL query to fulfill the request."
bq041,"Compute the monthly statistics for new StackOverflow users created in 2021. For each month, report the total number of new users, the percentage of these new users who asked at least one question within 30 days of signing up, and among those who asked a question within 30 days, the percentage who then answered at least one question after their first question and within 30 days following their first question.","The provided context does not contain any table related to StackOverflow users, their activities like asking or answering questions, or any data that pertains to user interaction with a site similar to StackOverflow. Thus, it's impossible to generate a SQL query to compute the specified monthly statistics for new StackOverflow users created in 2021 with the available tables."
sf_bq121,"How do the average reputation and number of badges vary among Stack Overflow users based on the number of complete years they have been members, considering only those who joined on or before October 1, 2021?","The provided context is insufficient to generate the SQL query because there are no tables related to Stack Overflow users, their reputation, badges, or membership duration mentioned in the table structures provided. This information is necessary to create an appropriate SQL query to answer the question."
bq123,"You need to determine which day of the week has the third highest percentage of questions on Stack Overflow that receive an answer within an hour. To do this, use the question creation date from the posts_questions table and the earliest answer creation date from the posts_answers table. Once you’ve calculated the percentage of questions that get answered within an hour for each day, identify the day with the third highest percentage and report that percentage.",The provided context is insufficient to generate the SQL query as there are no tables named `posts_questions` and `posts_answers` described in the database schema provided. These tables are necessary for accessing the creation times of questions and answers to calculate the required response times. Please provide the schema for these tables or confirm their structure and relationships.
bq280,"Please provide the display name of the user who has answered the most questions on Stack Overflow, considering only users with a reputation greater than 10.","The provided context is insufficient to generate a SQL query for finding the display name of the user who has answered the most questions on Stack Overflow, considering only users with a reputation greater than 10. The tables and fields described do not include any relevant information about Stack Overflow, user reputations, or question-answering activities."
bq300,"What is the highest number of answers received for a single Python 2 specific question on Stack Overflow, excluding any discussions that involve Python 3?","The provided context is insufficient to generate the SQL query as there is no table or dataset related to Stack Overflow questions, answers, or any details about programming languages like Python 2 or Python 3 provided in the schema definitions."
bq301,"Retrieve details of accepted answers to Stack Overflow questions posted in January 2016 that have tags including ""javascript"" and at least one of ""xss"", ""cross-site"", ""exploit"", or ""cybersecurity""; the answers themselves must also have been posted in January 2016. For each accepted answer, include the answer's ID, the answerer's reputation, score, and comment count, along with the associated question's tags, score, answer count, the asker's reputation, view count, and comment count.","The provided context does not contain any information about Stack Overflow data, such as tables related to questions, answers, tags, user reputations, or related metadata. Therefore, it is impossible to generate the requested SQL query based on the current context."
bq302,What is the monthly proportion of Stack Overflow questions tagged with 'python' in the year 2022?,"The provided context is insufficient to generate the SQL query because there is no table containing information about Stack Overflow questions, their tags, or their respective counts. A relevant table containing data about Stack Overflow questions and their tags is needed to answer this query."
bq303,"From July 1, 2019 through December 31, 2019, for all users with IDs between 16712208 and 18712208 on Stack Overflow, retrieve the user ID and the tags of the relevant question for each of their contributions, including comments on both questions and answers, any answers they posted, and any questions they authored, making sure to correctly associate the comment or answer with its parent question’s tags.","The provided tables do not contain any information regarding Stack Overflow data, user contributions, or tags associated with questions and answers. Thus, it is not possible to generate a SQL query based on the current database schema to fulfill the request regarding Stack Overflow user contributions and tags."
bq304,"Retrieve the top 50 most viewed questions for each of the following Android-related tags on StackOverflow: 'android-layout', 'android-activity', 'android-intent', 'android-edittext', 'android-fragments', 'android-recyclerview', 'listview', 'android-actionbar', 'google-maps', and 'android-asynctask'. Each question must contain the word 'how' in either its title or body and must not contain any of the following troubleshooting terms in either its title or body: 'fail', 'problem', 'error', 'wrong', 'fix', 'bug', 'issue', 'solve', or 'trouble'. Only include tags that have at least 50 questions meeting these criteria, and for each such tag, select the top 50 questions ranked by view count.","The provided tables and the description of the database do not include any data or structure related to StackOverflow, its questions, views, or tags. To generate a SQL query for retrieving most viewed questions filtered by specific tags and content conditions, the necessary data schema and tables would need to include information about questions, their view counts, tags, and textual content which are not present in the provided database schema.

Without access to a relevant database schema that includes StackOverflow-like data, it's not possible to generate the requested SQL query."
bq310,"What is the title of the most viewed ""how"" question related to Android development on StackOverflow, across specified tags such as 'android-layout', 'android-activity', 'android-intent', and others","The provided context is insufficient to generate a SQL query because there is no table or data schema related to StackOverflow, questions, views, tags, or any related entity in the provided schema information. To generate a query based on StackOverflow data, access to a relevant database schema that includes tables for questions, their view counts, titles, and associated tags would be required."
bq305,"Which 10 users have the highest combined view counts for questions they are associated with, where a user is considered associated if they own the question, or their answer is the accepted answer, or their answer's score is greater than 5, or their answer's score exceeds 20% of the total answer scores for that question (and is above 0), or their answer is among the top three highest-scoring answers for that question?","The provided context is insufficient to generate the requested SQL query because there are no tables or schema details related to users, questions, answers, view counts, or any relational data that links these entities in such a way that would allow the analysis of user associations with questions based on ownership, accepted answer status, or scoring criteria."
bq306,"Identify the top 10 tags for user 1908967, based only on answers posted before June 7, 2018, where each tag’s score is 10 times the number of upvotes (vote_type_id=2) and 15 times the number of accepted answers (vote_type_id=1). Derive tags from the questions associated with those answers, and consider only the upvotes and accepted answers for those answers. Return the tags with the highest total scores in descending order, limited to 10 tags.","The provided context is insufficient to generate the requested SQL query because there are no tables related to users, tags, answers, questions, or votes in the provided schema. Additional information about the structure of these tables, such as how answers are linked to questions and how votes are stored, is necessary to accurately fulfill the request."
sf_bq307,"Find the top 10 gold badges that users most commonly earn as their first gold badge on Stack Overflow. For each of these badges, display the badge name, the number of users who earned it as their first gold badge, and the average number of days from the user's account creation date to the date they earned the badge, calculated in days without any adjustments for date formats.","The provided context is insufficient to generate the required SQL query because there are no tables related to Stack Overflow users, their badges, or account creation dates in the database schema provided. Therefore, I am unable to generate the SQL query without information about the relevant tables and columns that store user badges and account creation dates on Stack Overflow."
bq308,"Show the number of Stack Overflow questions asked each day of the week in 2021, and find out how many and what percentage of those were answered within one hour.","The provided context does not include any tables or data directly related to Stack Overflow, such as questions, their timestamps, or their response times. Therefore, I cannot generate a SQL query to answer the question about Stack Overflow questions and their response times in 2021. Please provide the relevant table structure or data source that includes Stack Overflow questions and their details."
bq309,"Retrieve the top 10 longest questions on Stack Overflow, measured by the length of their body text, where each question either has an accepted answer or has no accepted answer but has at least one answer with a score-to-view ratio exceeding 0.01. For each of these questions, include the reputation of the user who asked the question, the user's net votes (calculated as their total up_votes minus down_votes), and the total number of badges the user has earned.","The provided context is insufficient to generate the SQL query as it lacks details about the necessary tables and their schema related to Stack Overflow questions, answers, user reputations, user votes, and user badges. These are essential to retrieve the information requested about the questions, their answers, and the associated user details."
bq124,"Among all patients, how many individuals remain alive (i.e., with no recorded deceased.dateTime), have a diagnosis of either Diabetes or Hypertension, and are prescribed at least seven distinct active medications?","The provided context is insufficient to generate an SQL query that answers the question about patients, their diagnoses, whether they are alive, and their prescriptions. The tables described do not contain any information related to medical records, patient diagnoses, medication prescriptions, or deceased status. Therefore, it's not possible to construct the required query with the available table schemas."
bq391,"Among living patients whose last names begin with ""A"" and who each have exactly one distinct condition, which eight conditions have the highest number of different active medications prescribed to any single patient, and what are their corresponding codes?","The provided context is insufficient to generate a SQL query for the question asked. There is no available information or tables containing data on patients, their medical conditions, medications, or any details that relate directly to healthcare or patient data management."
bq126,"What are the titles, artist names, mediums, and original image URLs of objects with 'Photograph' in their names from the 'Photographs' department, created not by an unknown artist, with an object end date of 1839 or earlier?","The provided context is insufficient to generate the requested SQL query because there are no tables related to objects, their names, mediums, departments, or image URLs. Additionally, there is no information on how to identify objects created by known or unknown artists, or filtering based on object end dates."
bq366,"What are the top three most frequently associated labels with artworks from each historical period in The Met's collection, only considering labels linked to 500 or more artworks? Provide me with the period, label, and the associated count.","The provided context does not contain any information about The Met's collection, historical periods, or any table that would relate artworks, labels, and historical periods. Therefore, it is impossible to generate a SQL query based on the given database schemas to answer your question."
bq414,"Retrieve the object id, title, and the formatted metadata date (as a string in 'YYYY-MM-DD' format) for objects in the ""The Libraries"" department where the cropConfidence is greater than 0.5, the object's title contains the word ""book"".","The provided context is insufficient to generate the required SQL query. The tables described do not include any information about an ""object id,"" ""The Libraries"" department, ""cropConfidence,"" or metadata related to these specifics."
bq200,"Using data from both the regular season and the post-season, identify the pitcher who achieved the highest non-zero pitch speed for each team by confirming whether the pitcher’s ID appears in the relevant home or away player lists for that game, then retrieve that pitcher’s full name along with the maximum valid pitch speed they achieved while playing for that specific team.","The provided context is insufficient to generate the requested SQL query because there is no data regarding pitch speeds or information linking pitchers to specific games with designated home or away status in the provided tables. Additionally, there are no tables containing details about the player lists for games that would allow us to verify whether a pitcher's ID appears in the home or away player lists. Therefore, I cannot generate the SQL query as requested."
sf_bq458,"Tokenize the body text of each article into words, excluding stop words, and obtain the corresponding word vectors for these words from the glove vector. For each word, weight its word vector by dividing each component by the 0.4th power of the word's frequency from the word frequencies. Then, for each article, aggregate these weighted word vectors by summing their components to form an article vector. Normalize each article vector to unit length by dividing by its magnitude. Finally, retrieve the ID, date, title, and the normalized article vector for each article.","The provided context is insufficient to generate a SQL query for the described operation. The operation involves processing natural language text, handling stop words, computing word frequencies, manipulating word vectors, and performing mathematical operations like normalization that are not supported by SQL. This task requires a combination of natural language processing (NLP) tools and possibly machine learning libraries, such as NLTK for stop word removal and Gensim for handling word vectors, along with a programming language like Python to execute these operations. The tables provided do not contain any columns related to article texts, word vectors, or any related attributes necessary to perform these operations."
sf_bq459,"Please find the top 10 most relevant articles by only processing each article’s 'body' field, where each body is tokenized with no stopwords, each remaining token is turned into a GloVe-based word vector and weighted by dividing each dimension by the 0.4th power of its word frequency, then these weighted vectors are summed and normalized to get a unit vector for each article. Perform the same weighting and normalization on the query phrase 'Epigenetics and cerebral organoids: promising directions in autism spectrum disorders' and compute the cosine similarity between the query vector and each article vector. Finally, return the id, date, title, and the cosine similarity score for the top 10 articles with the highest similarity.","The provided context and tables do not include any table or column related to articles, their bodies, or any fields like date and title that would be used to perform the task you described. This task involves text processing, vectorization, and querying based on computed similarities, which would typically be handled outside of SQL in a data processing or machine learning environment, using specialized libraries for natural language processing and vector computations.

SQL databases are not equipped to handle tasks like text vectorization or cosine similarity computations directly, especially with methods like GloVe. Such operations would require external tools or programming libraries, such as Python with libraries like NLTK for text processing and spaCy or gensim for word vector computations.

Therefore, this SQL query cannot be generated based on the current database schema and the nature of the task. More appropriate tools and a different computational environment are needed."
sf_bq460,"Please process the articles from the 'nature' dataset by first tokenizing the body text into words and removing stopwords. For each remaining word, retrieve its word vector from the glove_vectors table and its frequency from the word_frequencies table, then divide each word vector by the 0.4th power of the word's frequency to weight it. Sum the weighted vectors to obtain an aggregate vector for each article, normalize this aggregate vector to unit length, and then compute the cosine similarity scores between these normalized vectors. Finally, return the IDs, dates, titles, and cosine similarity scores of the top 10 articles most similar to the article with the ID '8a78ef2d-d5f7-4d2d-9b47-5adb25cbd373'.","The context provided does not include any tables or data sources named 'nature', 'glove_vectors', or 'word_frequencies', which are necessary to generate the SQL query as requested. Additionally, SQL itself does not support text processing operations like tokenization, stopword removal, or vector manipulations as described. These operations typically require external programming support or specialized text processing software. Therefore, I cannot generate the requested SQL query based on the given context."
bq204,Find the user with the highest total clicks across all records from all available photo collections.,The provided context does not contain information about photo collections or user clicks in a structured format that would allow the generation of a SQL query for the requested information. Please provide details on the relevant tables and schema that include user interactions with photo collections.
bq389,"Please calculate the monthly average levels of PM10, PM2.5 FRM, PM2.5 non-FRM, volatile organic emissions, SO2 (scaled by a factor of 10), and Lead (scaled by a factor of 100) air pollutants in California for the year 2020.","The provided context does not contain any data or tables related to air quality measurements such as PM10, PM2.5 FRM, PM2.5 non-FRM, volatile organic emissions, SO2, or Lead. Therefore, it is not possible to generate a SQL query to calculate the requested averages without the relevant data. Please provide the appropriate dataset or table schema that includes these air pollutant measurements."
sf_bq345,"How large are the DICOM image files with SEG or RTSTRUCT modalities and the SOP Class UID ""1.2.840.10008.5.1.4.1.1.66.4"", when grouped by collection, study, and series IDs, if they have no references to other series, images, or sources? Can you also provide a viewer URL formatted as ""https://viewer.imaging.datacommons.cancer.gov/viewer/"" followed by the study ID, and list these sizes in kilobytes, sorted from largest to smallest?","The provided context is insufficient to generate the requested SQL query. There is no mention of a table that contains information about DICOM image files, their modalities, SOP Class UIDs, or any associated image file metadata like collection, study, or series IDs. Additionally, details about references to other series, images, or sources are not provided in any of the tables described. Therefore, I cannot generate a SQL query based on the current information."
sf_bq346,"In publicly accessible DICOM data where the Modality is 'SEG' and the SOPClassUID is '1.2.840.10008.5.1.4.1.1.66.4', and each segmentation references its original SOPInstanceUID, which five segmentation categories (by 'SegmentedPropertyCategory.CodeMeaning') occur most frequently?","The provided context is insufficient to generate a SQL query for the question about DICOM data, as there are no tables related to DICOM, Modality, SOPClassUID, SOPInstanceUID, or SegmentedPropertyCategory in the schema information provided. This information would be necessary to construct an appropriate query to determine the frequency of segmentation categories."
sf_bq347,"From the union of the specified MR series with SeriesInstanceUID 1.3.6.1.4.1.14519.5.2.1.3671.4754.105976129314091491952445656147 and all associated segmentation instances, which modality has the greatest number of SOP instances in total, and how many are there?","The provided context is insufficient to generate a SQL query because there are no tables or fields related to medical imaging data, MR series, SeriesInstanceUID, segmentation instances, modalities, or SOP instances in the schema information provided. The tables described are unrelated to the domain of medical imaging or DICOM data, which are necessary to answer the question about MR series and modalities."
sf_bq390,"In the ""qin_prostate_repeatability"" collection, please provide the distinct StudyInstanceUIDs for studies that include T2-weighted axial MR imaging and also contain anatomical structure segmentations labeled as ""Peripheral zone.""","The provided context is insufficient to generate the requested SQL query because there is no table named ""qin_prostate_repeatability"" or any related table schema provided which includes fields such as ""StudyInstanceUID"", ""T2-weighted axial MR imaging"", or ""Peripheral zone."""
sf_bq421,"Can you list all unique pairs of embedding medium and staining substance code meanings, along with the number of occurrences for each pair, based on distinct embedding medium and staining substance codes from the 'SM' modality in the DICOM dataset's un-nested specimen preparation sequences, ensuring that the codes are from the SCT coding scheme?","The provided context is insufficient to generate the SQL query because there is no table or column information related to the DICOM dataset's specimen preparation sequences, embedding medium, staining substance codes, or any specific modality such as 'SM'. Additionally, there are no details on coding schemes like SCT in the provided table schemas. Without access to the relevant dataset or schema details reflecting these requirements, it's not possible to construct the SQL query."
sf_bq422,"Using the 'nlst' collection's CT images, calculate and compare two separate metrics: 1) The average series size in MiB for the top 3 patients with the highest slice interval difference tolerance (defined as the difference between the maximum and minimum unique slice intervals across all their series), and 2) The average series size in MiB for the top 3 patients with the highest exposure difference (defined as the difference between the maximum and minimum unique exposure values across all their series). For each patient, calculate the series size by summing the instance sizes of all images in that series and converting to MiB. Return the results as two separate groups labeled ""Top 3 by Slice Interval"" and ""Top 3 by Max Exposure"" with their respective average series sizes.","The provided context does not contain any information about a 'nlst' collection, CT images, patient data, slice intervals, exposure values, or instance sizes. It is therefore impossible to generate a SQL query based on the requirements specified, as the necessary tables and columns are not defined in the provided schema details.

To proceed with such a query, the database schema would need to include tables detailing patient series, slice intervals, exposure settings, and image instance sizes. Without this information, the query cannot be created."
sf_bq069,"Could you help me generate a report of CT image series from the dicom_all table such that all series from the NLST collection are excluded, any localizers or JPEG-compressed series (transfer syntaxes 1.2.840.10008.1.2.4.70 or 1.2.840.10008.1.2.4.51) are skipped, and only those passing certain geometry checks—namely a single orientation, identical pixel spacing, matching SOP instance and position counts, uniform pixel rows and columns, and a near-unity dot product of image orientation vectors—are included, while also computing slice interval differences, exposure differences, and approximate series size in MB for each qualified series?","The provided context is insufficient to generate the requested SQL query. There is no table named `dicom_all` or any table related to medical imaging or DICOM data provided in the schema information. Additionally, the specific details required for filtering and calculations related to medical imaging data, such as SOP instance counts, image orientation vectors, and other DICOM-specific attributes, are not available in the described database schema."
sf_bq219,"In the Iowa Liquor Sales dataset, starting from January 1, 2022 through the last fully completed month, which two liquor categories, each contributing an average of at least 1% to the monthly sales volume over at least 24 months of available data, have the lowest Pearson correlation coefficient when comparing their monthly percentages of total liquor sales across those months, and what are their names?","The provided context is insufficient to generate the SQL query because there is no table or dataset directly mentioned or described in the provided schema that pertains to ""Iowa Liquor Sales"" or includes relevant columns such as liquor categories or sales volumes. Therefore, I cannot create the requested SQL query without access to the appropriate data structure and content."
bq199,"Identify the top 10 liquor categories in Iowa in 2021 by calculating, for each category, the average of the per-liter retail prices across all sales transactions in that category during 2021. For these top categories, provide their average per-liter retail prices calculated in the same manner for the years 2019, 2020, and 2021.","The provided context is insufficient to generate the SQL query as requested:

1. There is no table containing information about liquor categories or associated with Iowa specifically.
2. No table contains data related to sales transactions with specific dates (like 2019, 2020, and 2021) linked to liquor or any categories of products with per-liter pricing details.

To provide a useful query, additional details about the relevant tables containing the necessary information would be required."
bq218,What are the top 5 items with the highest year-over-year growth percentage in total sales revenue for the year 2023?,"WITH YearlySales AS (
    SELECT
        prod_id,
        SUM(amount_sold) AS total_sales,
        strftime('%Y', time_id) AS year
    FROM
        sales
    GROUP BY
        prod_id, year
),
YearOverYearGrowth AS (
    SELECT
        a.prod_id,
        ((a.total_sales - b.total_sales) / b.total_sales) * 100 AS growth_percentage
    FROM
        YearlySales a
    JOIN
        YearlySales b ON a.prod_id = b.prod_id
    WHERE
        a.year = '2023' AND b.year = '2022'
)
SELECT
    p.productid,
    p.productname,
    y.growth_percentage
FROM
    YearOverYearGrowth y
JOIN
    products p ON y.prod_id = p.productid
ORDER BY
    y.growth_percentage DESC
LIMIT 5;"
bq049,"Please show the monthly per capita Bourbon Whiskey sales during 2022 in Dubuque County for the zip code that ranks third in total Bourbon Whiskey sales, using only the population aged 21 and older.","The provided context is insufficient to generate the SQL query as requested because the necessary tables or fields are not present in the given schema. Specifically, the following details are missing or incomplete:

1. There is no table or field directly indicating the type of alcohol (e.g., Bourbon Whiskey) in the product tables. 
2. There is no table or field indicating the population of each zip code, or specifically the population aged 21 and older.
3. There is no table or data linking sales data to specific counties or zip codes to filter sales by Dubuque County.
4. There is no linkage or data provided on how to determine the third-ranking zip code in terms of Bourbon Whiskey sales.

These limitations prevent the creation of a SQL query to satisfy the request based on the provided schema. Additional data sources or tables containing this specific information would be necessary."
bq360,"Among healthcare providers whose practice location is in Mountain View, CA, and who have a specified specialization in the field healthcare provider taxonomy, identify the top 10 most common specializations based on the count of distinct NPIs. Then determine which of those top 10 has a count of distinct NPIs closest to the average count across those 10 specializations.","The provided context is insufficient to generate a SQL query as there is no table schema or information related to healthcare providers, their practice locations, specializations (taxonomy), or NPIs (National Provider Identifier) provided in the database schemas available. For generating the requested query, information about healthcare provider details including their NPIs, specializations, and practice locations would be necessary."
bq286,"Can you tell me the name of the most popular female baby in Wyoming for the year 2021, based on the proportion of female babies given that name compared to the total number of female babies given the same name across all states?","The provided context is insufficient to generate the SQL query you requested, as there is no table in the given schema containing information about baby names, their gender distribution by state, or specific data for the year 2021. To answer the query about the most popular female baby name in Wyoming for 2021 based on proportions, a dataset containing baby names, counts by gender, state, and year would be required."
sf_bq044,"For bladder cancer patients who have mutations in the CDKN2A (cyclin-dependent kinase inhibitor 2A) gene, using clinical data from the Genomic Data Commons Release 39, what types of mutations are they, what is their gender, vital status, and days to death - and for four downstream genes (MDM2 (MDM2 proto-oncogene), TP53 (tumor protein p53), CDKN1A (cyclin-dependent kinase inhibitor 1A), and CCNE1 (Cyclin E1)), what are the gene expression levels for each patient?","The provided context is insufficient to generate the requested SQL query. There are no tables related to genomic data, clinical data, or specific patient information such as gene mutations, gender, vital status, days to death, or gene expression levels in the given database schema. This information would be necessary to answer the question regarding bladder cancer patients with mutations in the CDKN2A gene and related data points."
sf_bq043,"What are the RNA expression levels of the genes MDM2, TP53, CDKN1A, and CCNE1, along with associated clinical information, in bladder cancer patients with CDKN2A mutations in the 'TCGA-BLCA' project?  Use clinical data from the Genomic Data Commons Release 39, data about somatic mutations derived from the hg19 human genome reference in Feb 2017.","The provided context is insufficient to generate the requested SQL query. The tables listed do not contain any relevant information pertaining to RNA expression levels, clinical data, gene mutations, or specific projects like 'TCGA-BLCA'. Additionally, there are no references to Genomic Data Commons or any genomic datasets. Therefore, the query cannot be generated from the available tables."
bq143,"Use CPTAC proteomics and RNAseq data for Clear Cell Renal Cell Carcinoma to select 'Primary Tumor' and 'Solid Tissue Normal' samples. Join the datasets on sample submitter IDs and gene symbols. Calculate the correlation between protein abundance (log2 ratio) and gene expression levels (log-transformed+1 FPKM) for each gene and sample type. Filter out correlations with an absolute value greater than 0.5, and compute the average correlation for each sample type.","The provided context does not contain any information about tables or fields related to CPTAC proteomics, RNAseq data, sample submitter IDs, gene symbols, protein abundance, or gene expression levels. Therefore, it is impossible to generate a valid SQL query based on the given database schema. Please provide the relevant table and column details or adjust the schema to include this data."
sf_bq147,"Can you identify the TCGA breast cancer cases from the RNA sequencing hg38 r35` where the protein_coding gene and the project TCGA-BRCA, and which have RNA sequencing samples of multiple tissue types—including ""Solid Tissue Normal""—within the same case?","The provided context is insufficient to generate the SQL query because there are no tables or schemas related to TCGA, breast cancer cases, RNA sequencing, gene types, or projects specified in the given database schema information."
sf_bq148,Could you identify the top five protein-coding genes that exhibit the highest variance in their expression levels (measured as fpkm_uq_unstranded) specifically within 'Solid Tissue Normal' samples? Please limit the analysis to TCGA-BRCA project cases that include at least one 'Solid Tissue Normal' sample type.,"The provided context is insufficient to generate the SQL query because there are no tables or columns related to gene expression data, TCGA-BRCA project cases, or sample types such as 'Solid Tissue Normal' mentioned in the database schema provided."
sf_bq175,"Identify cytoband names on chromosome 1 in the TCGA-KIRC segment allelic dataset where the frequency of amplifications, gains, and heterozygous deletions each rank within the top 11. Calculate these rankings based on the maximum copy number observed across various genomic studies of kidney cancer, reflecting the severity of genetic alterations.","The provided context is insufficient to generate the requested SQL query because there is no table or schema information related to cytoband names, chromosome data, or related genomic studies such as TCGA-KIRC segment allelic dataset in the provided table schemas. The necessary data concerning genomic studies, including details about amplifications, gains, and heterozygous deletions, is not available in the provided database schema."
sf_bq176,"Identify the case barcodes from the TCGA-LAML study with the highest weighted average copy number in cytoband 15q11 on chromosome 15, using segment data and cytoband overlaps from TCGA's genomic and Mitelman databases.","The provided context is insufficient to generate the SQL query because there are no provided table structures or table names related to TCGA's genomic or Mitelman databases, segment data, and cytoband overlaps in the list of tables. These specific databases and data types are necessary to answer the question regarding the TCGA-LAML study and cytoband 15q11 on chromosome 15. Without this information, I cannot generate an appropriate SQL query."
sf_bq170,"For breast cancer cases (TCGA-BRCA) from Release 23 of the active GDC archive, identify and categorize copy number variations (CNVs) across all cytobands on every chromosome. For each cytoband and each case, determine the overlap between the cytoband region and the case's copy number segments, and compute the overlap-weighted average copy number for that cytoband in the case, rounding to the nearest whole number. Classify the rounded copy number into CNV types as follows: homozygous deletions (0), heterozygous deletions (1), normal diploid state (2), gains (3), and amplifications (greater than 3). For each cytoband, provide its name and start/end positions, and calculate the frequency of each CNV type across all cases as a percentage of the total number of cases, rounded to two decimal places.","The context provided does not include any information or metadata related to genomic data, cytobands, copy number variations (CNVs), or specific cancer cases like TCGA-BRCA. The existing tables do not contain relevant data to perform genomic data analysis, compute copy number variations, or classify them into CNV types. Therefore, it's not possible to generate the requested SQL query based on the provided table structures and data context. Further details about the schema containing genomic data and definitions for cytobands and CNVs would be required to create an appropriate query."
sf_bq150,"Assess whether different genetic variants affect the log10-transformed TP53 expression levels in TCGA-BRCA samples using sequencing and mutation data. Provide the total number of samples, the number of mutation types, the mean square between groups, the mean square within groups, and the F-statistic.","The provided context does not contain any tables or data related to genetic variants, TP53 expression levels, or TCGA-BRCA samples, sequencing, and mutation data. Therefore, it is not possible to generate a SQL query to assess the impact of genetic variants on TP53 expression levels as requested. Without the relevant data structure or dataset details, the query cannot be formed."
sf_bq152,"For breast cancer cases (TCGA-BRCA) from Release 23 of the active GDC archive, identify and categorize copy number variations (CNVs) across all cytobands on every chromosome. For each cytoband and each case, determine the overlap between the cytoband region and the case's copy number segments, and compute the overlap-weighted average copy number for that cytoband in the case, rounding to the nearest whole number. Classify the rounded copy number into CNV types as follows: homozygous deletions (0), heterozygous deletions (1), normal diploid state (2), gains (3), and amplifications (greater than 3). For each cytoband, provide its name and start/end positions, and calculate the frequency of each CNV type across all cases as a percentage of the total number of cases, rounded to two decimal places.","The provided context is insufficient to generate the SQL query you requested because it lacks the necessary tables and fields related to genetic data, cytobands, and copy number variations (CNVs) for breast cancer cases. There are no tables related to genetic data, cytobands, or any indication of copy number segments or related genomic information in the table schemas provided. As such, it's not possible to create a SQL query for your request without access to the appropriate database schema containing this specific type of biological and genetic data."
sf_bq155,"In the TCGA-BRCA cohort of patients who are 80 years old or younger at diagnosis and have a pathological stage of Stage I, Stage II, or Stage IIA, calculate the t-statistic derived from the Pearson correlation between the log10-transformed average RNA-Seq expression levels (using HTSeq__Counts + 1) of the gene SNORA31 and the average microRNA-Seq expression levels of all unique microRNAs, only considering pairs with more than 25 samples and where the absolute Pearson correlation coefficient is between 0.3 and 1.0","The provided context is insufficient to generate the requested SQL query. The tables and schemas listed do not include any information related to patient cohorts, RNA-Seq expression levels, microRNA-Seq expression levels, or specific genetic data such as gene SNORA31. Additional information regarding the relevant database schema including these details is necessary to create the desired query."
sf_bq141,"Using the TCGA-KIRP dataset, select patients from the 'TCGA_bioclin_v0.Clinical' table who have a non-null clinical_stage and a disease_code of 'KIRP.' Retrieve their gene expression data from the 'TCGA_hg38_data_v0.RNAseq_Gene_Expression' table for the genes 'MT-CO3,' 'MT-CO1,' and 'MT-CO2,' and randomly split the patients into a training set (90%) and a test set (10%) based on their case_barcode via the FARM_FINGERPRINT method. For each clinical stage in the training set, calculate the average HTSeq__FPKM_UQ expression of the three genes. For each patient in the test set, compute the Euclidean distance between the patient’s expression values and the stage-specific averages, and assign that patient to the clinical stage whose average is closest. Finally, output the case_barcode and the predicted clinical stage.","The context provided does not include any information or schema details about the TCGA-KIRP dataset, the 'TCGA_bioclin_v0.Clinical' table, or the 'TCGA_hg38_data_v0.RNAseq_Gene_Expression' table. Without this information, I am unable to generate a valid SQL query to fulfill the requirements specified in the question. Please provide the schema or structure of the relevant tables, including column names and data types, to proceed with generating the SQL query."
bq046,"Find case barcodes and their corresponding GDC file URLs for female patients aged 30 or younger diagnosed with breast cancer (BRCA) in TCGA database. The query should first identify patients from the Annotations table where entity_type is ""Patient"" and either category is ""History of unacceptable prior treatment related to a prior/other malignancy"" or classification is ""Redaction"". Also identify patients from the Clinical table with disease_code ""BRCA"", age_at_diagnosis less than or equal to 30, and gender ""FEMALE"". Perform a FULL JOIN between these two sets, but only keep patients where both categoryName and classificationName are NULL in the final result. Use GDC metadata from archive release 14 (specifically rel14_caseData, rel14_fileData_current, and rel14_GDCfileID_to_GCSurl_NEW tables) to get the corresponding file URLs. ","The context provided is insufficient to generate the requested SQL query. The tables necessary for the described operations such as ""Annotations"", ""Clinical"", ""rel14_caseData"", ""rel14_fileData_current"", and ""rel14_GDCfileID_to_GCSurl_NEW"" are not present in the given database schema. Additional information about the schema of the TCGA database or these specific tables is necessary to create the SQL query."
sf_bq153,"Calculate, for each histology type specified in the 'icd_o_3_histology' field (excluding those enclosed in square brackets), the average of the per-patient average log10(normalized_count + 1) expression levels of the IGF2 gene among LGG patients with valid IGF2 expression data. Match gene expression and clinical data using the ParticipantBarcode field.","The provided context is insufficient to generate the requested SQL query because there are no tables or fields described that include information about histology types, gene expression levels, the IGF2 gene, LGG patients, or ParticipantBarcode. Additional information about the relevant database schema and data is necessary to proceed with crafting the SQL query."
sf_bq154,"Calculate the Kruskal-Wallis H-score among groups of LGG patients for IGF2 gene expression, where each patient’s IGF2 expression is determined by applying log10(normalized_count + 1) and then averaging across samples. Group the patients by ICD-O-3 histology codes, exclude any codes fully enclosed in square brackets, only include groups with more than one patient, and ensure that normalized count is not null. Finally, return the total number of groups, the total number of samples, and the Kruskal-Wallis H-score in descending order.","The provided context is insufficient to generate the desired SQL query because there is no table described in the schema that contains data related to patients, IGF2 gene expression, or ICD-O-3 histology codes. The tables given focus on different domains such as movies, sports statistics, order processing, and cricket scoring, but none relate to medical or genetic data. Therefore, it is not possible to construct the required query without information on the appropriate database structure that includes these specific medical and genetic fields."
sf_bq156,"Compute the t-score (rounded to 2 decimals) to compare the difference in mean expression levels of the gene DRG2 between two groups (TP53 mutated vs. non-mutated) in the Lower Grade Glioma (LGG) study, where the expression levels are calculated as the average of log10(normalized_count + 1) for each participant, only considering samples with TP53 mutations that have a 'FILTER' status of 'PASS' in the mutation data, and ignoring any groups with fewer than 10 samples or with zero variance; refer to `t_score.md` for the method of computing the t-score.","The provided context and tables do not contain any data or schema related to gene expression, mutations, or any biological/clinical study such as the Lower Grade Glioma (LGG) study. As such, it is not possible to generate a SQL query relating to the specific requirements (gene DRG2, TP53 mutation status, FILTER status, and expression levels calculation) mentioned in the question without the appropriate dataset or table schema. Please provide the relevant database schema or confirm the dataset to proceed with generating the SQL query."
sf_bq157,"Please compute the T-score to determine the statistical difference in the expression of the DRG2 gene between LGG patients with and without TP53 mutation: for each patient, calculate the average of log10(normalized_count + 1) of DRG2 expression across all their samples, using only samples present in the `MC3_MAF_V5_one_per_tumor_sample` table for the LGG study; identify patients with TP53 mutations from this table where `Hugo_Symbol` is 'TP53' and `FILTER` is 'PASS'; then perform a T-test comparing the mean averaged log-transformed DRG2 expression between patients with and without TP53 mutation.","The provided context is insufficient to generate the SQL query because there are no tables or schema details related to genetic studies, patient sample data, or TP53 mutations provided in the context. The required data elements and table structure need to be defined to address this query appropriately."
sf_bq158,Which top five histological types of breast cancer (BRCA) in the PanCancer Atlas exhibit the highest percentage of CDH1 gene mutations?,"The provided context and tables do not contain information regarding histological types of breast cancer (BRCA), the PanCancer Atlas, or specific gene mutations such as CDH1. Therefore, I cannot generate a SQL query to answer this question based on the available tables. Please provide relevant data related to cancer research, gene mutations, or any specific table that includes this information."
sf_bq159,Calculate the chi-square value to assess the association between histological types and the presence of CDH1 gene mutations in BRCA patients using data from the PanCancer Atlas. Focus on patients with known histological types and consider only reliable mutation entries.  Exclude any histological types or mutation statuses with marginal totals less than or equal to 10. Match clinical and mutation data using ParticipantBarcode,"The context provided is insufficient to generate the SQL query because there are no tables or schema details related to the PanCancer Atlas, histological types, CDH1 gene mutations, or BRCA patients in the provided database schema information. Details about how clinical and mutation data are structured and stored are necessary to perform such an analysis, including fields for ParticipantBarcode, histological types, and mutation statuses."
bq161,"Calculate the net difference between the number of pancreatic adenocarcinoma (PAAD) patients in TCGA's dataset who are confirmed to have mutations in both KRAS and TP53 genes, and those without mutations in either gene. Utilize patient clinical and follow-up data alongside genomic mutation details from TCGA’s cancer genomics database, focusing specifically on PAAD studies where the mutations have passed quality filters.","The provided context does not contain any information or structure about TCGA's database, patient clinical and follow-up data, or genomic mutation details. Therefore, it's not possible to generate a SQL query based on the tables and schema provided. To proceed, access to a database containing information about patients, genetic mutations, and specific data on pancreatic adenocarcinoma (PAAD) would be required."
bq151,"Using TCGA dataset, calculate the chi-squared statistic to evaluate the association between KRAS and TP53 gene mutations in patients diagnosed with pancreatic adenocarcinoma (PAAD). Incorporate clinical follow-up data and high-quality mutation annotations to accurately determine the frequency of patients with co-occurring KRAS and TP53 mutations compared to those with each mutation occurring independently. Ensure that patient records are meticulously matched based on unique identifiers to maintain data integrity. This analysis aims to identify and quantify potential correlations between KRAS and TP53 genetic alterations within the PAAD patient population.","The provided context is insufficient to generate the SQL query as there are no details or tables related to TCGA dataset, gene mutations, clinical follow-up data, or patient identifiers. Additional information about the database schema including how gene mutations are stored and how they are linked to patient records is necessary to construct the query."
bq162,"Based on the 5th revision (r5) of the HTAN data, list the imaging assay types available at the HTAN WUSTL center that have Level2 data and any associated higher-level data (Level3, Level4) derived from them through 'entityId' relationships in the 'id_provenance_r5' table; exclude any records where the 'Component' is NULL or contains 'Auxiliary' or 'OtherAssay'; for each imaging assay type, provide the available data levels (Level2, Level3, Level4), and do not include Level1 data or Electron Microscopy assay types.","The context provided does not include any details about the ""HTAN data"", the ""HTAN WUSTL center"", the structure of the ""id_provenance_r5"" table, or the ""entityId"" relationships. Without information about these tables, fields, and relationships, it is impossible to generate a valid SQL query to fulfill the request. Please provide the relevant table structures and relationships to proceed."
sf_bq163,Which 20 genes in the HTAN scRNAseq MSK-SCLC combined samples dataset show the greatest difference in average X_value expression between female and male epithelial cells specifically in cluster 41 of 74-year-old human stage patients? Please calculate the difference by subtracting male average X_value from female average X_value for each gene,"The provided context is insufficient to generate a SQL query because there are no tables or columns in the database schema related to genes, HTAN scRNAseq datasets, X_value expression, cell types, clusters, or patient stage information. A different dataset with relevant tables and columns is required to address this query."
sf_bq164,"Consolidate metadata from spatial transcriptomics and scRNAseq datasets—including levels 1 through 4 and auxiliary files—for the run ID 'HT264P1-S1H2Fc2U1Z1Bs1-H2Bs2-Test'. Include Filename, HTAN Parent Biospecimen ID, Component, File Format, Entity ID, and Run ID.","The provided context is insufficient to generate the SQL query as there is no table or schema information related to spatial transcriptomics or scRNAseq datasets, nor metadata containing Filename, HTAN Parent Biospecimen ID, Component, File Format, Entity ID, or Run ID. Please provide the relevant table structure or additional information necessary to construct the query."
sf_bq166,"Using segment-level copy number data from the copy_number_segment_allelic_hg38_gdc_r23 dataset restricted to 'TCGA-KIRC' samples, merge these segments with the cytogenetic band definitions in 'CytoBands_hg38' to identify each sample’s maximum copy number per cytoband. Classify these maximum copy numbers into amplifications (>3), gains (=3), homozygous deletions (=0), heterozygous deletions (=1), or normal (=2), then calculate the frequency of each subtype out of the total number of distinct cases, and finally present these frequencies as percentages sorted by chromosome and cytoband.","The context provided is insufficient to generate a SQL query as there are no tables named `copy_number_segment_allelic_hg38_gdc_r23` or `CytoBands_hg38` defined in the provided database schema, which are essential for creating the SQL query as described. Please provide the schema for these tables or confirm their existence and structure in the database to proceed with generating the required SQL query."
bq165,"Can you use CytoConverter genomic coordinates to calculate the frequency of chromosomal gains and losses across a cohort of breast cancer (morphology='3111') and adenocarcinoma (topology='0401') samples? Concretely, please include the number and frequency (2 decimals in percentage) of amplifications (gains of more than 1 copy), gains (1 extra copy), losses (1 copy) and homozygous deletions (loss of 2 copies) for each chromosomal band. And sort the result by the ordinal of each chromosome and the starting-ending base-pair position of each band in ascending order.","The provided context is insufficient to generate the requested SQL query. There is no information or table schema related to CytoConverter genomic coordinates, breast cancer or adenocarcinoma sample details, chromosomal gains and losses, or chromosomal band data in the provided database schema. Therefore, I cannot create a SQL query without the necessary table and column definitions related to genomic data."
bq169,"Retrieve distinct case references, case numbers, investigation numbers, and clone information where a single clone simultaneously exhibits all three of the following genetic alterations: (1) a loss on chromosome 13 between positions 48,303,751 and 48,481,890, (2) a loss on chromosome 17 between positions 7,668,421 and 7,687,490, and (3) a gain on chromosome 11 between positions 108,223,067 and 108,369,102. For each matching clone, display the chromosomal details for each of these three regions (including chromosome number represented by ChrOrd, start position, and end position) and the corresponding karyotype short description from the KaryClone table. Use the CytoConverted and KaryClone.","The provided context is insufficient to generate the requested SQL query since there is no mention of tables named ""CytoConverted"", ""KaryClone"", or any related schema that includes fields for chromosomal data, genetic alterations, or clone information. Therefore, I cannot generate the required SQL query based on the provided table structures."
bq111,"Could you compute, by chromosome, the Pearson correlation between the frequency of copy number aberrations (including amplifications, gains, losses, and deletions) from the Mitelman database for cases with morph = 3111 and topo = 0401, and those computed from TCGA data, returning correlation coefficients and corresponding p-values for each aberration type, ensuring only results with at least five matching records are shown.","The provided context is insufficient to generate the requested SQL query because there is no table or schema related to genetic data, the Mitelman database, or TCGA data as described in the question. None of the tables provided in the schema you've shared contain information on genetic aberrations, chromosomes, or specific identifiers like ""morph"" or ""topo."" Therefore, I'm unable to construct a SQL query based on the current database schema provided."
bq451,"Extract genotype data for single nucleotide polymorphisms (SNPs) on chromosome X, excluding positions where the `start` value is between 59999 and 2699519 or between 154931042 and 155260559. For each sample, identify genotype calls where the genotype array has at least one allele. Classify each genotype call into one of the following categories: homozygous reference alleles (both alleles are 0), homozygous alternate alleles (both alleles are the same and greater than 0), or heterozygous alleles (alleles are different, or any allele is null, and at least one allele is greater than 0). Compute the total number of callable sites (the sum of all three genotype categories), the number of homozygous reference, homozygous alternate, and heterozygous genotype calls, the total number of single nucleotide variants (SNVs) as the sum of homozygous alternate and heterozygous genotype calls, the percentage of heterozygous genotype calls among all SNVs, and the percentage of homozygous alternate genotype calls among all SNVs. Output the sample ID along with these computed counts and percentages, and order the results by the percentage of heterozygous genotype calls among SNVs in descending order, then by sample ID.","The provided context is insufficient to generate the SQL query because there is no table in the given schema that contains relevant fields such as `genotype`, `chromosome`, `start`, or `sample_id` which are essential for extracting and analyzing genotype data for single nucleotide polymorphisms (SNPs). These fields are necessary to filter and classify data based on the specified conditions related to genotypes and chromosomal positions."
bq452,"Identify variants on chromosome 12 and, for each variant, calculate the chi-squared score using allele counts in cases and controls, where cases are individuals from the 'EAS' super population and controls are individuals from all other super populations. Apply Yates's correction for continuity in the chi-squared calculation, ensuring that the expected counts for each allele in both groups are at least 5. Return the start position, end position, and chi-squared score of the top variants where the chi-squared score is no less than 29.71679.","The provided context is insufficient to generate the SQL query because there are no tables with information about genomic variants, chromosome data, allele counts, or population classifications (such as 'EAS' and other super populations). Additional database schema details containing these elements are required to construct the query."
bq453,"In chromosome 17 between positions 41196311 and 41277499, what are the reference names, start and end positions, reference bases, distinct alternate bases, variant types, and the chi-squared scores (calculated from Hardy-Weinberg equilibrium) along with the total number of genotypes, their observed and expected counts for homozygous reference, heterozygous, and homozygous alternate genotypes, as well as allele frequencies (including those from 1KG), for each variant?","The provided context does not include any database tables or information related to genetic data, chromosomes, positions, variant types, or related genetic analysis metrics. To generate a SQL query for the requested genetic information, a database containing genetic data and related attributes would be necessary. Without such a database, it's impossible to create the requested SQL query."
bq454,"For the 1000 Genomes dataset, analyze common autosomal variants (those with an allele frequency of at least 0.05) across different super populations. For each super population, count how many variants are shared by each specific number of samples within that super population. Include in your results the total population size of each super population, whether the variant is common (allele frequency ≥ 0.05), the number of samples having each variant, and the total count of variants shared by that many samples. Only include autosomal variants by explicitly excluding sex chromosomes (X, Y) and mitochondrial DNA (MT) from the analysis. Consider only samples that have at least one alternate allele (non-reference) for the variant.","The provided context is insufficient to generate a SQL query that meets the request because there are no tables or fields related to genomic data, allele frequencies, super populations, or chromosome information in the schema definitions provided. A different database schema that includes the necessary genomic datasets and population information is required to perform this analysis."
sf_bq415,List the top 10 samples in the genome data that have the highest number of positions where there is exactly one alternate allele and the sample's genotype is homozygous for the reference allele (both alleles are 0). Order the results in descending order of these counts.,"The provided context is insufficient to generate a SQL query because there is no table or column information related to genome data, samples, alleles, or genotypes in the database schema provided. To proceed, information about the relevant database structure containing genetic data is necessary."
bq279,Can you provide the number of distinct active and closed bike share stations for each year 2013 and 2014?,"The provided context is insufficient to generate a SQL query for counting distinct active and closed bike share stations for each year 2013 and 2014, as there is no table or column information related to bike share stations or their statuses (active or closed) in the schema descriptions provided."
bq281,"What is the highest number of electric bike rides lasting more than 10 minutes taken by subscribers with 'Student Membership' in a single day, excluding rides starting or ending at 'Mobile Station' or 'Repair Shop'?","The provided context is insufficient to generate the required SQL query because there are no tables related to bike rides, subscribers, or any data that categorizes transportation or membership types. Therefore, I cannot create the SQL query based on the given database schema."
bq282,"Can you tell me the numeric value of the active council district in Austin which has the highest number of bike trips that start and end within the same district, but not at the same station?","The context provided does not include any information about bike trips, council districts, or stations. Therefore, I cannot generate a relevant SQL query based on the current tables and data descriptions. Please provide the appropriate table structures or data related to bike trips and council districts to proceed with generating the SQL query."
bq006,What is the date with the second highest Z-score for daily counts of 'PUBLIC INTOXICATION' incidents in Austin for the year 2016? List the date in the format of '2016-xx-xx'.,"The provided context is insufficient as there is no table or data related to incidents, specifically ""PUBLIC INTOXICATION"" incidents in Austin, and no data or method mentioned for calculating Z-scores. Please provide the relevant table structure or data source that includes incident types and their dates."
sf_bq283,"Among all stations that are currently active, identify those that rank in the top 15 (including ties) based on the total number of trips that start at each station. For each of these stations, return the station ID, the total number of starting trips, the percentage of those trips out of the overall starting trips from active stations, and the average trip duration in minutes. Order the results by the station’s rank.","The provided context is insufficient to generate the requested SQL query as there are no tables or columns related to ""stations,"" ""active status,"" ""trip duration,"" or ""trip counts"" provided in the schema details. Please provide the relevant table structures or specify the details about how trips, stations, and their statuses are stored in the database to proceed with generating an appropriate SQL query."
bq284,"Can you provide a breakdown of the total number of articles into different categories and the percentage of those articles that mention ""education"" within each category from the BBC News?","The provided context is insufficient to generate the SQL query as there is no table related to BBC News, articles, or categories mentioned in these tables."
bq413,"Retrieve the venue titles of publications that have a `date_inserted` from the year 2021 onwards and are associated with a grid whose address city is 'Qianjiang'. For each publication, prioritize the venue title by selecting the journal title first if it exists; if not, then the proceedings title; if that's also unavailable, then the book title; and finally, if none of those are available, the book series title.","The context provided is insufficient to generate a query as there are no tables related to publications, venue titles, journal titles, proceedings titles, book titles, book series titles, or any details about a `date_inserted` or grid with address cities. Further information about the database schema including these details is required to write the requested query."
bq425,"Using data from ChEMBL Release 23, retrieve all distinct molecules associated with the company 'SanofiAventis,' listing the trade name and the most recent approval date for each molecule. Make sure to keep only the latest approval date per molecule and ensure the company field precisely matches 'SanofiAventis' without relying on other fields.","The provided tables and context do not contain information related to ChEMBL data, molecules, trade names, or approval dates. Therefore, I cannot generate a SQL query to retrieve data about molecules associated with 'SanofiAventis' from ChEMBL Release 23. Please provide the relevant database schema or confirm if the data resides in a different dataset."
bq430,"Find pairs of different molecules tested in the same assay and standard type, where both have 10–15 heavy atoms, fewer than 5 activities in that assay, fewer than 2 duplicate activities, non-null standard values, and pChEMBL values over 10. For each pair, report the maximum heavy atom count, the latest publication date (calculated based on the document's rank within the same journal and year, and map it to a synthetic month and day), the highest document ID, classify the change in standard values as 'increase', 'decrease', or 'no-change' based on their values and relations, and generate UUIDs from their activity IDs and canonical SMILES.","The provided context is insufficient to generate the requested SQL query. There are no tables related to molecules, assays, activities, standard values, pChEMBL values, or related attributes necessary to fulfill the query requirements. Additional information about the database schema including relevant tables and their relationships is needed to proceed."
bq023,"Using the 2018 5-Year American Community Survey (ACS) for median incomes at the census tract level and the 2020 Federal Election Commission (FEC) individual contributions dataset filtered for donors in New York, matched to census tract geographies via a ZIP code to census tract crosswalk, calculate and list the average political donation amount and the median income for each census tract located in Kings County (Brooklyn), New York. ","The context provided is insufficient to generate an SQL query as there are no tables available in the provided schema related to the 2018 5-Year American Community Survey (ACS), the 2020 Federal Election Commission (FEC) individual contributions dataset, or a ZIP code to census tract crosswalk. These datasets are essential for matching political contributions to census tract geographies and median income data. Please provide the relevant tables or data sources to proceed with the SQL query generation."
bq094,"Please provide a list of all 2016 committees that supported at least one candidate and received a total amount of individual contributions between $0 and $200 (inclusive of more than $0 and less than $200) where these small-dollar contributions sum to more than $0 overall. For each qualifying committee, include its name, the number of unique candidates it supported, the candidates’ names in alphabetical order (separated by commas), and the total sum of these small-dollar donations received by the committee.","The provided context is insufficient to generate the requested SQL query. The tables provided do not contain any direct information about committees, their support for candidates, individual contributions, or details linking committees to candidates. Additional data is required to fulfill the requirements of this query."
bq287,"Among all Utah ZIP codes, what is the 2017 American Community Survey employment rate for the population aged 16 or older in the ZIP code that has the fewest FDIC-insured bank locations?","The provided context is insufficient to generate the SQL query requested because:

1. There is no table containing employment rate data from the American Community Survey.
2. There is no table listing FDIC-insured bank locations by ZIP code.

To proceed with this request, you would need a table or dataset that includes employment rates per ZIP code from the American Community Survey and another dataset that counts FDIC-insured bank locations per ZIP code."
bq432,"Please provide the food events data where both \""date_created\"" and \""date_started\"" are between January 1 and January 31, 2015, apply the following data cleansing steps: split reactions and outcomes fields into arrays by commas, handle special numeric patterns in the products_brand_name field (where a digit is followed by comma and another digit) by preserving those numeric patterns while replacing other "", "" with "" -- "", replace "", "" with "" -- "" in products_industry_code, products_role, and products_industry_name fields, and calculate industry_code_length and brand_name_length as the array lengths after splitting. ","The provided context does not include a table or fields that directly relate to ""food events,"" ""date_created,"" ""date_started,"" ""reactions,"" ""outcomes,"" ""products_brand_name,"" ""products_industry_code,"" ""products_role,"" or ""products_industry_name."" Therefore, it's impossible to generate a SQL query based on the requirements you've specified due to the lack of relevant table and column information in the provided schema."
bq285,Could you provide me with the zip code of the location that has the highest number of bank institutions in Florida?,The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
bq288,"What is the total number of all banking institutions in the state that has the highest sum of assets from banks established between January 1, 1900, and December 31, 2000, with institution names starting with 'Bank'?","The provided context is insufficient to generate the requested SQL query because there is no table containing information about banking institutions, their assets, or establishment dates. Additionally, there's no data regarding states in relation to banking institutions in the given tables."
sf_bq412,"Please retrieve the page URLs, first shown time, last shown time, removal reason, violation category, and the lower and upper bounds of times shown for the five most recently removed ads in the Croatia region (region code 'HR'), where the times shown availability date is null, the times shown lower bound exceeds 10,000, the times shown upper bound is below 25,000, and the ads used at least one non-unused audience selection approach among demographics, geographic location, contextual signals, customer lists, or topics of interest, ordering the resulting ads by their last shown time in descending order.","The provided context is insufficient to generate the requested SQL query. There are no tables or fields related to ads, region codes, removal reasons, violation categories, or audience selection approaches in the provided schema. Additional schema information about advertisement data and related attributes is necessary to write the query."
sf_bq423,"Between January 1, 2023, and January 1, 2024, which image-type advertisement on the topic of Health, published by a verified advertiser located in Cyprus, was shown in Croatia, has times_shown_availability_date as NULL (meaning the times shown data is available), utilized demographic information, geo-location targeting, contextual signals, customer lists, and topics of interest without any of these selection methods being unused, and additionally had its first shown date strictly after January 1, 2023, and last shown date strictly before January 1, 2024? Among such ads, provide the page URL of the one with the highest upper bound of times shown.","The context you provided doesn't contain any information related to advertisements, advertisers, or any tables that include advertisement-specific data such as times shown, demographics, page URLs, or geolocation targeting. Therefore, I cannot generate a SQL query based on the current database schema provided. Please provide the relevant table schema or additional information related to advertisements to proceed with generating the required SQL query."
sf_bq070,"Could you provide a clean, structured dataset from dicom_all table that only includes SM images marked as VOLUME from the TCGA-LUAD and TCGA-LUSC collections, excluding any slides with compression type “other,” where the specimen preparation step explicitly has “Embedding medium” set to “Tissue freezing medium,” and ensuring that the tissue type is only “normal” or “tumor” and the cancer subtype is reported accordingly?","The provided context is insufficient to generate the requested SQL query as there is no description or schema for a table named `dicom_all`. Without this information, it's impossible to know the fields available for filtering on image type, collection type, compression type, specimen preparation details, tissue type, or cancer subtype. Please provide the relevant table schema or further details on the available fields in the `dicom_all` table."
sf_bq320,"In the dicom_pivot table, how many unique StudyInstanceUID values exactly match the SegmentedPropertyTypeCodeSequence of ""15825003"" (case-insensitive) and also have a collection_id of either ""Community"" or ""nsclc_radiomics""?","The provided context does not include a table named ""dicom_pivot"" or definitions for columns such as ""StudyInstanceUID"" or ""SegmentedPropertyTypeCodeSequence."" Therefore, I cannot generate a valid SQL query to answer your question based on the available information. Please provide the schema or additional details about the ""dicom_pivot"" table and its relevant columns."
sf_bq321,"How many unique StudyInstanceUIDs are there from the DWI, T2 Weighted Axial, Apparent Diffusion Coefficient series, and T2 Weighted Axial Segmentations in the 'qin_prostate_repeatability' collection?","The provided context is insufficient to generate a SQL query for the question regarding unique StudyInstanceUIDs in specific series types within the 'qin_prostate_repeatability' collection. There is no table or column information related to medical imaging, StudyInstanceUIDs, or specific series types in the provided schema."
sf_bq323,"Within the 'prostatex' collection, for MRI sequences where the Modality is 'MR', assign the label 't2w_prostateX' to sequences whose SeriesDescription contains 't2_tse_tra' and 'adc_prostateX' to sequences whose SeriesDescription contains 'ADC'. For all sequences labeled as 't2w_prostateX' or 'adc_prostateX', calculate the average Repetition Time, the average Echo Time, and the average Slice Thickness, and then compute the sum of these averages to obtain the combined overall average.","The provided context is insufficient to generate the required SQL query as there is no table or schema definition related to the 'prostatex' collection, MRI sequences, Modality, SeriesDescription, Repetition Time, Echo Time, or Slice Thickness in the given database schema. Please provide the relevant table structure or confirm the table and columns to proceed."
sf_bq417,"Please provide identification details, study and series information, storage location, and total size in MB for the medical images belonging to male patients who are exactly 18 years old based on the numeric portion of the PatientAge field, where the BodyPartExamined is set to 'MEDIASTINUM' and the study date is strictly after September 1, 2014.","The provided context is insufficient to generate a SQL query for the requested information. The tables given do not contain any information about medical images, patient ages, body parts examined, or study dates. Further details about a relevant database schema containing medical imaging data are needed to construct an appropriate query."
sf_bq455,"Identify the top five CT scan series by size (in MiB), including their SeriesInstanceUID, series number, patient ID, and series size. These series must be from the CT modality and not part of the 'nlst' collection. Exclude any series where the ImageType is classified as 'LOCALIZER' or where the TransferSyntaxUID is either '1.2.840.10008.1.2.4.70' or '1.2.840.10008.1.2.4.51' (i.e., JPEG compressed). The selected series must have consistent slice intervals, exposure levels, image orientation (with only one unique ImageOrientationPatient value), pixel spacing, image positions (both z-axis and xy positions), and pixel dimensions (rows and columns). Ensure that the number of images matches the number of unique z-axis positions, indicating no duplicate slices. Additionally, the z-axis component of the cross product of the x and y direction cosines from ImageOrientationPatient must have an absolute value between 0.99 and 1.01, ensuring alignment with the expected imaging plane. Finally, order the results by series size in descending order and limit the output to the top five series satisfying these conditions.","The provided context is insufficient to generate the requested SQL query. There is no information about the database schema related to medical images, CT scans, or DICOM data such as `SeriesInstanceUID`, `ImageType`, `TransferSyntaxUID`, `ImageOrientationPatient`, or other DICOM attributes mentioned in the query requirements. A proper schema including tables and fields concerning medical imaging data would be needed to create such a query."
sf_bq456,"Please retrieve from the dicom_all table each PatientID, StudyInstanceUID, StudyDate, and the CodeMeaning of the FindingSite for patients whose StudyDate is in the year 2001, along with the maximum values of each of the following measurements identified by their CodeMeaning (Elongation, Flatness, Least Axis in 3D Length, Major Axis in 3D Length, Maximum 3D Diameter of a Mesh, Minor Axis in 3D Length, Sphericity, Surface Area of Mesh, Surface to Volume Ratio, Volume from Voxel Summation, and Volume of Mesh), ensuring that the quantitative_measurements table is joined on segmentationInstanceUID matching the SOPInstanceUID in dicom_all, and grouping by PatientID, StudyInstanceUID, StudyDate, and FindingSite CodeMeaning.","The provided context is insufficient to generate the requested SQL query. The tables mentioned in the query (`dicom_all` and `quantitative_measurements`) are not defined in the provided context. Additionally, the fields mentioned like `PatientID`, `StudyInstanceUID`, `StudyDate`, `CodeMeaning`, `segmentationInstanceUID`, and `SOPInstanceUID` are not present in any of the described tables. Without access to these tables and field definitions, the SQL query cannot be created."
sf_bq324,How many frames in total are present across all whole slide microscopy images from the TCGA-BRCA collection that use the SM modality and include an eosin-based staining step in their SpecimenPreparationSequence?,"The provided context is insufficient to generate the required SQL query as there is no information or table structure related to ""whole slide microscopy images"", ""TCGA-BRCA collection"", ""SM modality"", or ""SpecimenPreparationSequence"" including ""eosin-based staining step"". Consequently, it is not possible to construct a SQL query without having access to the relevant schema or data structure that includes these details."
bq418,"Determine which three lowest-level Reactome pathways (with TAS evidence) have the highest chi-squared statistics, considering only Homo sapiens targets associated with sorafenib under the conditions that the median assay value is ≤ 100 and both low and high assay values are ≤ 100 or null. For each of these three pathways, how many of these targets and non-targets lie within the pathway and outside it?","The provided context is insufficient to generate a SQL query to answer the question about Reactome pathways, TAS evidence, Homo sapiens targets associated with sorafenib, assay values, and chi-squared statistics because there is no information or description of tables that contain data about Reactome pathways, TAS evidence, assay values, chi-squared statistics, or any data specifically linking to the drug sorafenib and Homo sapiens targets. Please provide the schema or details of relevant tables that contain this information to proceed with generating an appropriate SQL query."
bq330,"Which Colorado zip code has the highest concentration of bank locations per block group, based on the overlap between zip codes and block groups?","The provided context is insufficient to generate a SQL query for determining the concentration of bank locations per block group in Colorado based on zip codes. There is no data or table mentioned that contains information about bank locations, block groups, or their overlap with zip codes. Additionally, there is no information on how to measure or calculate the concentration of bank locations."
bq398,What are the top three debt indicators for Russia based on the highest debt values?,"The provided context is insufficient to generate the requested SQL query as there is no table or data related to ""debt indicators"" or specific country-based financial metrics such as those for Russia in the database schema provided. To accurately generate this SQL query, I would need access to a table that includes debt indicators by country, or similar financial data."
bq399,"Which high-income country had the highest average crude birth rate respectively in each region, and what are their corresponding average birth rate, during the 1980s?","The provided context is insufficient to generate a SQL query because there are no tables containing information about crude birth rates or income classifications for countries. Additionally, there's no data specified for the 1980s timeframe in terms of birth rates. To proceed, tables containing both the economic classification of countries (like high-income) and their birth rates over time are necessary."
bq457,"Get details of repositories that use specific feature toggle libraries. For each repository, include the full name with owner, hosting platform type, size in bytes, primary programming language, fork source name (if any), last update timestamp, the artifact and library names of the feature toggle used, and the library's programming languages. Include repositories that depend on the specified feature toggle libraries, defined by their artifact names, library names, platforms, and languages.","The provided context is insufficient to generate the requested SQL query as there are no tables related to repositories, feature toggle libraries, dependencies, or any relevant data that matches the description provided in the request. Further information about the database schema containing data about repositories, libraries, and their dependencies is necessary to proceed with the SQL query generation."
bq227,"Could you provide the annual percentage shares, rounded to two decimal places, of the top 5 minor crime categories from 2008 in London's total crimes, with each year displayed in one row?","The provided context is insufficient to generate the desired SQL query because there are no tables related to crime data, including details about crime categories, crime counts, or geographical information relating to London. Please provide the relevant database schema or specify the correct tables and fields that contain the necessary data for generating this query."
bq232,Could you provide the total number of 'Other Theft' incidents within the 'Theft and Handling' category for each year in the Westminster borough?,"The provided context is insufficient to generate the requested SQL query as there is no table or column described that clearly includes data regarding 'incidents', 'categories', or 'boroughs' related to 'Theft and Handling' or 'Other Theft'."
bq228,"Please provide a list of the top three major crime categories in the borough of Barking and Dagenham, along with the number of incidents in each category.","The provided context is insufficient to generate the SQL query as there is no direct information or table containing data about crime incidents, their categories, or specific details related to the borough of Barking and Dagenham."
bq229,"Using the bigquery-public-data.open_images dataset, can you provide a count of how many distinct image URLs are categorized as 'cat' (where the image has label '/m/01yrx' with confidence=1) and how many distinct image URLs are categorized as 'other' (meaning they have no cat label '/m/01yrx' at all)?","The provided context is insufficient to generate the SQL query as there is no information or schema provided about tables from the ""bigquery-public-data.open_images"" dataset in the context. To write a SQL query, details about the relevant tables and their structures from the dataset are necessary. Please provide the schema for the relevant tables from the ""bigquery-public-data.open_images"" dataset, or access to explore the dataset directly."
bq230,"Using the crops dataset, find the total 2022 production figures, measured in bushels, for corn from the 'FIELD CROPS' category and mushrooms from the 'HORTICULTURE' group for each U.S. state. Only include data rows where 'statisticcat_desc' is 'PRODUCTION', 'agg_level_desc' is 'STATE', 'value' is not null, and ensure that for corn the 'unit_desc' is 'BU'. Combine both results so that each state’s 2022 corn and mushroom totals are presented.","The context provided is insufficient to generate the SQL query.

There is no ""crops dataset"" or tables related to agriculture, field crops, or horticulture provided in the schema. Therefore, it's not possible to create a SQL query for retrieving 2022 production figures for corn and mushrooms without the relevant dataset and table structure."
bq326,"Based on the World Bank global population dataset and the World Bank health nutrition population dataset, how many countries experienced an increase of more than 1% from the previous year to 2018 in both their total population and per capita current health expenditure (PPP)?","The provided context is insufficient to generate a SQL query for the request involving analysis on the World Bank global population dataset and the World Bank health nutrition population dataset, as neither of these datasets nor related tables are described in the schema provided. To proceed, I would need access to or detailed descriptions of the relevant tables and their structures from these datasets."
bq424,"List the top 10 countries with respect to the total amount of long-term external debt in descending order, excluding those without a specified region.","The provided tables and context do not contain any information related to ""long-term external debt"" or similar financial data for countries. Therefore, it is not possible to generate a SQL query to answer the question based on the available data."
bq327,"How many debt indicators for Russia have a value of 0, excluding NULL values?","The provided context does not contain any table or column related to debt indicators or specific country-related financial data. Therefore, I cannot generate a SQL query to answer your question. Please provide the relevant table and column details or ensure that the question pertains to the existing schema."
bq328,Which region has the highest median GDP (constant 2015 US$) value?,"The provided context and table schemas do not include any data or tables related to GDP (Gross Domestic Product) values, either current or constant (such as constant 2015 US$). As such, it is not possible to generate a SQL query to determine which region has the highest median GDP value based on the information provided."
sf_bq331,"Find the top three users who have authored the first message in forum topics, ranked in descending order by their message scores, where a message score is defined as the number of distinct users who voted on that message. For each of these users, provide their username and the absolute difference between their message score and the average message score across all first messages in forum topics.","The provided context is insufficient to generate the SQL query as there are no tables or columns related to forum topics, forum messages, users who authored messages, or details about voting on these messages. The required data to address the query is not available in the described database schema."
sf_bq380,"Using the data from Forum Message Votes and Users tables, find the top three users who have received the most distinct upvotes on the Kaggle forum. For each of these users, list their username, the total number of distinct upvotes they have received (based on the ToUserId field), and the total number of distinct upvotes they have given (based on the FromUserId field), sorted by the number of upvotes received in descending order and restricted to only the top three results.","The provided context is insufficient to generate the SQL query as there are no tables named ""Forum Message Votes"" or ""Users"" described in the schema details. Please provide the relevant table structures or confirm their names to proceed with the query generation."
sf_bq370,"How many customers have orders and invoices that match at the line-item level and, when aggregated, result in each customer having an equal count of orders and invoices as well as an identical total value for the orders and invoices?","SELECT 
    o.customer_id
FROM 
    orders o
JOIN 
    orderlines ol ON o.id = ol.order_id
JOIN 
    Invoice i ON o.customer_id = i.CustomerId
JOIN 
    invoice_items ii ON i.InvoiceId = ii.InvoiceId
GROUP BY 
    o.customer_id
HAVING 
    COUNT(DISTINCT o.id) = COUNT(DISTINCT i.InvoiceId)
    AND SUM(ol.amount) = SUM(ii.UnitPrice * ii.Quantity);"
sf_bq371,"In the year 2013, considering each invoice’s total value as the product of unit price and quantity and grouping by the quarter (Q1, Q2, Q3, Q4) in which the invoice date occurs, what is the difference between the maximum and minimum average invoice values across these quarters?","WITH InvoiceValues AS (
    SELECT
        strftime('%m', modifieddate) AS month,
        unitprice * orderqty AS total_value
    FROM salesorderdetail
    WHERE strftime('%Y', modifieddate) = '2013'
),
QuarterlyAverages AS (
    SELECT
        CASE
            WHEN month IN ('01', '02', '03') THEN 'Q1'
            WHEN month IN ('04', '05', '06') THEN 'Q2'
            WHEN month IN ('07', '08', '09') THEN 'Q3'
            WHEN month IN ('10', '11', '12') THEN 'Q4'
        END AS quarter,
        AVG(total_value) AS average_value
    FROM InvoiceValues
    GROUP BY quarter
)

SELECT MAX(average_value) - MIN(average_value) AS difference_between_quarters
FROM QuarterlyAverages;"
sf_bq372,"Among all orders that do not appear in the invoice table, for each customer category calculate the maximum lost order value, then determine which customer category’s maximum lost order value is closest to the overall average of these maximum lost order values across all categories?",SELECT DISTINCT category_id FROM category;
sf_bq373,"Using the invoice date to determine each month of the year 2014, and summing the total invoice line amounts for each customer across these months, what is the median of the resulting average monthly spending across all customers?","The provided context lacks a table or column explicitly mentioning ""invoice date"" or ""invoice line amounts."" Therefore, I cannot generate the exact SQL query requested. Please provide a table schema that includes details about invoices and their associated amounts, or confirm if any existing table should be interpreted differently to align with invoice data."
bq393,"I want to identify users who had activity followed by inactivity. Specifically, find the user ID and their corresponding month number (counting from their first activity month) for the user with the highest month number who became inactive (no activity recorded) after their last recorded activity month. For this analysis, only consider data up until September 10, 2024, and ensure the month number represents the count of months since the user's first activity. The user should have at least one month where they were expected to be active (within their activity span) but actually had no records.","WITH UserActivity AS (
    SELECT 
        user_id,
        strftime('%Y-%m', start_date) AS activity_month,
        MIN(strftime('%Y-%m', start_date)) OVER (PARTITION BY user_id) AS first_activity_month,
        MAX(strftime('%Y-%m', start_date)) OVER (PARTITION BY user_id) AS last_activity_month
    FROM 
        shopping_cart_users
    WHERE 
        start_date <= '2024-09-10'
    GROUP BY 
        user_id, activity_month
),

MonthRange AS (
    SELECT 
        user_id,
        first_activity_month,
        last_activity_month,
        generate_series(
            strftime('%Y%m', first_activity_month),
            strftime('%Y%m', last_activity_month),
            '1 month'
        ) AS expected_month
    FROM 
        UserActivity
),

MissingActivity AS (
    SELECT 
        m.user_id,
        m.first_activity_month,
        m.expected_month,
        IFNULL(u.activity_month, 'inactive') AS status
    FROM 
        MonthRange m
    LEFT JOIN 
        UserActivity u ON m.user_id = u.user_id AND m.expected_month = u.activity_month
    WHERE 
        u.activity_month IS NULL
),

MonthNumbers AS (
    SELECT 
        user_id,
        first_activity_month,
        expected_month,
        (strftime('%Y', expected_month) - strftime('%Y', first_activity_month)) * 12 
        + (strftime('%m', expected_month) - strftime('%m', first_activity_month)) + 1 AS month_number
    FROM 
        MissingActivity
    WHERE 
        status = 'inactive'
)

SELECT 
    user_id,
    MAX(month_number) AS highest_inactive_month_number
FROM 
    MonthNumbers
GROUP BY 
    user_id
ORDER BY 
    highest_inactive_month_number DESC
LIMIT 1;"
bq403,Which three years in 2012-2017 have the smallest absolute difference between median revenue and median functional expenses for organizations filing IRS 990 forms? Please output three years and respective differences.,"The provided context is insufficient to generate a SQL query for the request regarding IRS 990 forms. There is no table described in the schema that contains information on organizations' median revenue or median functional expenses. To proceed, a table containing such financial data for organizations would be necessary."
bq397,"After removing any duplicate records from the rev_transactions dataset, identify each channel grouping that has transactions from more than one country. For each such channel grouping, find the country with the highest total number of transactions and report both the country name and the sum of transactions for that channel grouping.","The provided context is insufficient to generate the SQL query. There is no mention or description of a ""rev_transactions"" dataset in the provided table schemas. Additionally, the structure of the ""rev_transactions"" dataset, including necessary columns such as transaction counts and country identifiers, is not described. Please provide the schema or additional details about the ""rev_transactions"" dataset to proceed with creating an accurate SQL query."
bq402,"Calculate the conversion rate from unique visitors to purchasers by using data exclusively from the `web_analytics` table in the `data-to-insights.ecommerce` dataset. A visitor is defined as a unique `fullVisitorId` present in the table, while a purchaser is a visitor who has at least one transaction recorded (`totals.transactions` is not null). The conversion rate is computed by dividing the number of unique purchasers by the total number of unique visitors. Additionally, calculate the average number of transactions per purchaser, considering only those visitors who have made at least one transaction.","The provided context does not include a table named `web_analytics` in the `data-to-insights.ecommerce` dataset or any fields named `fullVisitorId` or `totals.transactions`. Therefore, it is impossible to generate the requested SQL query with the provided context. Please provide the schema for the relevant table or confirm the dataset details to proceed with generating the correct SQL query."
sf_bq160,"Please provide the creation date, title, parent forum title, reply count, distinct user replies count, total upvotes, and total views for the earliest five forum topics that belong to any sub-forum under the parent forum titled ""General"". The reply count should be calculated as the total number of messages associated with each topic, and the distinct user replies count should be the number of unique users who have posted messages in the topic. The upvotes should be calculated as the total number of upvotes on all messages within each topic. If any values are missing or None, please treat them as zero","The provided context is insufficient to generate the requested SQL query because there is no table or column information related to forums, sub-forums, forum topics, messages, replies, user information, upvotes, or views in the database schema provided."
sf_bq167,"Identify the pair of Kaggle users involved in ForumMessageVotes such that one user has given the other the greatest distinct number of upvotes, then also display how many upvotes that recipient returned. Present the usernames of both users, the total distinct upvotes one received from the other, and the upvotes they gave back, sorting by the highest received count and then by the highest given count, and show only the top result.","The provided context is insufficient to generate the requested SQL query, as there are no tables or fields related to ""Kaggle users"" or ""ForumMessageVotes"" described in the schema information provided. To accurately create the query, I would need the schema for tables related to Kaggle users and their forum message voting, including how votes are tracked (upvotes and downvotes) and linked between users. Please provide the relevant table structures or further details about the database schema."
sf_bq171,"Whose Forum message upvotes are closest to the average in 2019? If there’s a tie, tell me the one with the alphabetically first username.","The provided context is insufficient to generate a valid SQL query as there is no table described in the schema that contains information about forum messages, upvotes, or related timestamps. Additionally, there is no mention of usernames in the tables provided. Please provide the relevant table structures or clarify the context to proceed with generating the appropriate SQL query."
sf_bq118,"Among individuals identified as white, how much higher is the average number of deaths from ICD-10 codes whose descriptions contain the word “discharge” (specifically excluding “Urethral discharge,” “Discharge of firework,” and “Legal intervention involving firearm discharge”) compared to the average number of deaths from ICD-10 codes whose descriptions contain the word “vehicle,” when aggregated by age groups?","The provided context does not contain information about ICD-10 codes, descriptions of these codes, or data specifically related to causes of death such as ""discharge"" or ""vehicle"" related incidents. The tables mainly focus on collisions, vehicle details, parties involved in accidents, and some general statistics about incidents, none of which include medical coding or detailed cause of death descriptions necessary to perform the analysis requested. Therefore, I cannot generate a SQL query based on the current context."
sf_bq072,"Please provide, for each age from 12 through 18 (inclusive), the total number of deaths and the number of deaths among individuals identified as Black (based on race descriptions containing the word ‘black’), specifically for deaths associated with ICD-10 codes whose descriptions include the word ‘vehicle’ and for deaths associated with ICD-10 codes whose descriptions include the word ‘firearm.’ Use the EntityAxisConditions table to determine which ICD-10 codes were involved in each death, rather than joining ICD-10 code information directly on the death records.","The provided context is insufficient to generate the SQL query because the tables necessary for this query, such as those containing death records, ICD-10 codes, or EntityAxisConditions, are not described in the provided schema information. Without these tables, it's impossible to query the data as requested. Please provide the schema for relevant tables or any additional information needed to access death records and ICD-10 code descriptions."
ga001,I want to know the preferences of customers who purchased the Google Navy Speckled Tee in December 2020. What other product was purchased with the highest total quantity alongside this item?,The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
ga002,Tell me the most purchased other products and their quantities by customers who bought the Google Red Speckled Tee each month for the three months starting from November 2020.,The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
ga003,"I'm trying to evaluate which board types were most effective on September 15, 2018. Can you find out the average scores for each board type from the quick play mode completions on that day?","The provided context is insufficient to generate the requested SQL query as there are no tables or columns mentioned in the schema related to board types, quick play mode completions, or scores. Please provide additional information or clarify the database schema related to the query."
ga004,Can you figure out the average difference in pageviews between users who bought something and those who didn’t in December 2020? Just label anyone who was involved in purchase events as a purchaser.,The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
ga008,"Could you provide the total number of page views for each day in November 2020 as well as the average number of page views per user on those days, restricted to users who made at least one purchase in November 2020?",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
ga017,How many distinct users viewed the most frequently visited page during January 2021?,"SELECT COUNT(DISTINCT client_ip) AS distinct_users
FROM web_page_visits
WHERE (app_id, page_no) = (
    SELECT app_id, page_no
    FROM web_page_visits
    WHERE strftime('%Y-%m', visit_time) = '2021-01'
    GROUP BY app_id, page_no
    ORDER BY COUNT(*) DESC
    LIMIT 1
) AND strftime('%Y-%m', visit_time) = '2021-01';"
ga007,"Please find out what percentage of the page views on January 2, 2021, were for PDP type pages.",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
ga013,"I want to know all the pages visited by user 1402138.5184246691 on January 2, 2021. Please show the names of these pages and adjust the names to PDP or PLP where necessary.",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
ga018,"On January 2nd, 2021, I want to determine the percentage of times users transition from a product list page (PLP) view to a product detail page (PDP) view within the same session, using only page_view events. Could you calculate how many PLP views eventually led to a PDP view in the same session on that date, and then provide the resulting percentage of PLP-to-PDP transitions?",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
ga032,"Can you generate the navigation flow for user with pseudo_id '1362228.4966015575' on January 28th 2021, showing only the page_view events? Please connect the page titles with '>>' between each step, convert product detail pages to 'PDP' and product listing pages to 'PLP' based on the URL structure, and merge adjacent identical pages so they only appear once in the sequence. I need to understand how this specific user navigated through our website on that day.","WITH EventFlow AS (
    SELECT 
        s.cookie_id,
        w.friendly_url,
        s.event_time,
        CASE 
            WHEN w.friendly_url LIKE '%/product/%' THEN 'PDP'
            WHEN w.friendly_url LIKE '%/category/%' THEN 'PLP'
            ELSE w.friendly_url
        END AS page_type
    FROM shopping_cart_events s
    JOIN web_pages w ON s.page_id = w.page_no
    JOIN shopping_cart_users u ON s.cookie_id = u.cookie_id
    WHERE u.user_id = '1362228.4966015575'
      AND DATE(s.event_time) = '2021-01-28'
      AND s.event_type = 1 -- Assuming event_type = 1 for page views
    ORDER BY s.event_time
),
DistinctFlow AS (
    SELECT 
        cookie_id,
        page_type,
        event_time,
        LAG(page_type, 1) OVER (PARTITION BY cookie_id ORDER BY event_time) AS prev_page_type
    FROM EventFlow
)
SELECT 
    STRING_AGG(page_type, ' >> ') WITHIN GROUP (ORDER BY event_time) AS navigation_flow
FROM (
    SELECT 
        cookie_id,
        page_type,
        event_time
    FROM DistinctFlow
    WHERE page_type <> prev_page_type OR prev_page_type IS NULL
) AS FilteredFlow"
ga031,"I want to know the user session conversion rate on January 2nd, 2021, using only 'page_view' events. The conversion rate should be calculated as the percentage of user visits that reached both the Home and Checkout Confirmation pages in one session, relative to those that landed on the Home page.",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
ga006,"For the date range November 1–30, 2020, can you retrieve each user_pseudo_id and its average purchase revenue in USD per session for users who had more than one purchase session, considering only events with event_name='purchase' and a non-null ecommerce.purchase_revenue_in_usd, grouping sessions by the ga_session_id from event_params","The context provided does not include tables or columns that directly match with ""user_pseudo_id"", ""event_name"", ""ecommerce.purchase_revenue_in_usd"", or ""ga_session_id"" from ""event_params"". Therefore, it is not possible to generate the requested SQL query based on the provided table structures."
ga009,"Could you tell me the average number of engaged sessions per user for December 2020, counting only those sessions where the event parameter 'session_engaged' is equal to '1' and using 'user_pseudo_id' combined with the 'ga_session_id' to identify distinct sessions?","The provided context is insufficient to generate the SQL query as requested. There is no table or columns mentioned that include 'session_engaged', 'user_pseudo_id', or 'ga_session_id'. These identifiers are necessary to formulate the query, and without a clear indication of which table or fields these relate to, the query cannot be generated. Please provide details on the tables or data structure where these fields exist to proceed with generating the SQL query."
ga010,Can you give me an overview of our website traffic for December 2020? I'm particularly interested in the channel with the fourth highest number of sessions.,"SELECT channel, COUNT(*) AS sessions_count
FROM web_events
WHERE strftime('%Y-%m', occurred_at) = '2020-12'
GROUP BY channel
ORDER BY sessions_count DESC
LIMIT 1 OFFSET 3;"
ga014,"Can you provide the total number of sessions for each traffic channel in December 2020, using the information from the 'event_params' ",The context provided is insufficient as there is no mention or description of a table named 'event_params' in the provided schema. Please check the table names and provide the correct table to use or additional details about the 'event_params' table.
ga011,"What is the page with the second highest total page views, after cleaning up its URL (removing extra slashes) and extracting the correct page path, on the website 'shop.googlemerchandisestore.com' during December 2020?","The provided context does not include any information or table that directly links data to the specific website 'shop.googlemerchandisestore.com' or contains data filtered by specific dates such as December 2020. The tables related to web analytics like `web_pages` and `web_page_visits` do not include a field for the website URL or specific date filtering fields related to 2020 or December.

To proceed with the query generation, additional information linking the `app_id` in `web_pages` to specific websites or filtering records by a specific time period is necessary. Without such mappings or date-related fields, generating the exact query required is not feasible."
ga012,"On November 30, 2020, identify the item category with the highest tax rate by dividing tax value in usd by purchase revenue in usd for purchase events, and then retrieve the transaction IDs, total item quantities, and both purchase revenue in usd and purchase revenue for those purchase events in that top-tax-rate category.","The provided context is insufficient to generate the required SQL query because there are no tables or columns directly related to ""tax rate,"" ""purchase revenue,"" ""item category,"" or ""purchase events"" mentioned in the table schemas provided. Further details about these elements or a relevant table containing this information would be necessary to construct the SQL query."
ga019,Could you determine what percentage of users either did not uninstall our app within seven days or never uninstalled it after installing during August and September 2018?,"SELECT
  ROUND(
    (
      COUNT(CASE 
        WHEN withdraw_date IS NULL OR julianday(withdraw_date) - julianday(register_date) > 7 THEN 1 
      END) * 100.0 / COUNT(*)
    ), 2
  ) AS Percentage
FROM
  mst_users
WHERE
  strftime('%Y-%m', register_date) IN ('2018-08', '2018-09');"
ga030,"Can you group users by the week of their first session start, starting from July 2, 2018? For each group, calculate the retention rate in the fourth week (i.e., the percentage of users from the original group who returned in the fourth week after their first session). Please identify the cohort with the highest retention rate in the fourth week, and name the group by the Monday date of the cohort's first session week. Return the result in the format ""YYYY-MM-DD"".","WITH FirstSession AS (
    SELECT 
        user_id, 
        MIN(start_date) AS first_session_date
    FROM 
        shopping_cart_users
    GROUP BY 
        user_id
),

Cohorts AS (
    SELECT 
        user_id,
        DATE_TRUNC('week', first_session_date::date) + INTERVAL '1 day' * (1 - EXTRACT(ISODOW FROM first_session_date::date)) AS cohort_week_start
    FROM 
        FirstSession
    WHERE 
        first_session_date::date >= '2018-07-02'
),

Week4Returns AS (
    SELECT 
        A.user_id,
        A.cohort_week_start,
        COUNT(DISTINCT B.start_date) > 0 AS returned_in_week_4
    FROM 
        Cohorts A
    LEFT JOIN 
        shopping_cart_users B 
    ON 
        A.user_id = B.user_id
        AND B.start_date::date >= A.cohort_week_start + INTERVAL '21 days'
        AND B.start_date::date < A.cohort_week_start + INTERVAL '28 days'
    GROUP BY 
        A.user_id, A.cohort_week_start
),

RetentionRates AS (
    SELECT 
        cohort_week_start,
        COUNT(*) AS total_users,
        SUM(returned_in_week_4::int) AS users_returned_week_4,
        SUM(returned_in_week_4::int) * 100.0 / COUNT(*) AS retention_rate_week_4
    FROM 
        Week4Returns
    GROUP BY 
        cohort_week_start
),

MaxRetention AS (
    SELECT 
        cohort_week_start, 
        retention_rate_week_4
    FROM 
        RetentionRates
    ORDER BY 
        retention_rate_week_4 DESC
    LIMIT 1
)

SELECT 
    TO_CHAR(cohort_week_start, 'YYYY-MM-DD') AS highest_retention_cohort_start
FROM 
    MaxRetention;"
ga005,"Conduct a weekly cohort analysis for user retention, starting from July 9, 2018, and ending on October 2, 2018. Group users by the week of their first session_start event (with weeks starting on Monday), and identify new users as those where the event_date matches the date of their user_first_touch_timestamp. Calculate the Week 2 retention rate for each weekly cohort, defined as the percentage of users who had a session_start event exactly 2 weeks after their first week. Only include cohorts from July 9, 2018 through September 17, 2018 (the last cohort that can be analyzed for 2-week retention given the available data). Present the results with each weekly cohort and its corresponding Week 2 retention rate, ordered by cohort date.","The provided context does not include details about tables or columns that track user sessions, event dates, or user_first_touch_timestamps. As such, it is not possible to generate the requested SQL query for conducting a weekly cohort analysis for user retention. Specific data regarding user sessions and timestamps is essential for identifying new users and calculating retention rates, which is not available in the provided table schemas."
ga028,"Please perform a 7-day retention analysis for users who first session start the app during the week starting on July 2, 2018. For each week from Week 0 (the week of their first session) to Week 4, provide the total number of new users in Week 0 and the number of retained users for each subsequent week. Ensuring that you only count events up to October 2, 2018, and group dates by Monday-based weeks","WITH first_sessions AS (
    SELECT 
        user_id, 
        MIN(start_date) AS first_session_date
    FROM shopping_cart_users
    WHERE start_date >= '2018-07-02' AND start_date < '2018-07-09'
    GROUP BY user_id
),
weekly_retention AS (
    SELECT 
        fs.user_id,
        fs.first_session_date,
        FLOOR(DATEDIFF(scu.start_date, fs.first_session_date) / 7) AS week_number
    FROM first_sessions fs
    JOIN shopping_cart_users scu ON fs.user_id = scu.user_id
    WHERE scu.start_date < '2018-10-02'
    GROUP BY fs.user_id, week_number
)
SELECT 
    week_number,
    COUNT(DISTINCT CASE WHEN week_number = 0 THEN user_id END) AS new_users,
    COUNT(DISTINCT user_id) AS retained_users
FROM weekly_retention
WHERE week_number BETWEEN 0 AND 4
GROUP BY week_number
ORDER BY week_number;"
ga020,"Which quickplay event type had the lowest user retention rate during the second week after their initial engagement, for users who first engaged between August 1 and August 15, 2018, as measured by the presence of session_start events??",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
ga021,"What is the retention rate for users two weeks after their initial quickplay event, calculated separately for each quickplay event type, within the period from July 2, 2018, to July 16, 2018? Please focus on users who started a session (session_start) during this period.","The provided context is insufficient to generate the SQL query as requested. The databases do not contain tables or fields explicitly related to quickplay events or session starts, which are necessary for calculating user retention based on these specific events and activities within a given timeframe. Further details about tables containing user events or session information would be required to construct an appropriate query."
ga022,"Could you please help me get the weekly customer retention rate in September 2018 for new customers who first used our app (first_open event) within the first week starting from September 1st, 2018 (timezone in Shanghai)? The retention rates should cover the following weeks 1, 2, and 3 period after the initial use and display them in column format.",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
ga025,"For all users who first opened the app in September 2018 and then uninstalled within seven days, I want to know what percentage of them experienced an app crash (app_exception). The calculation should be done by converting the timestamps to dates first, and then calculating the days to uninstall based on the dates. Only users who uninstalled within 7 days and experienced a crash should be considered in the final percentage.","The provided context is insufficient to generate the required SQL query as there are no tables with relevant information about app usage specifics such as app installations, uninstallations, or app crash (app_exception) events. Therefore, I cannot generate a valid SQL query based on the current dataset."
local002,"Can you calculate the 5-day symmetric moving average of predicted toy sales for December 5 to 8, 2018, using daily sales data from January 1, 2017, to August 29, 2018, with a simple linear regression model? Finally provide the sum of those four 5-day moving averages?","The provided context is insufficient to generate a SQL query for the following reasons:

1. There is no table provided in the context that includes predicted toy sales data or any indication of how to derive such predictions using SQL.
2. The task requires a simple linear regression model and subsequent moving average calculations, which are typically not handled directly in SQL without extensive setup or using specific SQL extensions that support predictive analytics.
3. There are no details regarding the structure of the toy sales data, such as table names or columns, which are necessary to write a SQL query.

This task is better suited for a programming environment with statistical or machine learning capabilities, such as Python with pandas and scikit-learn libraries, rather than SQL."
local003,"According to the RFM definition document, calculate the average sales per order for each customer within distinct RFM segments, considering only 'delivered' orders. Use the customer unique identifier. Clearly define how to calculate Recency based on the latest purchase timestamp and specify the criteria for classifying RFM segments. The average sales should be computed as the total spend divided by the total number of orders. Please analyze and report the differences in average sales across the RFM segments",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
local004,"Could you tell me the number of orders, average payment per order and customer lifespan in weeks of the 3 custumers with the highest average payment per order, where the lifespan is calculated by subtracting the earliest purchase date from the latest purchase date in days, dividing by seven, and if the result is less than seven days, setting it to 1.0?","WITH CustomerPayments AS (
    SELECT 
        o.customer_id,
        COUNT(DISTINCT o.order_id) AS number_of_orders,
        AVG(op.payment_value) AS avg_payment_per_order,
        MIN(DATE(o.order_purchase_timestamp)) AS first_purchase_date,
        MAX(DATE(o.order_purchase_timestamp)) AS last_purchase_date
    FROM 
        olist_orders AS o
    INNER JOIN 
        olist_order_payments AS op ON o.order_id = op.order_id
    GROUP BY 
        o.customer_id
),
CustomerLifespan AS (
    SELECT 
        customer_id,
        number_of_orders,
        avg_payment_per_order,
        first_purchase_date,
        last_purchase_date,
        CASE 
            WHEN JULIANDAY(last_purchase_date) - JULIANDAY(first_purchase_date) < 7 THEN 1.0
            ELSE (JULIANDAY(last_purchase_date) - JULIANDAY(first_purchase_date)) / 7.0
        END AS lifespan_in_weeks
    FROM 
        CustomerPayments
)
SELECT 
    customer_id,
    number_of_orders,
    avg_payment_per_order,
    ROUND(lifespan_in_weeks, 2) AS lifespan_in_weeks
FROM 
    CustomerLifespan
ORDER BY 
    avg_payment_per_order DESC
LIMIT 3;"
local007,"Could you help me calculate the average single career span value in years for all baseball players? Please precise the result as a float number. First, calculate the difference in years, months, and days between the debut and final game dates. For each player, the career span is computed as the sum of the absolute number of years, plus the absolute number of months divided by 12, plus the absolute number of days divided by 365. Round each part to two decimal places before summing. Finally, average the career spans and round the result to a float number.","The required context to calculate the average single career span value in years for baseball players, specifically the debut and final game dates for each player, is not provided in the given database schema. There are no tables containing fields that directly store these dates for player careers. Therefore, it is not possible to generate a SQL query to calculate this average career span as described."
local008,"I would like to know the given names of baseball players who have achieved the highest value of games played, runs, hits, and home runs, with their corresponding score values.","SELECT 
    p.name_given AS player_given_name,
    MAX(b.g) AS max_games_played,
    MAX(b.r) AS max_runs,
    MAX(b.h) AS max_hits,
    MAX(b.hr) AS max_home_runs
FROM 
    batting b
JOIN 
    player p ON b.player_id = p.player_id
GROUP BY 
    p.name_given
ORDER BY 
    max_games_played DESC, 
    max_runs DESC, 
    max_hits DESC, 
    max_home_runs DESC
LIMIT 1;"
local009,What is the distance of the longest route where Abakan is either the departure or destination city (in kilometers)?,"SELECT MAX(delivery_distance_meters) AS longest_route_distance
FROM deliveries
JOIN hubs ON deliveries.driver_id = hubs.hub_id
WHERE hubs.hub_city = 'Abakan';"
local010,"Distribute all the unique city pairs into the distance ranges 0, 1000, 2000, 3000, 4000, 5000, and 6000+, based on their average distance of all routes between them. Then how many pairs are there in the distance range with the fewest unique city paires?","WITH city_pairs AS (
    SELECT 
        c1.city_id AS city1_id, 
        c2.city_id AS city2_id,
        AVG((ACOS(COS(RADIANS(90-c1.latitude)) * COS(RADIANS(90-c2.latitude)) + SIN(RADIANS(90-c1.latitude)) * SIN(RADIANS(90-c2.latitude)) * COS(RADIANS(c1.longitude-c2.longitude))) * 6371)) AS average_distance
    FROM 
        cities c1
    JOIN 
        cities c2 ON c1.city_id != c2.city_id
    GROUP BY 
        c1.city_id, c2.city_id
),
distance_ranges AS (
    SELECT
        city1_id,
        city2_id,
        average_distance,
        CASE
            WHEN average_distance < 1000 THEN '0-999'
            WHEN average_distance < 2000 THEN '1000-1999'
            WHEN average_distance < 3000 THEN '2000-2999'
            WHEN average_distance < 4000 THEN '3000-3999'
            WHEN average_distance < 5000 THEN '4000-4999'
            WHEN average_distance < 6000 THEN '5000-5999'
            ELSE '6000+'
        END AS distance_range
    FROM 
        city_pairs
),
range_counts AS (
    SELECT
        distance_range,
        COUNT(*) AS num_pairs
    FROM 
        distance_ranges
    GROUP BY 
        distance_range
)
SELECT
    MIN(num_pairs) AS fewest_pairs_in_range
FROM 
    range_counts;"
local015,"Please calculate the fatality rate for motorcycle collisions, separated by helmet usage. Specifically, calculate two percentages: 1) the percentage of motorcyclist fatalities in collisions where parties (drivers or passengers) were wearing helmets, and 2) the percentage of motorcyclist fatalities in collisions where parties were not wearing helmets. For each group, compute this by dividing the total number of motorcyclist fatalities by the total number of collisions involving that group. Use the parties table to determine helmet usage (from party_safety_equipment fields).","SELECT 
    Helmet_Usage,
    SUM(CASE WHEN p.party_number_killed > 0 THEN 1 ELSE 0 END) * 100.0 / COUNT(DISTINCT c.case_id) AS Fatality_Rate
FROM 
    parties p
JOIN 
    collisions c ON p.case_id = c.case_id
WHERE 
    c.motorcycle_collision = 1
    AND (p.party_safety_equipment_1 LIKE '%helmet%' OR p.party_safety_equipment_2 LIKE '%helmet%')
GROUP BY 
    Helmet_Usage;"
local017,In which year were the two most common causes of traffic accidents different from those in other years?,"WITH CauseCounts AS (
    SELECT 
        CAST(strftime('%Y', collision_date) AS INTEGER) AS year,
        pcf_violation_category,
        COUNT(*) AS total
    FROM 
        collisions
    GROUP BY 
        year,
        pcf_violation_category
),
RankedCauses AS (
    SELECT
        year,
        pcf_violation_category,
        total,
        RANK() OVER (PARTITION BY year ORDER BY total DESC) AS rank
    FROM 
        CauseCounts
),
TopTwoCommonCauses AS (
    SELECT
        year,
        GROUP_CONCAT(pcf_violation_category) AS common_causes
    FROM
        RankedCauses
    WHERE
        rank <= 2
    GROUP BY
        year
)
SELECT DISTINCT 
    YearA.year AS Year
FROM
    TopTwoCommonCauses YearA
CROSS JOIN
    TopTwoCommonCauses YearB
WHERE 
    YearA.year <> YearB.year AND
    YearA.common_causes <> YearB.common_causes
GROUP BY
    YearA.year
HAVING 
    COUNT(*) = (SELECT COUNT(DISTINCT year) - 1 FROM TopTwoCommonCauses);"
local018,"For the primary collision factor violation category that was the most common cause of traffic accidents in 2021, how many percentage points did its share of annual road incidents in 2021 decrease compared to its share in 2011?","WITH YearlyData AS (
    SELECT 
        strftime('%Y', collision_date) AS year,
        pcf_violation_category,
        COUNT(*) AS total_accidents
    FROM 
        collisions
    WHERE 
        strftime('%Y', collision_date) IN ('2011', '2021')
    GROUP BY 
        year, pcf_violation_category
),
YearlySum AS (
    SELECT 
        year,
        SUM(total_accidents) AS year_total
    FROM 
        YearlyData
    GROUP BY 
        year
),
MaxCategoryPerYear AS (
    SELECT 
        y.year,
        yd.pcf_violation_category,
        yd.total_accidents,
        ys.year_total,
        CAST(yd.total_accidents AS REAL) / ys.year_total * 100 AS category_percentage
    FROM 
        YearlyData yd
        JOIN YearlySum ys ON yd.year = ys.year
),
RankCategories AS (
    SELECT 
        year,
        pcf_violation_category,
        category_percentage,
        RANK() OVER (PARTITION BY year ORDER BY category_percentage DESC) AS rank
    FROM 
        MaxCategoryPerYear
),
TopCategories AS (
    SELECT 
        year,
        pcf_violation_category,
        category_percentage
    FROM 
        RankCategories
    WHERE 
        rank = 1
)
SELECT 
    t2011.pcf_violation_category AS category_2011,
    t2021.pcf_violation_category AS category_2021,
    t2011.category_percentage AS percentage_2011,
    t2021.category_percentage AS percentage_2021,
    (t2011.category_percentage - t2021.category_percentage) AS percentage_point_decrease
FROM 
    TopCategories t2011
    JOIN TopCategories t2021 ON t2011.year = '2011' AND t2021.year = '2021';"
local019,"For the NXT title that had the shortest match (excluding titles with ""title change""), what were the names of the two wrestlers involved?","SELECT w1.name AS Winner, w2.name AS Loser
FROM Matches
JOIN Wrestlers w1 ON Matches.winner_id = w1.id
JOIN Wrestlers w2 ON Matches.loser_id = w2.id
WHERE Matches.title_id = 'NXT' AND Matches.title_change = 0
ORDER BY CAST(Matches.duration AS INTEGER)
LIMIT 1;"
local026,"Please help me identify the top 3 bowlers who, in the overs where the maximum runs were conceded in each match, gave up the highest number of runs in a single over across all matches. For each of these bowlers, provide the match in which they conceded these maximum runs. Only consider overs that had the most runs conceded within their respective matches, and among these, determine which bowlers conceded the most runs in a single over overall.","WITH TotalRunsPerOver AS (
    SELECT
        match_id,
        over_id,
        innings_no,
        SUM(runs_scored + COALESCE(extra_runs, 0)) AS total_runs
    FROM
        batsman_scored
    LEFT JOIN
        extra_runs USING (match_id, over_id, ball_id, innings_no)
    GROUP BY
        match_id, over_id, innings_no
),
MaxRunsPerMatch AS (
    SELECT
        match_id,
        MAX(total_runs) AS max_runs
    FROM
        TotalRunsPerOver
    GROUP BY
        match_id
),
TopOversByRuns AS (
    SELECT
        t.match_id,
        t.over_id,
        t.innings_no,
        t.total_runs
    FROM
        TotalRunsPerOver t
    JOIN
        MaxRunsPerMatch m ON t.match_id = m.match_id AND t.total_runs = m.max_runs
),
BowlerPerformanceInTopOvers AS (
    SELECT
        b.match_id,
        b.over_id,
        b.bowler,
        tor.total_runs,
        b.innings_no
    FROM
        ball_by_ball b
    JOIN
        TopOversByRuns tor ON b.match_id = tor.match_id AND b.over_id = tor.over_id AND b.innings_no = tor.innings_no
)
SELECT
    match_id,
    bowler,
    over_id,
    innings_no,
    total_runs
FROM
    BowlerPerformanceInTopOvers
ORDER BY
    total_runs DESC
LIMIT 3;"
local020,Which bowler has the lowest bowling average per wicket taken?,"SELECT
    bowler,
    AVG(batsman_scored.runs_scored) AS average_per_wicket
FROM
    wicket_taken
JOIN
    batsman_scored USING (match_id, over_id, ball_id, innings_no)
GROUP BY
    bowler
ORDER BY
    average_per_wicket ASC
LIMIT 1;"
local021,Could you calculate the average of the total runs scored by all strikers who have scored more than 50 runs in any single match?,"SELECT AVG(total_runs) AS average_runs FROM (
    SELECT match_id, striker, SUM(runs_scored) AS total_runs
    FROM batsman_scored
    JOIN ball_by_ball USING(match_id, over_id, ball_id, innings_no)
    GROUP BY match_id, striker
    HAVING SUM(runs_scored) > 50
) AS strikers_over_50_runs;"
local022,Retrieve the names of players who scored no less than 100 runs in a match while playing for the team that lost that match.,The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
local023,"Please help me find the names of top 5 players with the highest average runs per match in season 5, along with their batting averages.",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
local024,"Can you help me find the top 5 countries whose players have the highest average of their individual average runs per match across all seasons? Specifically, for each player, calculate their average runs per match over all matches they played, then compute the average of these player averages for each country, and include these country batting averages in the result.",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
local025,"For each match, considering every innings, please combine runs from both batsman scored and extra runs for each over, then identify the single over with the highest total runs, retrieve the bowler for that over from the ball by ball table, and calculate the average of these highest over totals across all matches, ensuring that all runs and bowler details are accurately reflected.","WITH CombinedRuns AS (
    SELECT b.match_id, b.over_id, b.innings_no, COALESCE(SUM(bs.runs_scored), 0) + COALESCE(SUM(er.extra_runs), 0) AS total_runs
    FROM ball_by_ball b
    LEFT JOIN batsman_scored bs ON b.match_id = bs.match_id AND b.over_id = bs.over_id AND b.ball_id = bs.ball_id AND b.innings_no = bs.innings_no
    LEFT JOIN extra_runs er ON b.match_id = er.match_id AND b.over_id = er.over_id AND b.ball_id = er.ball_id AND b.innings_no = er.innings_no
    GROUP BY b.match_id, b.over_id, b.innings_no
),
MaxRunsPerMatch AS (
    SELECT match_id, innings_no, MAX(total_runs) AS max_runs
    FROM CombinedRuns
    GROUP BY match_id, innings_no
)
SELECT AVG(max_runs) AS average_highest_over_total
FROM (
    SELECT m.match_id, m.innings_no, m.max_runs, b.bowler
    FROM MaxRunsPerMatch m
    JOIN ball_by_ball b ON m.match_id = b.match_id AND m.innings_no = b.innings_no
    JOIN (
        SELECT match_id, innings_no, over_id, MAX(total_runs) AS total_runs
        FROM CombinedRuns
        GROUP BY match_id, innings_no, over_id
    ) cr ON m.match_id = cr.match_id AND m.innings_no = cr.innings_no AND m.total_runs = m.max_runs
    WHERE b.over_id = cr.over_id
) t;"
local028,"Could you generate a report that shows the number of delivered orders for each month in the years 2016, 2017, and 2018? Each column represents a year, and each row represents a month","SELECT 
    strftime('%m', order_delivered_customer_date) AS Month,
    SUM(CASE WHEN strftime('%Y', order_delivered_customer_date) = '2016' THEN 1 ELSE 0 END) AS Delivered_2016,
    SUM(CASE WHEN strftime('%Y', order_delivered_customer_date) = '2017' THEN 1 ELSE 0 END) AS Delivered_2017,
    SUM(CASE WHEN strftime('%Y', order_delivered_customer_date) = '2018' THEN 1 ELSE 0 END) AS Delivered_2018
FROM 
    orders
WHERE 
    order_status = 'delivered' AND
    strftime('%Y', order_delivered_customer_date) IN ('2016', '2017', '2018')
GROUP BY 
    strftime('%m', order_delivered_customer_date)
ORDER BY 
    Month;"
local031,"What is the highest monthly delivered orders volume in the year with the lowest annual delivered orders volume among 2016, 2017, and 2018?","WITH YearlyDeliveredOrders AS (
    SELECT
        strftime('%Y', order_delivered_customer_date) AS year,
        COUNT(*) AS delivered_orders_count
    FROM orders
    WHERE
        order_status = 'delivered' AND
        strftime('%Y', order_delivered_customer_date) IN ('2016', '2017', '2018')
    GROUP BY year
),

YearWithLowestOrders AS (
    SELECT year
    FROM YearlyDeliveredOrders
    ORDER BY delivered_orders_count ASC
    LIMIT 1
),

MonthlyDeliveredVolume AS (
    SELECT
        strftime('%Y', order_delivered_customer_date) AS year,
        strftime('%m', order_delivered_customer_date) AS month,
        COUNT(*) AS delivered_orders_count
    FROM orders
    WHERE
        order_status = 'delivered' AND
        strftime('%Y', order_delivered_customer_date) = (SELECT year FROM YearWithLowestOrders)
    GROUP BY year, month
)

SELECT
    month,
    MAX(delivered_orders_count) AS highest_monthly_delivered_orders
FROM MonthlyDeliveredVolume"
local029,"Please identify the top three customers, based on their customer_unique_id, who have the highest number of delivered orders, and provide the average payment value, city, and state for each of these customers.","SELECT
  c.customer_unique_id,
  c.customer_city,
  c.customer_state,
  AVG(o.order_amount) AS average_payment_value
FROM
  orders o
JOIN
  customers c ON o.customer_id = c.customer_id
WHERE
  o.order_status = 'delivered'
GROUP BY
  c.customer_unique_id, c.customer_city, c.customer_state
ORDER BY
  COUNT(o.order_id) DESC
LIMIT 3;"
local030,"Among all cities with delivered orders, find the five cities whose summed payments are the lowest, then calculate the average of their total payments and the average of their total delivered order counts.",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
local032,"Could you help me find the sellers who excel in the following categories, considering only delivered orders: the seller with the highest number of distinct customer unique IDs, the seller with the highest profit (calculated as price minus freight value), the seller with the highest number of distinct orders, and the seller with the most 5-star ratings? For each category, please provide the seller ID and the corresponding value, labeling each row with a description of the achievement.","-- The provided context is insufficient to generate the full query as requested because:
-- 1. There is no table or column explicitly containing customer unique IDs to calculate the seller with the highest number of distinct customer IDs.
-- 2. There is no table or column containing order status (e.g., 'delivered') to filter by only delivered orders.
-- 3. There is no table or column containing rating data to identify sellers with the most 5-star ratings."
local034,"Could you help me calculate the average of the total number of payments made using the most preferred payment method for each product category, where the most preferred payment method in a category is the one with the highest number of payments?",The context provided does not include necessary details about product categories or how payments are linked to product categories. Additional tables that link payments to product categories and products are required to generate the requested SQL query.
local037,"Identify the top three product categories whose most commonly used payment type has the highest number of payments across all categories, and specify the number of payments made in each category using that payment type.",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
local035,"In the “olist_geolocation” table, please identify which two consecutive cities, when sorted by geolocation_state, geolocation_city, geolocation_zip_code_prefix, geolocation_lat, and geolocation_lng, have the greatest distance between them based on the difference in distance computed between each city and its immediate predecessor in that ordering.","WITH SortedCities AS (
  SELECT
    geolocation_city,
    geolocation_state,
    geolocation_zip_code_prefix,
    geolocation_lat,
    geolocation_lng,
    LAG(geolocation_lat) OVER (ORDER BY geolocation_state, geolocation_city, geolocation_zip_code_prefix, geolocation_lat, geolocation_lng) AS prev_lat,
    LAG(geolocation_lng) OVER (ORDER BY geolocation_state, geolocation_city, geolocation_zip_code_prefix, geolocation_lat, geolocation_lng) AS prev_lng
  FROM olist_geolocation
),
Distances AS (
  SELECT
    geolocation_city,
    geolocation_state,
    geolocation_zip_code_prefix,
    geolocation_lat,
    geolocation_lng,
    prev_lat,
    prev_lng,
    CASE 
      WHEN prev_lat IS NULL OR prev_lng IS NULL THEN NULL
      ELSE 6371 * acos(
        cos(radians(geolocation_lat)) * cos(radians(prev_lat)) * cos(radians(prev_lng) - radians(geolocation_lng)) +
        sin(radians(geolocation_lat)) * sin(radians(prev_lat))
      )
    END AS distance
  FROM SortedCities
)
SELECT 
  geolocation_city,
  geolocation_state,
  geolocation_zip_code_prefix,
  geolocation_lat,
  geolocation_lng,
  distance
FROM Distances
ORDER BY distance DESC
LIMIT 2;"
local038,"Could you help me determine which actor starred most frequently in English-language children's category films that were rated either G or PG, had a running time of 120 minutes or less, and were released between 2000 and 2010? Please provide the actor's full name.","SELECT 
  a.first_name,
  a.last_name,
  COUNT(*) AS number_of_films
FROM 
  actor a
JOIN 
  film_actor fa ON a.actor_id = fa.actor_id
JOIN 
  film f ON fa.film_id = f.film_id
WHERE 
  f.rating IN ('G', 'PG')
  AND f.length <= 120
  AND f.release_year BETWEEN 2000 AND 2010
  AND f.language_id = (SELECT language_id FROM language WHERE name = 'English')
GROUP BY 
  a.actor_id
ORDER BY 
  number_of_films DESC
LIMIT 1;"
local039,"Please help me find the film category with the highest total rental hours in cities where the city's name either starts with ""A"" or contains a hyphen. ",SELECT DISTINCT city_name FROM cities;
local040,"In the combined dataset that unifies the trees data with the income data by ZIP code, filling missing ZIP values where necessary, which three boroughs, restricted to records with median and mean income both greater than zero and a valid borough name, contain the highest number of trees, and what is the average mean income for each of these three boroughs?","SELECT 
    t.boroname,
    COUNT(t.tree_id) AS total_trees,
    AVG(i.Estimate_Mean_income) AS average_mean_income
FROM 
    trees t
JOIN 
    income_trees i ON t.zipcode = i.zipcode
WHERE 
    t.boroname IS NOT NULL 
    AND i.Estimate_Median_income > 0 
    AND i.Estimate_Mean_income > 0
GROUP BY 
    t.boroname
ORDER BY 
    total_trees DESC
LIMIT 3;"
local041,What percentage of trees in the Bronx have a health status of Good?,"SELECT ROUND((COUNT(*) * 100.0 / (SELECT COUNT(*) FROM trees WHERE boroname = 'Bronx')), 2) AS percentage_good_health
FROM trees
WHERE boroname = 'Bronx' AND health = 'Good';"
local049,Can you help me calculate the average number of new unicorn companies per year in the top industry from 2019 to 2021?,The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
local054,"Could you tell me the first names of customers who spent less than $1 on albums by the best-selling artist, along with the amounts they spent?","The provided context is insufficient to generate the required SQL query. The databases and tables necessary for linking customers, their purchases, and the identification of ""best-selling artist"" (such as tables containing customer details, artist sales data, or direct album purchases linking to customers) are not defined or provided in the context. Additional information about how sales or transactions are structured and how artists are linked to sales would be necessary to fulfill this request."
local055,"Identify the artist with the highest overall sales of albums (tie broken by alphabetical order) and the artist with the lowest overall sales of albums (tie broken by alphabetical order), then calculate the amount each customer spent specifically on those two artists’ albums. Next, compute the average spending for the customers who purchased from the top-selling artist and the average spending for the customers who purchased from the lowest-selling artist, and finally return the absolute difference between these two averages.","WITH ArtistSales AS (
    SELECT a.ArtistId, SUM(t.UnitPrice * s.quantity_sold) AS TotalSales
    FROM sales s
    JOIN tracks t ON s.prod_id = t.TrackId
    JOIN albums a ON t.AlbumId = a.AlbumId
    GROUP BY a.ArtistId
),
RankedArtists AS (
    SELECT
        ArtistId,
        TotalSales,
        RANK() OVER (ORDER BY TotalSales DESC, ArtistId) AS SalesRankAsc,
        RANK() OVER (ORDER BY TotalSales ASC, ArtistId) AS SalesRankDesc
    FROM ArtistSales
),
TopAndLowestArtists AS (
    SELECT ArtistId AS TopArtistId FROM RankedArtists WHERE SalesRankAsc = 1
    UNION
    SELECT ArtistId AS LowestArtistId FROM RankedArtists WHERE SalesRankDesc = 1
),
CustomerSpending AS (
    SELECT
        s.cust_id,
        a.ArtistId,
        SUM(t.UnitPrice * s.quantity_sold) AS AmountSpent
    FROM sales s
    JOIN tracks t ON s.prod_id = t.TrackId
    JOIN albums a ON t.AlbumId = a.AlbumId
    JOIN TopAndLowestArtists tla ON a.ArtistId IN (tla.TopArtistId, tla.LowestArtistId)
    GROUP BY s.cust_id, a.ArtistId
),
AverageSpending AS (
    SELECT
        ArtistId,
        AVG(AmountSpent) AS AvgSpending
    FROM CustomerSpending
    GROUP BY ArtistId
)
SELECT ABS(MAX(AvgSpending) - MIN(AvgSpending)) AS AbsoluteDifference
FROM AverageSpending;"
local198,"Using the sales data, what is the median value of total sales made in countries where the number of customers is greater than 4?",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
local056,Which customer has the highest average monthly change in payment amounts? Provide the customer's full name.,"The context provided is insufficient to generate the required SQL query as there is no table linking customer names to their payment records or transactions. Additionally, there is no information on how to calculate monthly changes in payment amounts from the tables provided. Therefore, I am unable to generate the SQL query to find the customer with the highest average monthly change in payment amounts and provide the customer's full name."
local058,"Can you provide a list of hardware product segments along with their unique product counts for 2020 in the output, ordered by the highest percentage increase in unique fact sales products from 2020 to 2021?","WITH ProductCounts2020 AS (
    SELECT 
        p.segment,
        COUNT(DISTINCT h.product_code) AS unique_products_2020
    FROM 
        hardware_dim_product p
    JOIN 
        hardware_fact_sales_monthly h ON p.product_code = h.product_code
    WHERE 
        h.fiscal_year = 2020
    GROUP BY 
        p.segment
),
ProductCounts2021 AS (
    SELECT 
        p.segment,
        COUNT(DISTINCT h.product_code) AS unique_products_2021
    FROM 
        hardware_dim_product p
    JOIN 
        hardware_fact_sales_monthly h ON p.product_code = h.product_code
    WHERE 
        h.fiscal_year = 2021
    GROUP BY 
        p.segment
),
PercentageIncrease AS (
    SELECT 
        pc2020.segment,
        pc2020.unique_products_2020,
        pc2021.unique_products_2021,
        ((pc2021.unique_products_2021 - pc2020.unique_products_2020) * 1.0 / pc2020.unique_products_2020) * 100 AS percent_increase
    FROM 
        ProductCounts2020 pc2020
    JOIN 
        ProductCounts2021 pc2021 ON pc2020.segment = pc2021.segment
)
SELECT 
    segment,
    unique_products_2020,
    percent_increase
FROM 
    PercentageIncrease
ORDER BY 
    percent_increase DESC;"
local059,"For the calendar year 2021, what is the overall average quantity sold of the top three best-selling hardware products (by total quantity sold) in each division?","WITH RankedProducts AS (
    SELECT
        product_code,
        SUM(sold_quantity) AS total_sold
    FROM hardware_fact_sales_monthly
    WHERE fiscal_year = 2021
    GROUP BY product_code
    ORDER BY total_sold DESC
    LIMIT 3
)

SELECT
    AVG(total_sold) AS average_quantity_sold
FROM RankedProducts;"
local060,"In the United States, for Q4 2019 and Q4 2020, first select only those cities where total sales (with no promotions) rose by at least 20% from Q4 2019 to Q4 2020. Among these cities, rank products by their overall sales (still excluding promotions) in those quarters and take the top 20%. Then compute each top product’s share of total sales in Q4 2019 and Q4 2020 and calculate the difference in share from Q4 2019 to Q4 2020, returning the results in descending order of that share change.","The context provided is insufficient to generate the SQL query as requested because:

1. There is no mention of a ""cities"" table or column that links sales data to specific cities. This information is crucial to filter sales by city.
2. The provided ""sales"" table does not contain a direct link to city data.
3. Further information is needed on how to identify Q4 data specifically. While there is a ""time_id"" that likely links to another table containing detailed date information, the related ""times"" table is not described in the context.
4. The instruction to exclude promotions requires identifying non-promotional sales, but the mechanism to discern promotional from non-promotional sales (e.g., a specific promo_id indicating no promotion) is not specified.

To proceed with constructing a query, additional details about the linkage between sales data and city information, and the method to filter promotional and non-promotional sales, are required."
local063,"Among all products sold in the United States with promo_id=999, considering only those cities whose sales increased by at least 20% from Q4 2019 (calendar_quarter_id=1772) to Q4 2020 (calendar_quarter_id=1776), which product that ranks in the top 20% of total sales has the smallest percentage-point change in its share of total sales between these two quarters?","The provided context is insufficient to generate the desired SQL query because there are no tables containing information about cities, calendar quarters, or the mapping of sales to specific geographic locations in the United States. Moreover, there is no table linking products to specific quarters or providing a way to calculate changes in sales or market share over time directly."
local061,"What is the average projected monthly sales in USD for France in 2021, considering only product sales with promotions where promo_total_id = 1 and channels where channel_total_id = 1, by taking each product’s monthly sales from 2019 and 2020, calculating the growth rate from 2019 to 2020 for that same product and month, applying this growth rate to project 2021 monthly sales, converting all projected 2021 amounts to USD with the 2021 exchange rates, and finally averaging and listing them by month?","WITH SalesGrowth AS (
    SELECT 
        s.prod_id,
        strftime('%Y-%m', s.time_id) AS month,
        SUM(CASE WHEN strftime('%Y', s.time_id) = '2019' THEN s.quantity_sold ELSE 0 END) AS qty_2019,
        SUM(CASE WHEN strftime('%Y', s.time_id) = '2020' THEN s.quantity_sold ELSE 0 END) AS qty_2020
    FROM 
        sales s
    WHERE 
        s.promo_id = 1 
        AND s.channel_id = 1
    GROUP BY 
        s.prod_id, month
),
ProjectedSales AS (
    SELECT 
        sg.prod_id,
        sg.month,
        sg.qty_2020 * (1 + (sg.qty_2020 - sg.qty_2019) / NULLIF(sg.qty_2019, 0)) AS projected_qty
    FROM 
        SalesGrowth sg
),
ConvertedSales AS (
    SELECT 
        ps.month,
        ps.projected_qty * c.to_us AS usd_sales
    FROM 
        ProjectedSales ps
    JOIN 
        currency c ON c.year = 2021 AND c.month = CAST(strftime('%m', ps.month) AS INTEGER) AND c.country = 'France'
)
SELECT 
    strftime('%m', cs.month) AS month,
    AVG(cs.usd_sales) AS avg_monthly_sales_usd
FROM 
    ConvertedSales cs
GROUP BY 
    strftime('%m', cs.month);"
local050,"What is the median of the average monthly projected sales in USD for France in 2021, calculated by using the monthly sales data from 2019 and 2020 (filtered by promo_total_id=1 and channel_total_id=1), applying the growth rate from 2019 to 2020 to project 2021, converting to USD based on the currency table, and then determining the monthly averages before finding their median?","The provided context is insufficient to generate the requested SQL query due to several reasons:

1. **Missing Tables and Columns**: There is no table or column information provided about `promo_total_id` and `channel_total_id`. These filters are specified in the query request but not present in the table schemas provided.
   
2. **Growth Rate Calculation**: The request involves calculating a growth rate from sales data of 2019 to 2020. However, the tables provided do not clearly define how to link sales data specifically to a `promo_total_id` or `channel_total_id`, nor is there a direct indication of how to filter data by year except for the potential use of the `mth` column in `monthly_sales` which is formatted as a date.

3. **Sales Data Table**: The `monthly_sales` table could potentially be used for sales data, but without a clear understanding of the `promo_total_id` and `channel_total_id`, it's not possible to apply the specified filters.

4. **Currency Conversion**: The `currency` table indeed contains data for currency conversion, but again, due to the lack of detailed information on how sales data can be filtered and linked with these currency rates specifically for France, it's challenging to proceed.

Given these gaps, an accurate SQL query cannot be created based on the current information. Further details about the relevant tables and filters would be necessary to proceed."
local062,"Please group all Italian customers into ten buckets for December 2021 by summing their profits from all products purchased (where profit is calculated as quantity_sold multiplied by the difference between unit_price and unit_cost), then divide the overall range of total monthly profits into ten equal intervals. For each bucket, provide the number of customers, and identify the minimum and maximum total profits within that bucket.","WITH CustomerProfits AS (
    SELECT 
        s.cust_id,
        SUM((c.unit_price - c.unit_cost) * s.quantity_sold) AS total_profit
    FROM 
        sales s
    JOIN 
        costs c ON s.prod_id = c.prod_id AND s.time_id = c.time_id AND s.promo_id = c.promo_id AND s.channel_id = c.channel_id
    JOIN
        customers cu ON s.cust_id = cu.customer_id
    WHERE 
        s.time_id BETWEEN '2021-12-01' AND '2021-12-31'
        AND cu.country = 'Italy'
    GROUP BY 
        s.cust_id
),
ProfitRange AS (
    SELECT 
        MIN(total_profit) AS min_profit,
        MAX(total_profit) AS max_profit
    FROM 
        CustomerProfits
),
ProfitBuckets AS (
    SELECT 
        total_profit,
        NTILE(10) OVER (ORDER BY total_profit) AS bucket
    FROM 
        CustomerProfits,
        ProfitRange
),
BucketSummary AS (
    SELECT 
        bucket,
        COUNT(cust_id) AS num_customers,
        MIN(total_profit) AS min_profit,
        MAX(total_profit) AS max_profit
    FROM 
        ProfitBuckets
    GROUP BY 
        bucket
)
SELECT 
    bucket,
    num_customers,
    min_profit,
    max_profit
FROM 
    BucketSummary
ORDER BY 
    bucket;"
local067,Can you provide the highest and lowest profits for Italian customers segmented into ten evenly divided tiers based on their December 2021 sales profits?,SELECT * FROM sales LIMIT 1;
local070,"Please examine our database records for Chinese cities (country_code_2 = 'cn') during July 2021 and identify both the shortest and longest streaks of consecutive date entries. For each date in these streaks, return exactly one record per date along with the corresponding city name. In your output, please ensure the first letter of each city name is capitalized and the rest are lowercase. Display the dates and city names for both the shortest and longest consecutive date streaks, ordered by date.","WITH DateStreaks AS (
    SELECT 
        geolocation_city,
        geolocation_zip_code_prefix,
        geolocation_lat,
        geolocation_lng,
        geolocation_state,
        geolocation_city || CAST(geolocation_zip_code_prefix AS TEXT) AS streak_id,
        geolocation_lat,
        geolocation_lng,
        geolocation_city,
        geolocation_state,
        MIN(geolocation_zip_code_prefix) OVER (PARTITION BY geolocation_city, geolocation_state, geolocation_lat, geolocation_lng) AS min_zip,
        MAX(geolocation_zip_code_prefix) OVER (PARTITION BY geolocation_city, geolocation_state, geolocation_lat, geolocation_lng) AS max_zip,
        ROW_NUMBER() OVER (PARTITION BY geolocation_city ORDER BY geolocation_zip_code_prefix) - geolocation_zip_code_prefix AS grp
    FROM olist_geolocation
    WHERE geolocation_state = 'CN' AND geolocation_city like '%2021-07%'
),
StreakGroups AS (
    SELECT 
        geolocation_city,
        MIN(geolocation_zip_code_prefix) AS start_date,
        MAX(geolocation_zip_code_prefix) AS end_date,
        COUNT(*) AS streak_length
    FROM DateStreaks
    GROUP BY geolocation_city, grp
),
StreakLengths AS (
    SELECT 
        geolocation_city,
        start_date,
        end_date,
        streak_length,
        RANK() OVER (ORDER BY streak_length DESC) AS max_streak_rank,
        RANK() OVER (ORDER BY streak_length ASC) AS min_streak_rank
    FROM StreakGroups
),
ShortestStreak AS (
    SELECT 
        geolocation_city,
        start_date,
        end_date
    FROM StreakLengths
    WHERE min_streak_rank = 1
),
LongestStreak AS (
    SELECT 
        geolocation_city,
        start_date,
        end_date
    FROM StreakLengths
    WHERE max_streak_rank = 1
),
FinalResults AS (
    SELECT 
        geolocation_city,
        geolocation_zip_code_prefix AS date,
        'Shortest Streak' AS streak_type
    FROM ShortestStreak
    JOIN olist_geolocation ON ShortestStreak.start_date <= geolocation_zip_code_prefix 
                         AND ShortestStreak.end_date >= geolocation_zip_code_prefix 
                         AND ShortestStreak.geolocation_city = olist_geolocation.geolocation_city
    UNION ALL
    SELECT 
        geolocation_city,
        geolocation_zip_code_prefix AS date,
        'Longest Streak' AS streak_type
    FROM LongestStreak
    JOIN olist_geolocation ON LongestStreak.start_date <= geolocation_zip_code_prefix 
                         AND LongestStreak.end_date >= geolocation_zip_code_prefix 
                         AND LongestStreak.geolocation_city = olist_geolocation.geolocation_city
)
SELECT 
    date,
    INITCAP(geolocation_city) AS city_name,
    streak_type
FROM FinalResults
ORDER BY streak_type, date;"
local071,Could you review our records in June 2022 and identify which countries have the longest streak of consecutive inserted city dates? Please list the 2-letter length country codes of these countries.,"WITH DateDifferences AS (
    SELECT
        c.country_code_2,
        c.insert_date,
        LAG(c.insert_date) OVER (PARTITION BY c.country_code_2 ORDER BY c.insert_date) AS previous_date
    FROM
        cities c
    WHERE
        strftime('%Y-%m', c.insert_date) = '2022-06'
),
Streaks AS (
    SELECT
        country_code_2,
        insert_date,
        CASE
            WHEN julianday(insert_date) - julianday(previous_date) = 1 THEN 1
            ELSE 0
        END AS is_consecutive
    FROM
        DateDifferences
),
CumulativeStreaks AS (
    SELECT
        country_code_2,
        SUM(is_consecutive) OVER (PARTITION BY country_code_2 ORDER BY insert_date) AS streak_length
    FROM
        Streaks
),
MaxStreaks AS (
    SELECT
        country_code_2,
        MAX(streak_length) AS max_streak_length
    FROM
        CumulativeStreaks
    GROUP BY
        country_code_2
),
CountriesWithLongestStreak AS (
    SELECT
        country_code_2
    FROM
        MaxStreaks
    WHERE
        max_streak_length = (SELECT MAX(max_streak_length) FROM MaxStreaks)
)
SELECT
    country_code_2
FROM
    CountriesWithLongestStreak;"
local072,"Identify the country with data inserted on nine different days in January 2022. Then, find the longest consecutive period with data insertions for this country during January 2022, and calculate the proportion of entries that are from its capital city within this longest consecutive insertion period.","WITH CountryInsertions AS (
    SELECT country_code_2, COUNT(DISTINCT insert_date) AS days_count
    FROM cities
    WHERE strftime('%Y-%m', insert_date) = '2022-01'
    GROUP BY country_code_2
    HAVING days_count = 9
),

ConsecutiveDays AS (
    SELECT country_code_2, insert_date,
           ROW_NUMBER() OVER (PARTITION BY country_code_2 ORDER BY insert_date) -
           ROW_NUMBER() OVER (PARTITION BY country_code_2, strftime('%Y-%m-%d', insert_date, '-1 day') ORDER BY insert_date) AS grp
    FROM cities
    WHERE country_code_2 IN (SELECT country_code_2 FROM CountryInsertions)
      AND strftime('%Y-%m', insert_date) = '2022-01'
),

LongestStreak AS (
    SELECT country_code_2, MIN(insert_date) AS start_date, MAX(insert_date) AS end_date,
           COUNT(*) AS streak_length
    FROM ConsecutiveDays
    GROUP BY country_code_2, grp
    ORDER BY streak_length DESC
    LIMIT 1
),

CapitalEntries AS (
    SELECT country_code_2, COUNT(*) AS capital_entries_count
    FROM cities
    WHERE country_code_2 = (SELECT country_code_2 FROM LongestStreak)
      AND capital = 1
      AND insert_date BETWEEN (SELECT start_date FROM LongestStreak) AND (SELECT end_date FROM LongestStreak)
),

TotalEntries AS (
    SELECT country_code_2, COUNT(*) AS total_entries_count
    FROM cities
    WHERE country_code_2 = (SELECT country_code_2 FROM LongestStreak)
      AND insert_date BETWEEN (SELECT start_date FROM LongestStreak) AND (SELECT end_date FROM LongestStreak)
)

SELECT LongestStreak.country_code_2 AS country_code,
       LongestStreak.start_date,
       LongestStreak.end_date,
       CapitalEntries.capital_entries_count,
       TotalEntries.total_entries_count,
       (CAST(CapitalEntries.capital_entries_count AS REAL) / TotalEntries.total_entries_count) * 100 AS capital_entry_proportion
FROM LongestStreak
JOIN CapitalEntries ON LongestStreak.country_code_2 = CapitalEntries.country_code_2
JOIN TotalEntries ON LongestStreak.country_code_2 = TotalEntries.country_code_2;"
local068,"Calculate the number of new cities inserted in April, May, and June for each year from 2021 to 2023. For each month, compute the cumulative running total of cities added for that specific month across the years up to and including the given year (i.e., sum the counts of that month over the years). Additionally, calculate the year-over-year growth percentages for both the monthly total and the running total for each month, comparing each year to the previous year. Present the results only for 2022 and 2023, listing the year, the month, the total number of cities added in that month, the cumulative running total for that month, and the year-over-year growth percentages for both the monthly total and the running total. Use the data from 2021 solely as a baseline for calculating growth rates, and exclude it from the final output.","WITH MonthlyInserts AS (
    SELECT 
        strftime('%Y', insert_date) AS year,
        strftime('%m', insert_date) AS month,
        COUNT(*) AS cities_added
    FROM cities
    WHERE strftime('%m', insert_date) IN ('04', '05', '06') AND strftime('%Y', insert_date) BETWEEN '2021' AND '2023'
    GROUP BY year, month
),
CumulativeTotals AS (
    SELECT
        mi.year,
        mi.month,
        mi.cities_added,
        SUM(mi2.cities_added) OVER (PARTITION BY mi.month ORDER BY mi.year) AS running_total
    FROM MonthlyInserts mi
    JOIN MonthlyInserts mi2 ON mi.month = mi2.month AND mi2.year <= mi.year
),
YearlyComparisons AS (
    SELECT
        ct.year,
        ct.month,
        ct.cities_added,
        ct.running_total,
        LAG(ct.cities_added, 1) OVER (PARTITION BY ct.month ORDER BY ct.year) AS previous_year_cities,
        LAG(ct.running_total, 1) OVER (PARTITION BY ct.month ORDER BY ct.year) AS previous_year_running_total
    FROM CumulativeTotals ct
),
GrowthCalculations AS (
    SELECT
        yc.year,
        yc.month,
        yc.cities_added,
        yc.running_total,
        CASE WHEN yc.previous_year_cities = 0 THEN NULL ELSE ((yc.cities_added - yc.previous_year_cities) * 100.0 / yc.previous_year_cities) END AS monthly_growth_percentage,
        CASE WHEN yc.previous_year_running_total = 0 THEN NULL ELSE ((yc.running_total - yc.previous_year_running_total) * 100.0 / yc.previous_year_running_total) END AS running_total_growth_percentage
    FROM YearlyComparisons yc
)
SELECT 
    gc.year,
    CASE gc.month 
        WHEN '04' THEN 'April' 
        WHEN '05' THEN 'May' 
        WHEN '06' THEN 'June' 
    END AS month,
    gc.cities_added,
    gc.running_total,
    gc.monthly_growth_percentage,
    gc.running_total_growth_percentage
FROM GrowthCalculations gc
WHERE gc.year IN ('2022', '2023')
ORDER BY gc.year, gc.month;"
local073,"For each pizza order, please list a single result row containing the row ID, order ID, customer ID, pizza name, and the final set of ingredients. The final set of ingredients should be determined by taking the standard toppings from the pizza’s recipe, removing any toppings specified as exclusions, then adding any toppings specified as extras. The resulting ingredients must be presented in alphabetical order, prefixed by ‘2x’ if the same ingredient appears multiple times (for example, if both standard and extra or added multiple times), and concatenated into a string that begins with the pizza name followed by ‘: ’. Group by row ID, order ID, pizza name, and order time so that each order appears once, and return the listing of toppings in ascending order by row ID, ensuring that ‘Meatlovers’ pizzas receive a pizza_id of 1 while all others receive a pizza_id of 2.","WITH RECIPE_INGREDIENTS AS (
    SELECT 
        pco.order_id,
        pco.customer_id,
        pn.pizza_name,
        STRING_AGG(pt.topping_name, ',') AS toppings
    FROM pizza_clean_customer_orders pco
    JOIN pizza_recipes pr ON pco.pizza_id = pr.pizza_id
    JOIN pizza_toppings pt ON INSTR(pr.toppings, pt.topping_id) > 0
    JOIN pizza_names pn ON pco.pizza_id = pn.pizza_id
    GROUP BY pco.order_id, pco.customer_id, pn.pizza_name
),
EXCLUSIONS AS (
    SELECT 
        pco.order_id,
        STRING_AGG(pt.topping_name, ',') AS exclusions
    FROM pizza_clean_customer_orders pco
    JOIN pizza_toppings pt ON INSTR(pco.exclusions, pt.topping_id) > 0
    GROUP BY pco.order_id
),
EXTRAS AS (
    SELECT 
        pco.order_id,
        STRING_AGG(pt.topping_name, ',') AS extras
    FROM pizza_clean_customer_orders pco
    JOIN pizza_toppings pt ON INSTR(pco.extras, pt.topping_id) > 0
    GROUP BY pco.order_id
),
FINAL_INGREDIENTS AS (
    SELECT 
        ri.order_id,
        ri.customer_id,
        ri.pizza_name,
        TRIM(BOTH ',' FROM ARRAY_TO_STRING(ARRAY(
            SELECT DISTINCT x
            FROM UNNEST(ARRAY_REMOVE(STRING_TO_ARRAY(CONCAT(ri.toppings, ',', e.extras), ','), NULL)) AS x 
            WHERE NOT ARRAY_CONTAINS(STRING_TO_ARRAY(e.exclusions, ','), x)
        ), ',')) AS final_ingredients
    FROM RECIPE_INGREDIENTS ri
    LEFT JOIN EXCLUSIONS e ON ri.order_id = e.order_id
    LEFT JOIN EXTRAS ex ON ri.order_id = ex.order_id
)
SELECT 
    ROW_NUMBER() OVER (ORDER BY ri.order_id) AS row_id,
    ri.order_id,
    ri.customer_id,
    ri.pizza_name || ': ' || ri.final_ingredients AS final_set_of_ingredients
FROM FINAL_INGREDIENTS ri
ORDER BY row_id;"
local066,"Based on our customer pizza order information, summarize the total quantity of each ingredient used in the pizzas we delivered. Output the name and quantity for each ingredient.","SELECT pt.topping_name, SUM(COALESCE(pge.extras_count, 0) + 1 - COALESCE(pgex.total_exclusions, 0)) AS total_quantity
FROM pizza_customer_orders pco
JOIN pizza_recipes pr ON pco.pizza_id = pr.pizza_id
JOIN pizza_runner_orders pro ON pco.order_id = pro.order_id
LEFT JOIN pizza_get_extras pge ON pco.order_id = pge.order_id
LEFT JOIN pizza_get_exclusions pgex ON pco.order_id = pgex.order_id
JOIN pizza_toppings pt ON pt.topping_id = pge.extras OR pt.topping_id = pgex.exclusions
WHERE pro.cancellation IS NULL OR pro.cancellation = ''
GROUP BY pt.topping_name;"
local065,Calculate the total income from Meat Lovers pizzas priced at $12 and Vegetarian pizzas at $10. Include any extra toppings charged at $1 each. Ensure that canceled orders are filtered out. How much money has Pizza Runner earned in total?,"SELECT SUM(total_income) AS total_earned
FROM (
    SELECT 
        CASE 
            WHEN p.pizza_id = 1 THEN 12 + COALESCE(pe.extras_count, 0)
            WHEN p.pizza_id = 2 THEN 10 + COALESCE(pe.extras_count, 0)
        END AS total_income
    FROM pizza_clean_customer_orders p
    LEFT JOIN pizza_get_extras pe ON p.order_id = pe.order_id
    LEFT JOIN pizza_clean_runner_orders r ON p.order_id = r.order_id
    WHERE r.cancellation IS NULL
) AS income_data;"
local074,"Please generate a summary of the closing balances at the end of each month for each customer transactions, show the monthly changes and monthly cumulative bank account balances. Ensure that even if a customer has no account activity in a given month, the balance for that month is still included in the output.",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
local064,"For each customer and each month of 2020, first calculate the month-end balance by adding all deposit amounts and subtracting all withdrawal amounts that occurred during that specific month. Then determine which month in 2020 has the highest count of customers with a positive month-end balance and which month has the lowest count. For each of these two months, compute the average month-end balance across all customers and provide the difference between these two averages","The provided context is insufficient to generate the SQL query as requested because there are no tables containing detailed transaction data (deposits and withdrawals) for customers, nor is there a table that categorizes transactions into deposits and withdrawals. Without information on specific transactions for each customer, it's impossible to calculate monthly balances or determine the highest and lowest counts of customers with positive balances."
local297,"For each customer, group all deposits and withdrawals by the first day of each month to obtain a monthly net amount, then calculate each month’s closing balance by cumulatively summing these monthly nets. Next, determine the most recent month’s growth rate by comparing its closing balance to the prior month’s balance, treating deposits as positive and withdrawals as negative, and if the previous month’s balance is zero, the growth rate should be the current month’s balance multiplied by 100. Finally, compute the percentage of customers whose most recent month shows a growth rate of more than 5%.",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
local298,"For each month, calculate the total balance from all users for the previous month (measured as of the 1st of each month), replacing any negative balances with zero. Ensure that data from the first month is used only as a baseline for calculating previous total balance, and exclude it from the final output. Sort the results in ascending order by month. ",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
local299,"For a bank database with customer transactions, calculate each customer's daily running balance (where deposits add to the balance and other transaction types subtract). For each customer and each day, compute the 30-day rolling average balance (only after having 30 days of data, and treating negative averages as zero). Then group these daily averages by month and find each customer's maximum 30-day average balance within each month. Sum these maximum values across all customers for each month. Consider the first month of each customer's transaction history as the baseline period and exclude it from the final results, presenting monthly totals of these summed maximum 30-day average balances.","WITH DailyBalances AS (
    SELECT 
        customer_id,
        txn_date,
        SUM(CASE WHEN txn_type = 'Deposit' THEN txn_amount ELSE -txn_amount END) 
        OVER (PARTITION BY customer_id ORDER BY txn_date) AS running_balance
    FROM 
        customer_transactions
),
RollingAverages AS (
    SELECT 
        customer_id,
        txn_date,
        AVG(running_balance) OVER (PARTITION BY customer_id ORDER BY txn_date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW) AS avg_balance
    FROM 
        DailyBalances
),
PositiveAverages AS (
    SELECT 
        customer_id,
        txn_date,
        CASE WHEN avg_balance < 0 THEN 0 ELSE avg_balance END AS positive_avg_balance
    FROM 
        RollingAverages
),
MonthlyMaxAverages AS (
    SELECT
        customer_id,
        strftime('%Y-%m', txn_date) AS month,
        MAX(positive_avg_balance) AS max_avg_balance
    FROM 
        PositiveAverages
    GROUP BY
        customer_id, month
),
ExcludedFirstMonth AS (
    SELECT
        customer_id,
        month,
        max_avg_balance
    FROM 
        MonthlyMaxAverages
    WHERE 
        month > (SELECT MIN(strftime('%Y-%m', txn_date)) FROM customer_transactions WHERE customer_id = MonthlyMaxAverages.customer_id)
),
MonthlySummedMaxAverages AS (
    SELECT
        month,
        SUM(max_avg_balance) AS summed_max_avg_balance
    FROM 
        ExcludedFirstMonth
    GROUP BY
        month
)
SELECT * FROM MonthlySummedMaxAverages;"
local300,"For each customer, calculate their daily balances for every day between their earliest and latest transaction dates, including days without transactions by carrying forward the previous day's balance. Treat any negative daily balances as zero. Then, for each month, determine the highest daily balance each customer had during that month. Finally, for each month, sum these maximum daily balances across all customers to obtain a monthly total.","WITH RECURSIVE DateRange AS (
    SELECT MIN(DATE(txn_date)) AS date, MAX(DATE(txn_date)) AS max_date, customer_id
    FROM customer_transactions
    GROUP BY customer_id
    UNION ALL
    SELECT DATE(date, '+1 day'), max_date, customer_id
    FROM DateRange
    WHERE date < max_date
),
DailyBalances AS (
    SELECT dr.date, dr.customer_id, COALESCE(SUM(ct.txn_amount), 0) AS daily_txn_amount
    FROM DateRange dr
    LEFT JOIN customer_transactions ct ON dr.date = DATE(ct.txn_date) AND dr.customer_id = ct.customer_id
    GROUP BY dr.date, dr.customer_id
),
CumulativeBalances AS (
    SELECT db.date, db.customer_id,
           MAX(CASE WHEN SUM(db.daily_txn_amount) OVER (PARTITION BY db.customer_id ORDER BY db.date) < 0 THEN 0
                    ELSE SUM(db.daily_txn_amount) OVER (PARTITION BY db.customer_id ORDER BY db.date)
               END) AS balance
    FROM DailyBalances db
),
MonthlyMaxBalances AS (
    SELECT customer_id, STRFTIME('%Y-%m', date) AS month, MAX(balance) AS max_balance
    FROM CumulativeBalances
    GROUP BY customer_id, STRFTIME('%Y-%m', date)
),
MonthlyTotalMaxBalances AS (
    SELECT month, SUM(max_balance) AS total_max_balance
    FROM MonthlyMaxBalances
    GROUP BY month
)
SELECT * FROM MonthlyTotalMaxBalances;"
local075,"Can you provide a breakdown of how many times each product was viewed, how many times they were added to the shopping cart, and how many times they were left in the cart without being purchased? Also, give me the count of actual purchases for each product. Ensure that products with a page id in (1, 2, 12, 13) are filtered out.","SELECT 
    scp.product_id,
    COUNT(DISTINCT CASE WHEN sce.event_type = 1 THEN sce.visit_id END) AS views,
    COUNT(DISTINCT CASE WHEN sce.event_type = 2 THEN sce.visit_id END) AS added_to_cart,
    COUNT(DISTINCT CASE WHEN sce.event_type = 3 THEN sce.visit_id END) AS left_in_cart,
    COALESCE(SUM(s.quantity_sold), 0) AS purchases
FROM 
    shopping_cart_events sce
JOIN 
    shopping_cart_page_hierarchy scp ON sce.page_id = scp.page_id
LEFT JOIN 
    sales s ON scp.product_id = s.prod_id
WHERE 
    sce.page_id NOT IN (1, 2, 12, 13)
GROUP BY 
    scp.product_id;"
local077,"Please analyze our interest data from September 2018 to August 2019. For each month, calculate the average composition for each interest by dividing the composition by the index value. Identify the interest with the highest average composition value each month and report its average composition as the max index composition for that month. Compute the three-month rolling average of these monthly max index compositions. Ensure the output includes the date, the interest name, the max index composition for that month, the rolling average, and the names and max index compositions of the top interests from one month ago and two months ago.","WITH MonthlyInterestData AS (
    SELECT 
        i.month_year,
        im.interest_name,
        AVG(i.composition / i.index_value) AS avg_composition
    FROM 
        interest_metrics i
    JOIN 
        interest_map im ON i.interest_id = im.id
    WHERE 
        (i._year = 2018 AND i._month >= 9) OR (i._year = 2019 AND i._month <= 8)
    GROUP BY 
        i.month_year, im.interest_name
),
MaxMonthlyInterest AS (
    SELECT 
        month_year,
        MAX(avg_composition) AS max_index_composition,
        FIRST_VALUE(interest_name) OVER (PARTITION BY month_year ORDER BY avg_composition DESC) AS interest_name
    FROM 
        MonthlyInterestData
    GROUP BY 
        month_year
),
RollingAverages AS (
    SELECT 
        a.month_year,
        a.interest_name,
        a.max_index_composition,
        (a.max_index_composition + 
         COALESCE(LAG(a.max_index_composition, 1) OVER (ORDER BY a.month_year), 0) +
         COALESCE(LAG(a.max_index_composition, 2) OVER (ORDER BY a.month_year), 0)
        ) / 3 AS rolling_avg,
        LAG(a.interest_name, 1) OVER (ORDER BY a.month_year) AS prev_month_interest,
        LAG(a.max_index_composition, 1) OVER (ORDER BY a.month_year) AS prev_month_composition,
        LAG(a.interest_name, 2) OVER (ORDER BY a.month_year) AS prev_two_month_interest,
        LAG(a.max_index_composition, 2) OVER (ORDER BY a.month_year) AS prev_two_month_composition
    FROM 
        MaxMonthlyInterest a
)
SELECT 
    month_year,
    interest_name,
    max_index_composition,
    rolling_avg,
    prev_month_interest,
    prev_month_composition,
    prev_two_month_interest,
    prev_two_month_composition
FROM 
    RollingAverages
ORDER BY 
    month_year;"
local078,"Identify the top 10 and bottom 10 interest categories based on their highest composition values across all months. For each category, display the time(MM-YYYY), interest name, and the composition value","WITH RankedInterests AS (
    SELECT
        i.month_year,
        im.interest_name,
        i.composition,
        RANK() OVER (PARTITION BY i.interest_id ORDER BY i.composition DESC) AS rank_asc,
        RANK() OVER (PARTITION BY i.interest_id ORDER BY i.composition ASC) AS rank_desc
    FROM interest_metrics i
    JOIN interest_map im ON i.interest_id = im.id
)

SELECT month_year, interest_name, composition
FROM RankedInterests
WHERE rank_asc = 1 OR rank_desc = 1
ORDER BY composition DESC
LIMIT 10

UNION ALL

SELECT month_year, interest_name, composition
FROM RankedInterests
WHERE rank_asc = 1 OR rank_desc = 1
ORDER BY composition ASC
LIMIT 10;"
local081,"Considering only the customers who placed orders in 1998, calculate the total amount each customer spent by summing the unit price multiplied by the quantity of all products in their orders, excluding any discounts. Assign each customer to a spending group based on the customer group thresholds, and determine how many customers are in each spending group and what percentage of the total number of customers who placed orders in 1998 each group represents.","WITH CustomerSpending AS (
    SELECT 
        salesorderheader.customerid AS customer_id,
        SUM(order_details.unitprice * order_details.quantity) AS total_spent
    FROM 
        salesorderheader
    INNER JOIN 
        order_details ON salesorderheader.salesorderid = order_details.orderid
    WHERE 
        strftime('%Y', salesorderheader.orderdate) = '1998'
    GROUP BY 
        salesorderheader.customerid
),

SpendingGroups AS (
    SELECT
        customer_id,
        total_spent,
        CASE
            WHEN total_spent >= 10000 THEN 'High spenders'
            WHEN total_spent >= 5000 THEN 'Medium spenders'
            ELSE 'Low spenders'
        END AS spending_group
    FROM 
        CustomerSpending
)

SELECT 
    spending_group,
    COUNT(customer_id) AS number_of_customers,
    ROUND((COUNT(customer_id) * 100.0 / (SELECT COUNT(*) FROM SpendingGroups)), 2) AS percentage_of_total
FROM 
    SpendingGroups
GROUP BY 
    spending_group;"
local085,"Among employees who have more than 50 total orders, which three have the highest percentage of late orders, where an order is considered late if the shipped date is on or after its required date? Please list each employee's ID, the number of late orders, and the corresponding late-order percentage.","The context provided does not give information about employee-related data in connection with the orders they handle, such as an employee ID linked to orders. Therefore, I cannot generate a SQL query to fulfill the request without knowing which table includes both orders and an identifier for the employee responsible for each order, and how 'late' is defined in terms of the data available."
local096,"For each year, calculate the percentage of films that had exclusively female actors (meaning no male actors and no actors with unknown/unspecified gender). Consider actors with gender marked as 'Male' or 'None' as non-female. For the results, display the year, the total number of movies in that year, and the percentage of movies with exclusively female actors. Extract the year from the Movie.year field by taking the last 4 characters and converting to a number.","The provided context is insufficient to generate a SQL query because there are no tables containing information about the actors, their gender, or their association with specific movies. To create the required query, data linking actors to movies and indicating the gender of the actors would be necessary."
local097,"Could you analyze our data and identify which ten-year period starting from any movie release year present in the data had the largest number of films, considering consecutive ten-year periods beginning at each unique year? Only output the start year and the total count for that specific period.","WITH YearlyCounts AS (
    SELECT year, COUNT(*) AS num_movies
    FROM movies
    GROUP BY year
),
DecadeRollingSum AS (
    SELECT a.year AS start_year, SUM(b.num_movies) AS total_movies
    FROM YearlyCounts a
    JOIN YearlyCounts b ON b.year BETWEEN a.year AND a.year + 9
    GROUP BY a.year
)
SELECT start_year, MAX(total_movies) AS max_movies
FROM DecadeRollingSum
GROUP BY start_year
ORDER BY max_movies DESC
LIMIT 1;"
local098,"From the first year each actor appeared in a film to the last, how many actors in the database never had a gap longer than three consecutive years without at least one new movie appearance, meaning there is no four-year span anywhere in their active career without at least a single film credit?","WITH ActorYears AS (
    SELECT fa.actor_id, MIN(f.release_year) AS first_year, MAX(f.release_year) AS last_year
    FROM film_actor fa
    JOIN film f ON fa.film_id = f.film_id
    GROUP BY fa.actor_id
),
YearlyFilms AS (
    SELECT fa.actor_id, f.release_year
    FROM film_actor fa
    JOIN film f ON fa.film_id = f.film_id
    GROUP BY fa.actor_id, f.release_year
),
Gaps AS (
    SELECT a.actor_id,
           y1.release_year AS start_year,
           MIN(y2.release_year) AS next_year
    FROM YearlyFilms y1
    JOIN YearlyFilms y2 ON y1.actor_id = y2.actor_id AND y2.release_year > y1.release_year
    GROUP BY a.actor_id, y1.release_year
    HAVING MIN(y2.release_year) - y1.release_year > 3
)
SELECT COUNT(DISTINCT a.actor_id)
FROM ActorYears a
LEFT JOIN Gaps g ON a.actor_id = g.actor_id
WHERE g.actor_id IS NULL;"
local099,I need you to look into the actor collaborations and tell me how many actors have made more films with Yash Chopra than with any other director. This will help us understand his influence on the industry better.,"WITH DirectorFilms AS (
    SELECT f.actor_id, dm.name_id AS director_id, COUNT(*) AS films_count
    FROM film_actor f
    JOIN director_mapping dm ON f.film_id = dm.movie_id
    GROUP BY f.actor_id, dm.name_id
),
MaxDirectorFilms AS (
    SELECT actor_id, MAX(films_count) AS max_films
    FROM DirectorFilms
    GROUP BY actor_id
),
YashChopraFilms AS (
    SELECT df.actor_id
    FROM DirectorFilms df
    JOIN MaxDirectorFilms mdf ON df.actor_id = mdf.actor_id AND df.films_count = mdf.max_films
    WHERE df.director_id = (SELECT name_id FROM director_mapping WHERE ""name_id"" LIKE '%Yash Chopra%')
),
TotalUniqueActors AS (
    SELECT COUNT(DISTINCT actor_id) AS total_actors
    FROM YashChopraFilms
)
SELECT total_actors FROM TotalUniqueActors;"
local100,"Find out how many actors have a 'Shahrukh number' of 2? This means they acted in a film with someone who acted with Shahrukh Khan, but not directly with him.",SELECT actor_id FROM actor WHERE first_name = 'Shahrukh' AND last_name = 'Khan';
local114,"Provide a detailed web sales report for each region, including the number of orders, total sales amount, and the name and sales amount of all sales representatives who achieved the highest total sales amount in that region (include all representatives in case of a tie).","WITH RegionSales AS (
    SELECT 
        ws.region_id,
        COUNT(wo.id) AS num_orders,
        SUM(wo.total_amt_usd) AS total_sales_amount
    FROM
        web_orders wo
    JOIN
        web_sales_reps ws ON wo.account_id = ws.id
    GROUP BY 
        ws.region_id
),
RepSales AS (
    SELECT
        ws.region_id,
        ws.name,
        SUM(wo.total_amt_usd) AS total_rep_sales
    FROM
        web_orders wo
    JOIN
        web_sales_reps ws ON wo.account_id = ws.id
    GROUP BY
        ws.region_id, ws.name
),
MaxRepSales AS (
    SELECT
        region_id,
        MAX(total_rep_sales) AS max_sales
    FROM
        RepSales
    GROUP BY
        region_id
)
SELECT
    rs.region_id,
    rs.num_orders,
    rs.total_sales_amount,
    rps.name,
    rps.total_rep_sales
FROM
    RegionSales rs
JOIN
    RepSales rps ON rs.region_id = rps.region_id
JOIN
    MaxRepSales mrs ON rs.region_id = mrs.region_id AND rps.total_rep_sales = mrs.max_sales
ORDER BY
    rs.region_id;"
local128,"List the bowlers (including their ID, first name, and last name), match number, game number, handicap score, tournament date, and location for only those bowlers who have won games with a handicap score of 190 or less at all three venues: Thunderbird Lanes, Totem Lanes, and Bolero Lanes. Only include the specific game records where they won with a handicap score of 190 or less at these three locations.","SELECT b.BowlerID, b.BowlerFirstName, b.BowlerLastName, bs.MatchID, bs.GameNumber, bs.HandiCapScore, tm.TourneyDate, tm.Lanes
FROM Bowler_Scores bs
JOIN Bowlers b ON bs.BowlerID = b.BowlerID
JOIN Tourney_Matches tm ON bs.MatchID = tm.MatchID
WHERE bs.WonGame = 1 AND bs.HandiCapScore <= 190
AND tm.Lanes IN ('Thunderbird Lanes', 'Totem Lanes', 'Bolero Lanes')
GROUP BY b.BowlerID, b.BowlerFirstName, b.BowlerLastName, bs.MatchID, bs.GameNumber, bs.HandiCapScore, tm.TourneyDate, tm.Lanes
HAVING COUNT(DISTINCT tm.Lanes) = 3;"
local130,"Could you provide a list of last names for all students who have completed English courses (where completion is defined as having a ClassStatus of 2), along with their quintile ranks based on their individual grades in those courses? The quintile should be determined by calculating how many students have grades greater than or equal to each student's grade, then dividing this ranking by the total number of students who completed English courses. The quintiles should be labeled as ""First"" (top 20%), ""Second"" (top 21-40%), ""Third"" (top 41-60%), ""Fourth"" (top 61-80%), and ""Fifth"" (bottom 20%). Please sort the results from highest performing quintile to lowest (First to Fifth).","WITH EnglishStudentGrades AS (
    SELECT 
        s.StudLastName,
        ss.Grade
    FROM 
        Student_Schedules ss
    JOIN 
        Classes c ON ss.ClassID = c.ClassID
    JOIN 
        Subjects sub ON c.SubjectID = sub.SubjectID
    JOIN 
        Students s ON ss.StudentID = s.StudentID
    WHERE 
        ss.ClassStatus = 2 AND 
        sub.SubjectName = 'English'
),
RankedGrades AS (
    SELECT 
        StudLastName,
        Grade,
        NTILE(5) OVER (ORDER BY Grade DESC) AS Quintile
    FROM 
        EnglishStudentGrades
),
QuintileLabels AS (
    SELECT 
        StudLastName,
        Grade,
        CASE 
            WHEN Quintile = 1 THEN 'First'
            WHEN Quintile = 2 THEN 'Second'
            WHEN Quintile = 3 THEN 'Third'
            WHEN Quintile = 4 THEN 'Fourth'
            WHEN Quintile = 5 THEN 'Fifth'
        END AS QuintileLabel
    FROM 
        RankedGrades
)
SELECT 
    StudLastName,
    QuintileLabel
FROM 
    QuintileLabels
ORDER BY 
    QuintileLabel;"
local131,"Could you list each musical style with the number of times it appears as a 1st, 2nd, or 3rd preference in a single row per style?","SELECT 
    ms.StyleName,
    SUM(CASE WHEN mp.PreferenceSeq = 1 THEN 1 ELSE 0 END) AS FirstPreference,
    SUM(CASE WHEN mp.PreferenceSeq = 2 THEN 1 ELSE 0 END) AS SecondPreference,
    SUM(CASE WHEN mp.PreferenceSeq = 3 THEN 1 ELSE 0 END) AS ThirdPreference
FROM 
    Musical_Styles ms
LEFT JOIN 
    Musical_Preferences mp ON ms.StyleID = mp.StyleID
GROUP BY 
    ms.StyleName;"
local133,"Given a database of musical styles and user preferences, where Musical_Preferences contains user rankings of musical styles (PreferenceSeq=1 for first choice, PreferenceSeq=2 for second choice, PreferenceSeq=3 for third choice): Calculate a weighted score for each musical style by assigning 3 points for each time it was ranked as first choice, 2 points for each second choice, and 1 point for each third choice ranking. Calculate the total weighted score for each musical style that has been ranked by at least one user. Then, compute the absolute difference between each style's total weighted score and the average total weighted score across all such styles.	","WITH StyleScores AS (
    SELECT
        StyleID,
        SUM(CASE WHEN PreferenceSeq = 1 THEN 3 ELSE 0 END
             + CASE WHEN PreferenceSeq = 2 THEN 2 ELSE 0 END
             + CASE WHEN PreferenceSeq = 3 THEN 1 ELSE 0 END) AS TotalWeightedScore
    FROM
        Musical_Preferences
    GROUP BY
        StyleID
),
AverageScore AS (
    SELECT AVG(TotalWeightedScore) AS AvgScore
    FROM StyleScores
)
SELECT
    ss.StyleID,
    ss.TotalWeightedScore,
    ABS(ss.TotalWeightedScore - ascore.AvgScore) AS ScoreDifference
FROM
    StyleScores ss,
    AverageScore ascore;"
local132,"Show all pairs of entertainers and customers who each have up to three style strengths or preferences, where the first and second style preferences of the customers match the first and second style strengths of the entertainers (or in reverse order). Only return the entertainer’s stage name and the customer’s last name","SELECT e.EntStageName, c.LastName
FROM Entertainers AS e
JOIN Entertainer_Styles AS es1 ON e.EntertainerID = es1.EntertainerID
JOIN Entertainer_Styles AS es2 ON e.EntertainerID = es2.EntertainerID AND es1.StyleID != es2.StyleID
JOIN Customers AS c
JOIN Musical_Preferences AS mp1 ON c.CustomerID = mp1.CustomerID
JOIN Musical_Preferences AS mp2 ON c.CustomerID = mp2.CustomerID AND mp1.StyleID != mp2.StyleID
WHERE ((es1.StyleID = mp1.StyleID AND es2.StyleID = mp2.StyleID) OR (es1.StyleID = mp2.StyleID AND es2.StyleID = mp1.StyleID))
  AND es1.StyleStrength = 1 AND es2.StyleStrength = 2
  AND mp1.PreferenceSeq = 1 AND mp2.PreferenceSeq = 2
  AND (SELECT COUNT(*) FROM Entertainer_Styles WHERE EntertainerID = e.EntertainerID) <= 3
  AND (SELECT COUNT(*) FROM Musical_Preferences WHERE CustomerID = c.CustomerID) <= 3
GROUP BY e.EntStageName, c.LastName"
local141,"How did each salesperson's annual total sales compare to their annual sales quota? Provide the difference between their total sales and the quota for each year, organized by salesperson and year.","SELECT 
    sp.businessentityid,
    strftime('%Y', soh.orderdate) AS year,
    SUM(soh.totaldue) AS total_sales,
    sq.salesquota AS annual_quota,
    (SUM(soh.totaldue) - sq.salesquota) AS difference
FROM 
    salesorderheader soh
JOIN 
    salesperson sp ON soh.salespersonid = sp.businessentityid
JOIN 
    SalesPersonQuotaHistory sq ON sp.businessentityid = sq.BusinessEntityID AND strftime('%Y', sq.QuotaDate) = strftime('%Y', soh.orderdate)
GROUP BY 
    sp.businessentityid, 
    strftime('%Y', soh.orderdate)"
local152,"Can you provide the top 9 directors by movie count, including their ID, name, number of movies, average inter-movie duration (rounded to the nearest integer), average rating (rounded to 2 decimals), total votes, minimum and maximum ratings, and total movie duration? Sort the output first by movie count in descending order and then by total movie duration in descending order.","SELECT 
    dm.name_id AS director_id,
    COUNT(DISTINCT dm.movie_id) AS movie_count,
    ROUND(AVG(julianday(m.date_published) - LAG(julianday(m.date_published)) OVER (PARTITION BY dm.name_id ORDER BY m.date_published))) AS avg_inter_movie_duration,
    ROUND(AVG(r.avg_rating), 2) AS average_rating,
    SUM(r.total_votes) AS total_votes,
    MIN(r.avg_rating) AS min_rating,
    MAX(r.avg_rating) AS max_rating,
    SUM(m.duration) AS total_movie_duration
FROM 
    director_mapping dm
JOIN 
    movies m ON dm.movie_id = m.id
JOIN 
    ratings r ON m.id = r.movie_id
GROUP BY 
    dm.name_id
ORDER BY 
    movie_count DESC,
    total_movie_duration DESC
LIMIT 9;"
local230,"Determine the top three genres with the most movies rated above 8, and then identify the top four directors who have directed the most films rated above 8 within those genres. List these directors and their respective movie counts.","WITH HighRatedGenres AS (
    SELECT g.genre, COUNT(*) AS MovieCount
    FROM genre g
    JOIN ratings r ON g.movie_id = r.movie_id
    WHERE r.avg_rating > 8
    GROUP BY g.genre
    ORDER BY MovieCount DESC
    LIMIT 3
), TopDirectors AS (
    SELECT dm.name_id, COUNT(*) AS DirectedMoviesCount
    FROM director_mapping dm
    JOIN genre g ON dm.movie_id = g.movie_id
    JOIN ratings r ON dm.movie_id = r.movie_id
    WHERE r.avg_rating > 8 AND g.genre IN (SELECT genre FROM HighRatedGenres)
    GROUP BY dm.name_id
    ORDER BY DirectedMoviesCount DESC
    LIMIT 4
)
SELECT name_id, DirectedMoviesCount
FROM TopDirectors;"
local156,"Analyze the annual average purchase price per Bitcoin by region, computed as the total dollar amount spent divided by the total quantity purchased each year, excluding the first year's data for each region. Then, for each year, rank the regions based on these average purchase prices, and calculate the annual percentage change in cost for each region compared to the previous year.","The provided context does not contain information on the region associated with Bitcoin transactions or purchases. This information is crucial to group data by region and compute the required metrics. Therefore, the SQL query cannot be generated without knowing the relationship between transactions and regions."
local157,"Using the ""bitcoin_prices"" table, please calculate the daily percentage change in trading volume for each ticker from August 1 to August 10, 2021, ensuring that any volume ending in ""K"" or ""M"" is accurately converted to thousands or millions, any ""-"" volume is treated as zero, only non-zero volumes are used to determine the previous day's volume, and the results are ordered by ticker and date.","WITH VolumeConversion AS (
    SELECT
        ticker,
        market_date,
        CASE 
            WHEN volume LIKE '%K' THEN CAST(SUBSTR(volume, 1, LENGTH(volume) - 1) AS REAL) * 1000
            WHEN volume LIKE '%M' THEN CAST(SUBSTR(volume, 1, LENGTH(volume) - 1) AS REAL) * 1000000
            WHEN volume = '-' THEN 0
            ELSE CAST(volume AS REAL)
        END AS volume_numeric
    FROM bitcoin_prices
    WHERE market_date BETWEEN '2021-08-01' AND '2021-08-10'
),
PreviousDayVolume AS (
    SELECT
        a.ticker,
        a.market_date,
        a.volume_numeric,
        LAG(a.volume_numeric) OVER (PARTITION BY a.ticker ORDER BY a.market_date) AS prev_day_volume
    FROM VolumeConversion a
    WHERE a.volume_numeric > 0
),
VolumeChange AS (
    SELECT
        ticker,
        market_date,
        volume_numeric,
        prev_day_volume,
        CASE
            WHEN prev_day_volume IS NULL OR prev_day_volume = 0 THEN NULL
            ELSE ((volume_numeric - prev_day_volume) / prev_day_volume) * 100
        END AS percentage_change
    FROM PreviousDayVolume
)
SELECT 
    ticker,
    market_date,
    percentage_change
FROM VolumeChange
ORDER BY ticker, market_date;"
local163,"Which university faculty members' salaries are closest to the average salary for their respective ranks? Please provide the ranks, first names, last names, and salaries.university","WITH RankAverage AS (
    SELECT FacRank, AVG(FacSalary) AS AvgSalary
    FROM university_faculty
    GROUP BY FacRank
)
SELECT uf.FacRank, uf.FacFirstName, uf.FacLastName, uf.FacSalary
FROM university_faculty uf
JOIN RankAverage ra ON uf.FacRank = ra.FacRank
ORDER BY ABS(uf.FacSalary - ra.AvgSalary)"
local168,"Among job postings that specifically have the Data Analyst, require a non-null annual average salary, and are remote, what is the overall average salary when considering only the top three most frequently demanded skills for these positions?","WITH RelevantJobs AS (
  SELECT j.job_id, j.salary_year_avg
  FROM job_postings_fact j
  WHERE j.job_title LIKE '%Data Analyst%'
    AND j.salary_year_avg IS NOT NULL
    AND j.job_work_from_home = 1
),
TopSkills AS (
  SELECT s.skill_id, COUNT(*) AS frequency
  FROM skills_job_dim sj
  JOIN RelevantJobs rj ON sj.job_id = rj.job_id
  GROUP BY s.skill_id
  ORDER BY frequency DESC
  LIMIT 3
),
AverageSalary AS (
  SELECT AVG(rj.salary_year_avg) AS AvgSalary
  FROM RelevantJobs rj
  JOIN skills_job_dim sj ON rj.job_id = sj.job_id
  WHERE sj.skill_id IN (SELECT skill_id FROM TopSkills)
)
SELECT AvgSalary
FROM AverageSalary;"
local169,"What is the annual retention rate of legislators who began their first term between January 1, 1917 and December 31, 1999, measured as the proportion of this cohort still in office on December 31st for each of the first 20 years following their initial term start? The results should show all 20 periods in sequence regardless of whether any legislators were retained in a particular year.","WITH FirstTerm AS (
    SELECT id_bioguide, MIN(term_start) AS first_term_start
    FROM legislators_terms
    WHERE term_start BETWEEN '1917-01-01' AND '1999-12-31'
    GROUP BY id_bioguide
),
YearlyRetention AS (
    SELECT 
        f.id_bioguide,
        f.first_term_start,
        (DATE_PART('year', t.term_end) - DATE_PART('year', f.first_term_start)) AS years_after_first_term,
        COUNT(*) FILTER (WHERE t.term_end >= '2000-01-01') AS still_in_office
    FROM FirstTerm f
    JOIN legislators_terms t ON f.id_bioguide = t.id_bioguide
    WHERE (DATE_PART('year', t.term_end) - DATE_PART('year', f.first_term_start)) BETWEEN 0 AND 19
    GROUP BY f.id_bioguide, years_after_first_term, f.first_term_start
),
AggregatedRetention AS (
    SELECT 
        years_after_first_term,
        COUNT(*) AS total_first_term_legislators,
        SUM(still_in_office) AS total_still_in_office
    FROM YearlyRetention
    GROUP BY years_after_first_term
),
FinalOutput AS (
    SELECT 
        years_after_first_term,
        total_first_term_legislators,
        total_still_in_office,
        CASE WHEN total_first_term_legislators = 0 THEN 0
             ELSE ROUND((total_still_in_office::FLOAT / total_first_term_legislators) * 100, 2)
        END AS retention_rate
    FROM AggregatedRetention
    ORDER BY years_after_first_term
)
SELECT * FROM FinalOutput;"
local171,"For male legislators from Louisiana, how many distinct legislators were actively serving on December 31 of each year from more than 30 years since their first term up to less than 50 years, grouping the results by the exact number of years elapsed since their first term?","WITH FirstTerm AS (
    SELECT 
        id_bioguide, 
        MIN(term_start) AS first_term_start
    FROM legislators_terms
    WHERE state = 'LA' AND gender = 'M'
    GROUP BY id_bioguide
),
Eligibility AS (
    SELECT 
        l.id_bioguide,
        ft.first_term_start,
        lt.term_end,
        DATE_PART('year', ft.first_term_start::date) AS first_year,
        DATE_PART('year', lt.term_end::date) AS last_year
    FROM legislators l
    JOIN FirstTerm ft ON l.id_bioguide = ft.id_bioguide
    JOIN legislators_terms lt ON l.id_bioguide = lt.id_bioguide
    WHERE l.gender = 'M'
),
YearlyService AS (
    SELECT DISTINCT 
        e.id_bioguide,
        y.year,
        y.year - e.first_year AS years_since_first_term
    FROM Eligibility e
    CROSS JOIN generate_series(e.first_year, e.last_year) AS y(year)
    WHERE y.year - e.first_year BETWEEN 31 AND 49
)
SELECT 
    years_since_first_term,
    COUNT(DISTINCT id_bioguide) AS number_of_legislators
FROM YearlyService
GROUP BY years_since_first_term
ORDER BY years_since_first_term;"
local167,"Based on the state each female legislator first represented, which state has the highest number of female legislators whose terms included December 31st at any point, and what is that count? Please provide the state's abbreviation.","SELECT us.stateabbr AS state_abbreviation, COUNT(DISTINCT l.id_bioguide) AS female_legislators_count
FROM legislators l
JOIN legislators_terms lt ON l.id_bioguide = lt.id_bioguide
JOIN usstates us ON us.statename = lt.state
WHERE l.gender = 'F' AND lt.term_start <= 'YYYY-MM-DD' AND lt.term_end >= 'YYYY-MM-DD'
GROUP BY us.stateabbr
ORDER BY female_legislators_count DESC
LIMIT 1;"
local170,"Identify the state abbreviations where, for both male and female legislators, the retention rate remains greater than zero at specific intervals of 0, 2, 4, 6, 8, and 10 years after their first term start date. A legislator is considered retained if they are serving on December 31 of the respective year. Only include states where both gender cohorts maintain non-zero retention rates at all six of these time points during the first decade of service.","WITH RetentionRates AS (
    SELECT 
        lt.state,
        l.gender,
        lt.term_start,
        COUNT(DISTINCT CASE WHEN DATE(lt.term_end) >= '2000-12-31' THEN l.id_bioguide END) AS retained_0_years,
        COUNT(DISTINCT CASE WHEN DATE(lt.term_end) >= '2002-12-31' THEN l.id_bioguide END) AS retained_2_years,
        COUNT(DISTINCT CASE WHEN DATE(lt.term_end) >= '2004-12-31' THEN l.id_bioguide END) AS retained_4_years,
        COUNT(DISTINCT CASE WHEN DATE(lt.term_end) >= '2006-12-31' THEN l.id_bioguide END) AS retained_6_years,
        COUNT(DISTINCT CASE WHEN DATE(lt.term_end) >= '2008-12-31' THEN l.id_bioguide END) AS retained_8_years,
        COUNT(DISTINCT CASE WHEN DATE(lt.term_end) >= '2010-12-31' THEN l.id_bioguide END) AS retained_10_years
    FROM 
        legislators_terms lt
    JOIN 
        legislators l ON lt.id_bioguide = l.id_bioguide
    WHERE 
        DATE(lt.term_start) BETWEEN '1990-01-01' AND '1999-12-31'
    GROUP BY 
        lt.state, l.gender, lt.term_start
)

SELECT 
    DISTINCT stateabbr
FROM 
    RetentionRates rr
JOIN 
    usstates us ON rr.state = us.statename
WHERE 
    retained_0_years > 0 AND 
    retained_2_years > 0 AND 
    retained_4_years > 0 AND 
    retained_6_years > 0 AND 
    retained_8_years > 0 AND 
    retained_10_years > 0
GROUP BY 
    rr.state
HAVING 
    COUNT(DISTINCT gender) = 2"
local193,"Could you find out the average percentage of the total lifetime sales (LTV) that occur in the first 7 and 30 days after a customer's initial purchase? Also, include the average total lifetime sales (LTV). Please exclude customers with zero lifetime sales. The 7- and 30-day periods should be based on the exact number of hours-minutes-seconds, not calendar days.","The provided context is insufficient to generate the requested SQL query. There is no information on customer lifetime sales tracking, initial purchase dates for customers, or specific transaction details that include timestamps necessary for computing sales within exact time frames (like hours-minutes-seconds). Additionally, there's no table explicitly linking customers with their purchase histories over time, which is essential for calculating lifetime value (LTV) and sales within specific periods after the initial purchase. To proceed, we would need a table that records each sale with timestamps and links sales to individual customers."
local194,"Please provide a list of the top three revenue-generating films for each actor, along with the average revenue per actor in those films, calculated by dividing the total film revenue equally among the actors for each film.","WITH RevenuePerActor AS (
    SELECT 
        fa.actor_id,
        m.id AS movie_id,
        m.title,
        CAST(REPLACE(m.worlwide_gross_income, '$', '') AS NUMERIC) / COUNT(fa.actor_id) OVER (PARTITION BY m.id) AS revenue_per_actor
    FROM 
        film_actor fa
    JOIN 
        movies m ON fa.film_id = m.id
    WHERE 
        m.worlwide_gross_income IS NOT NULL AND m.worlwide_gross_income != ''
),
RankedFilms AS (
    SELECT 
        a.actor_id,
        a.first_name,
        a.last_name,
        rpa.movie_id,
        rpa.title,
        rpa.revenue_per_actor,
        ROW_NUMBER() OVER (PARTITION BY a.actor_id ORDER BY rpa.revenue_per_actor DESC) AS revenue_rank
    FROM 
        RevenuePerActor rpa
    JOIN 
        actor a ON rpa.actor_id = a.actor_id
)
SELECT 
    rf.actor_id,
    rf.first_name,
    rf.last_name,
    rf.movie_id,
    rf.title,
    rf.revenue_per_actor,
    AVG(rf.revenue_per_actor) OVER (PARTITION BY rf.actor_id) AS average_revenue_per_top_films
FROM 
    RankedFilms rf
WHERE 
    rf.revenue_rank <= 3;"
local195,Please find out how widespread the appeal of our top five actors is. What percentage of our customers have rented films featuring these actors?,"The provided context does not include sufficient information to directly generate a SQL query to find the percentage of customers who have rented films featuring the top five actors. Specifically, there is no direct link or table relating customers to the films they have rented or indicating which actors are considered the ""top five."" 

To proceed, additional data or tables linking customers to film rentals and a definition or criteria for determining the top five actors (e.g., based on number of films, ratings, or rentals) are required."
local196,For each rating category of the first movie rented by customers—where the first movie is identified based on the earliest payment date per customer—please provide the average total amount spent per customer and the average number of subsequent rentals (calculated as the total number of rentals minus one) for customers whose first rented movie falls into that rating category.,"WITH FirstRentals AS (
    SELECT 
        c.customer_id,
        MIN(p.payment_date) AS first_payment_date,
        r.rental_id
    FROM 
        payment p
    JOIN rental r ON p.rental_id = r.rental_id
    JOIN customer c ON p.customer_id = c.customer_id
    GROUP BY c.customer_id
),
FirstMovies AS (
    SELECT 
        fr.customer_id,
        f.rating
    FROM 
        FirstRentals fr
    JOIN rental r ON fr.rental_id = r.rental_id
    JOIN inventory i ON r.inventory_id = i.inventory_id
    JOIN film f ON i.film_id = f.film_id
),
RentalCounts AS (
    SELECT 
        p.customer_id,
        COUNT(p.rental_id) - 1 AS subsequent_rentals
    FROM 
        payment p
    GROUP BY p.customer_id
),
TotalSpent AS (
    SELECT 
        p.customer_id,
        SUM(p.amount) AS total_amount
    FROM 
        payment p
    GROUP BY p.customer_id
)

SELECT 
    fm.rating,
    AVG(ts.total_amount) AS avg_total_amount,
    AVG(rc.subsequent_rentals) AS avg_subsequent_rentals
FROM 
    FirstMovies fm
JOIN TotalSpent ts ON fm.customer_id = ts.customer_id
JOIN RentalCounts rc ON fm.customer_id = rc.customer_id
GROUP BY 
    fm.rating;"
local197,"Among our top 10 paying customers, can you identify the largest change in payment amounts from one month to the immediately following month? Specifically, please determine for which customer and during which month this maximum month-over-month difference occurred, and provide the difference rounded to two decimal places.","WITH MonthlyPayments AS (
    SELECT 
        customer_id,
        EXTRACT(YEAR FROM payment_date) AS year,
        EXTRACT(MONTH FROM payment_date) AS month,
        SUM(amount) AS total_payment
    FROM payment
    GROUP BY customer_id, year, month
),
RankedCustomers AS (
    SELECT 
        customer_id,
        SUM(total_payment) AS total_payments
    FROM MonthlyPayments
    GROUP BY customer_id
    ORDER BY total_payments DESC
    LIMIT 10
),
TopCustomerPayments AS (
    SELECT
        mp.customer_id,
        mp.year,
        mp.month,
        mp.total_payment
    FROM MonthlyPayments mp
    INNER JOIN RankedCustomers rc ON mp.customer_id = rc.customer_id
),
MonthOverMonthDifferences AS (
    SELECT 
        tcp.customer_id,
        tcp.year,
        tcp.month,
        tcp.total_payment - LAG(tcp.total_payment) OVER (PARTITION BY tcp.customer_id ORDER BY tcp.year, tcp.month) AS difference
    FROM TopCustomerPayments tcp
)
SELECT 
    customer_id,
    year,
    month,
    ROUND(MAX(difference), 2) AS max_difference
FROM MonthOverMonthDifferences
WHERE difference IS NOT NULL
GROUP BY customer_id, year, month
ORDER BY max_difference DESC
LIMIT 1;"
local199,"Can you identify the year and month with the highest rental orders created by the store's staff for each store? Please list the store ID, the year, the month, and the total rentals for those dates.",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
local201,"Identify the first 10 words, sorted alphabetically, that are 4 to 5 characters long, start with 'r', and have at least one anagram of the same length, considering case-sensitive letters. Provide the count of such anagrams for each word.","WITH WordLengths AS (
    SELECT words, LENGTH(words) AS len
    FROM word_list
    WHERE words LIKE 'r____' OR words LIKE 'r___'
),
Anagrams AS (
    SELECT a.words AS word, COUNT(b.words) AS anagram_count
    FROM WordLengths a
    JOIN WordLengths b ON a.words <> b.words AND LENGTH(a.words) = LENGTH(b.words)
    WHERE LOWER(SUBSTR(a.words, 1, 1)) = LOWER(SUBSTR(b.words, 1, 1))
    GROUP BY a.words
    HAVING COUNT(b.words) > 0
)
SELECT word, anagram_count
FROM Anagrams
ORDER BY word
LIMIT 10;"
local202,"For alien data, how many of the top 10 states by alien population have a higher percentage of friendly aliens than hostile aliens, with an average alien age exceeding 200?","WITH StateAggression AS (
    SELECT
        a.state,
        COUNT(*) AS total_aliens,
        AVG(a.age) AS avg_age,
        SUM(CASE WHEN ad.aggressive = 1 THEN 1 ELSE 0 END) AS hostile_aliens,
        SUM(CASE WHEN ad.aggressive = 0 THEN 1 ELSE 0 END) AS friendly_aliens
    FROM alien_data AS a
    JOIN aliens_details AS ad ON a.favorite_food = ad.favorite_food AND a.feeding_frequency = ad.feeding_frequency
    GROUP BY a.state
    HAVING avg_age > 200
),
TopStates AS (
    SELECT
        state,
        total_aliens
    FROM StateAggression
    ORDER BY total_aliens DESC
    LIMIT 10
)
SELECT
    COUNT(*) AS friendly_states_count
FROM TopStates
WHERE friendly_aliens > hostile_aliens;"
local209,"In the dataset of orders joined with store information, which store has the highest total number of orders, and among that store’s orders, what is the ratio of orders that appear in the deliveries table with a 'DELIVERED' status to the total orders for that store?","with 'stores' to get store information.
-- Further assuming that the 'delivery_status' in 'deliveries' table contains the string 'DELIVERED' for delivered orders.

WITH StoreOrderCounts AS (
    SELECT
        o.store_id,
        COUNT(*) AS total_orders
    FROM
        orders o
    GROUP BY
        o.store_id
),

DeliveredOrderCounts AS (
    SELECT
        o.store_id,
        COUNT(*) AS delivered_orders
    FROM
        orders o
    JOIN
        deliveries d ON o.delivery_order_id = d.delivery_order_id
    WHERE
        d.delivery_status = 'DELIVERED'
    GROUP BY
        o.store_id
),

MaxOrdersStore AS (
    SELECT
        store_id,
        total_orders
    FROM
        StoreOrderCounts
    ORDER BY
        total_orders DESC
    LIMIT 1
)

SELECT
    m.store_id,
    COALESCE(d.delivered_orders, 0) / m.total_orders AS delivery_ratio
FROM
    MaxOrdersStore m
LEFT JOIN
    DeliveredOrderCounts d ON m.store_id = d.store_id;"
local210,Can you identify the hubs that saw more than a 20% increase in finished orders from February to March?,"WITH MonthlyOrders AS (
    SELECT 
        s.hub_id,
        COUNT(o.order_id) AS total_orders,
        o.order_created_month
    FROM 
        orders o
    JOIN 
        stores s ON o.store_id = s.store_id
    WHERE 
        o.order_status = 'finished' AND
        o.order_created_year = 2023 AND
        (o.order_created_month = 2 OR o.order_created_month = 3)
    GROUP BY 
        s.hub_id, o.order_created_month
)

SELECT 
    Feb.hub_id,
    Feb.total_orders AS February_Orders,
    Mar.total_orders AS March_Orders
FROM 
    MonthlyOrders Feb
JOIN 
    MonthlyOrders Mar ON Feb.hub_id = Mar.hub_id
WHERE 
    Mar.order_created_month = 3 AND
    Feb.order_created_month = 2 AND
    Mar.total_orders > 1.2 * Feb.total_orders;"
local212,Can you find 5 delivery drivers with the highest average number of daily deliveries?,"SELECT driver_id, AVG(daily_deliveries) AS avg_daily_deliveries
FROM (
    SELECT driver_id, COUNT(*) AS daily_deliveries, DATE(order_moment_delivered) AS delivery_date
    FROM deliveries
    JOIN orders ON deliveries.delivery_order_id = orders.order_id
    WHERE driver_id IS NOT NULL AND delivery_status = 'Delivered'
    GROUP BY driver_id, delivery_date
) AS daily_counts
GROUP BY driver_id
ORDER BY avg_daily_deliveries DESC
LIMIT 5;"
local218,Can you calculate the median from the highest season goals of each team?,"The provided context is insufficient to generate a SQL query for calculating the median of the highest season goals for each team. The reason is that there are no tables or columns related to ""goals"" in the provided schema, which primarily focuses on baseball statistics. The term ""goals"" is typically associated with sports like soccer or hockey, not baseball."
local219,"In each league, considering all seasons, which single team has the fewest total match wins based on comparing home and away goals, including teams with zero wins, ensuring that if multiple teams tie for the fewest wins, only one team is returned for each league?","WITH LeagueWins AS (
    SELECT 
        m.league_id,
        CASE 
            WHEN m.home_team_goal > m.away_team_goal THEN m.home_team_api_id
            WHEN m.away_team_goal > m.home_team_goal THEN m.away_team_api_id
        END AS team_id,
        COUNT(*) AS wins
    FROM 
        Match m
    GROUP BY 
        m.league_id, 
        team_id
)

SELECT 
    lw1.league_id,
    COALESCE(lw1.team_id, 'No team has won any match in this league.') AS team_id,
    MIN(lw1.wins) AS fewest_wins
FROM 
    LeagueWins lw1
LEFT JOIN 
    LeagueWins lw2 ON lw1.league_id = lw2.league_id AND lw1.wins > lw2.wins
WHERE 
    lw2.wins IS NULL
GROUP BY 
    lw1.league_id
ORDER BY 
    lw1.league_id;"
local221,Tell me top10 teams with the most wins across the league,"SELECT 
    team_id,
    SUM(w) AS total_wins
FROM 
    team
GROUP BY 
    team_id
ORDER BY 
    total_wins DESC
LIMIT 10;"
local220,"Which player has participated in the highest number of winning matches and which player has participated in the highest number of losing matches, considering only matches where they actually played (excluding null entries) and where their team won or lost (excluding draws)?","SELECT DISTINCT win_type, outcome_type FROM match;"
local228,"For each IPL season, identify the top three batsmen with the highest total runs scored and the top three bowlers with the most wickets taken, excluding ‘run out’, ‘hit wicket’, and ‘retired hurt’ dismissals. In the event of ties in runs or wickets, break the tie using the smaller player ID. Then output these six players in matched positions—batsman 1 with bowler 1, batsman 2 with bowler 2, and batsman 3 with bowler 3—in ascending order of the season ID, along with each player’s total runs or wickets.",SELECT DISTINCT year FROM batting;
local229,"Find the IDs of players who scored the highest number of partnership runs for each match. The output should include the IDs of two players, each with their individual scores and the total partnership score. For each pair, the player with the higher individual score should be listed as player 1, and the player with the lower score as player 2. In cases where both players have the same score, the player with the higher ID should be player 1, and the player with the lower ID should be player 2. There can be multiple rows for a single match.","WITH PlayerScores AS (
    SELECT 
        bb.match_id,
        bb.striker AS player_id,
        SUM(bs.runs_scored) AS individual_score
    FROM 
        ball_by_ball bb
    JOIN 
        batsman_scored bs ON bb.match_id = bs.match_id 
                          AND bb.over_id = bs.over_id 
                          AND bb.ball_id = bs.ball_id
                          AND bb.innings_no = bs.innings_no
    GROUP BY 
        bb.match_id, bb.striker
),
PartnershipScores AS (
    SELECT 
        ps1.match_id,
        ps1.player_id AS player1_id,
        ps2.player_id AS player2_id,
        ps1.individual_score AS player1_score,
        ps2.individual_score AS player2_score,
        (ps1.individual_score + ps2.individual_score) AS total_partnership_score
    FROM 
        PlayerScores ps1
    JOIN 
        PlayerScores ps2 ON ps1.match_id = ps2.match_id AND ps1.player_id < ps2.player_id
),
RankedPartnerships AS (
    SELECT 
        *,
        ROW_NUMBER() OVER (PARTITION BY match_id ORDER BY total_partnership_score DESC) as rank
    FROM 
        PartnershipScores
)
SELECT 
    CASE 
        WHEN player1_score >= player2_score THEN player1_id
        ELSE player2_id
    END AS player_1,
    CASE 
        WHEN player1_score >= player2_score THEN player2_id
        ELSE player1_id
    END AS player_2,
    CASE 
        WHEN player1_score >= player2_score THEN player1_score
        ELSE player2_score
    END AS player_1_score,
    CASE 
        WHEN player1_score >= player2_score THEN player2_score
        ELSE player1_score
    END AS player_2_score,
    total_partnership_score
FROM 
    RankedPartnerships
WHERE 
    rank = 1;"
local244,"Calculate the duration of each track, classify them as short, medium, or long, output the minimum and maximum time for each kind (in minutes) and the total revenue for each category, group by the category.","WITH TrackDuration AS (
    SELECT
        race_id,
        SUM(milliseconds) / 1000 / 60 AS total_minutes
    FROM lap_times
    GROUP BY race_id
),
ClassifiedTracks AS (
    SELECT
        race_id,
        total_minutes,
        CASE
            WHEN total_minutes <= 120 THEN 'Short'
            WHEN total_minutes BETWEEN 120 AND 180 THEN 'Medium'
            ELSE 'Long'
        END AS category
    FROM TrackDuration
)
SELECT
    category,
    MIN(total_minutes) AS min_time,
    MAX(total_minutes) AS max_time,
    SUM(order_amount) AS total_revenue
FROM ClassifiedTracks
JOIN races_ext ON ClassifiedTracks.race_id = races_ext.race_id
JOIN orders ON races_ext.circuit_id = orders.store_id
GROUP BY category;"
local253,"Using a Salary Dataset where the salary values need to be cleaned by removing non-numeric characters and converting them to a numeric type, write a detailed SQL query that identifies the top 5 companies by average salary in each of Mumbai, Pune, New Delhi, and Hyderabad, then compares each company’s average salary in those cities to the overall national average salary. The final result should display four columns: Location, Company Name, Average Salary in State, and Average Salary in Country, listing only the top 5 companies in each of the specified locations.","WITH CleanedSalaries AS (
    SELECT 
        CompanyName,
        Location,
        CAST(REPLACE(REPLACE(Salary, '$', ''), ',', '') AS REAL) AS NumericSalary
    FROM 
        SalaryDataset
    WHERE 
        Location IN ('Mumbai', 'Pune', 'New Delhi', 'Hyderabad')
),

AvgSalaries AS (
    SELECT 
        CompanyName,
        Location,
        AVG(NumericSalary) AS AvgSalary
    FROM 
        CleanedSalaries
    GROUP BY 
        CompanyName, Location
),

NationalAvgSalaries AS (
    SELECT 
        CompanyName,
        AVG(NumericSalary) AS NationalAvgSalary
    FROM 
        CleanedSalaries
    GROUP BY 
        CompanyName
),

Ranking AS (
    SELECT 
        Location,
        CompanyName,
        AvgSalary,
        RANK() OVER (PARTITION BY Location ORDER BY AvgSalary DESC) AS Rank
    FROM 
        AvgSalaries
),

TopCompanies AS (
    SELECT 
        r.Location,
        r.CompanyName,
        r.AvgSalary AS Average_Salary_in_Location,
        n.NationalAvgSalary AS Average_Salary_in_Country
    FROM 
        Ranking r
    JOIN 
        NationalAvgSalaries n ON r.CompanyName = n.CompanyName
    WHERE 
        r.Rank <= 5
)

SELECT 
    Location,
    CompanyName,
    Average_Salary_in_Location,
    Average_Salary_in_Country
FROM 
    TopCompanies
ORDER BY 
    Location, Average_Salary_in_Location DESC;"
local258,"Calculate the total number of wickets taken by each bowler (excluding run-outs and other dismissals not attributed to the bowler), their economy rate (total runs conceded divided by total overs bowled, considering only runs scored off the bat and ignoring any extra runs like wides and no-balls), their strike rate (average number of balls bowled per wicket taken), and their best bowling performance in a single match (the match with the most wickets taken by the bowler, formatted as ""wickets-runs"" where runs are the runs conceded excluding extras).","WITH Wickets AS (
    SELECT 
        bbb.bowler,
        COUNT(*) AS wickets,
        bbb.match_id
    FROM wicket_taken wt
    JOIN ball_by_ball bbb ON wt.match_id = bbb.match_id AND wt.over_id = bbb.over_id AND wt.ball_id = bbb.ball_id AND wt.innings_no = bbb.innings_no
    WHERE wt.kind_out NOT IN ('run out', 'retired hurt', 'obstructing the field')
    GROUP BY bbb.bowler, bbb.match_id
),
RunsConceded AS (
    SELECT
        bbb.bowler,
        SUM(bs.runs_scored) AS runs_conceded
    FROM ball_by_ball bbb
    JOIN batsman_scored bs ON bbb.match_id = bs.match_id AND bbb.over_id = bs.over_id AND bbb.ball_id = bs.ball_id AND bbb.innings_no = bs.innings_no
    GROUP BY bbb.bowler
),
TotalBallsBowled AS (
    SELECT 
        bowler,
        COUNT(*) AS balls_bowled
    FROM ball_by_ball
    GROUP BY bowler
),
BestPerformance AS (
    SELECT 
        bowler,
        match_id,
        wickets,
        SUM(bs.runs_scored) AS runs_conceded
    FROM Wickets w
    JOIN ball_by_ball bbb ON w.match_id = bbb.match_id AND w.bowler = bbb.bowler
    JOIN batsman_scored bs ON bbb.match_id = bs.match_id AND bbb.over_id = bs.over_id AND bbb.ball_id = bs.ball_id AND bbb.innings_no = bs.innings_no
    GROUP BY bowler, match_id, wickets
    ORDER BY bowler, wickets DESC, runs_conceded
),
BestPerformancePerBowler AS (
    SELECT 
        bowler,
        MAX(wickets || '-' || runs_conceded) AS best_performance
    FROM BestPerformance
    GROUP BY bowler
)
SELECT 
    w.bowler,
    COUNT(*) AS total_wickets,
    IFNULL(SUM(rc.runs_conceded) / (SUM(tbb.balls_bowled) / 6.0), 0) AS economy_rate,
    SUM(tbb.balls_bowled) / COUNT(*) AS strike_rate,
    bpb.best_performance
FROM Wickets w
JOIN RunsConceded rc ON w.bowler = rc.bowler
JOIN TotalBallsBowled tbb ON w.bowler = tbb.bowler
JOIN BestPerformancePerBowler bpb ON w.bowler = bpb.bowler
GROUP BY w.bowler"
local259,"For each player, list their ID, name, their most frequent role across all matches, batting hand, bowling skill, total runs scored, total matches played, total times they were dismissed, batting average (total runs divided by total dismissals), highest score in a single match, the number of matches in which they scored at least 30 runs, at least 50 runs, and at least 100 runs, total balls faced in their career, strike rate (total runs divided by total balls faced, multiplied by 100), total wickets taken, economy rate (average runs conceded per over), and their best bowling performance in a single match (most wickets taken in a match, formatted as ""wickets taken-runs given"", where the best performance is the one with the most wickets, and if tied, the fewest runs conceded). Ignore the extra runs data.","WITH PlayerMatches AS (
    SELECT 
        p.player_id,
        p.player_name,
        p.batting_hand,
        p.bowling_skill,
        COUNT(DISTINCT b.match_id) AS total_matches,
        SUM(bs.runs_scored) AS total_runs,
        COUNT(w.player_out) AS total_dismissals,
        MAX(bs.runs_scored) AS highest_score
    FROM player p
    LEFT JOIN ball_by_ball b ON p.player_id = b.striker
    LEFT JOIN batsman_scored bs ON b.match_id = bs.match_id AND b.over_id = bs.over_id AND b.ball_id = bs.ball_id AND b.innings_no = bs.innings_no
    LEFT JOIN wicket_taken w ON b.match_id = w.match_id AND b.over_id = w.over_id AND b.ball_id = w.ball_id AND b.innings_no = w.innings_no AND w.player_out = p.player_id
    GROUP BY p.player_id
),
RunsCategories AS (
    SELECT 
        p.player_id,
        COUNT(DISTINCT b.match_id) FILTER (WHERE bs.runs_scored >= 30) AS matches_30plus_runs,
        COUNT(DISTINCT b.match_id) FILTER (WHERE bs.runs_scored >= 50) AS matches_50plus_runs,
        COUNT(DISTINCT b.match_id) FILTER (WHERE bs.runs_scored >= 100) AS matches_100plus_runs
    FROM player p
    JOIN ball_by_ball b ON p.player_id = b.striker
    JOIN batsman_scored bs ON b.match_id = bs.match_id AND b.over_id = bs.over_id AND b.ball_id = bs.ball_id AND b.innings_no = bs.innings_no
    GROUP BY p.player_id
),
TotalBallsFaced AS (
    SELECT 
        p.player_id,
        COUNT(*) AS total_balls_faced
    FROM player p
    JOIN ball_by_ball b ON p.player_id = b.striker
    GROUP BY p.player_id
),
TotalWickets AS (
    SELECT 
        p.player_id,
        COUNT(*) AS total_wickets
    FROM player p
    JOIN ball_by_ball b ON p.player_id = b.bowler
    JOIN wicket_taken w ON b.match_id = w.match_id AND b.over_id = w.over_id AND b.ball_id = w.ball_id AND b.innings_no = w.innings_no
    GROUP BY p.player_id
),
BestBowlingPerformance AS (
    SELECT 
        p.player_id,
        MAX(CONCAT(wickets_per_match, '-', runs_given)) AS best_performance
    FROM (
        SELECT 
            b.bowler AS player_id,
            b.match_id,
            COUNT(*) AS wickets_per_match,
            SUM(bs.runs_scored) AS runs_given
        FROM ball_by_ball b
        JOIN wicket_taken w ON b.match_id = w.match_id AND b.over_id = w.over_id AND b.ball_id = w.ball_id AND b.innings_no = w.innings_no
        LEFT JOIN batsman_scored bs ON b.match_id = bs.match_id AND b.over_id = bs.over_id AND b.ball_id = bs.ball_id AND b.innings_no = bs.innings_no
        GROUP BY b.bowler, b.match_id
    ) AS MatchPerformance
    GROUP BY player_id
)
SELECT 
    pm.player_id,
    pm.player_name,
    pm.batting_hand,
    pm.bowling_skill,
    pm.total_matches,
    pm.total_runs,
    pm.total_dismissals,
    CASE WHEN pm.total_dismissals > 0 THEN (pm.total_runs * 1.0 / pm.total_dismissals) ELSE NULL END AS batting_average,
    pm.highest_score,
    rc.matches_30plus_runs,
    rc.matches_50plus_runs,
    rc.matches_100plus_runs,
    tbf.total_balls_faced,
    CASE WHEN tbf.total_balls_faced > 0 THEN (pm.total_runs * 100.0 / tbf.total_balls_faced) ELSE NULL END AS strike_rate,
    tw.total_wickets,
    bbp.best_performance
FROM PlayerMatches pm
JOIN RunsCategories rc ON pm.player_id = rc.player_id
JOIN TotalBallsFaced tbf ON pm.player_id = tbf.player_id
JOIN TotalWickets tw ON pm.player_id = tw.player_id
LEFT JOIN BestBowlingPerformance bbp ON pm.player_id = bbp.player_id;"
local262,"Which problems exceed the total number of times they appear in the solution table when counting all occurrences, across steps 1, 2, and 3, where any non-""Stack"" model's maximum test score is lower than the ""Stack"" model's test score for the same step and version?","SELECT p.name
FROM problem p
JOIN solution s ON p.name = s.name
JOIN model_score ms ON s.name = ms.name AND s.version = ms.version
LEFT JOIN model_score ms_stack ON ms.name = ms_stack.name AND ms.version = ms_stack.version AND ms.step = ms_stack.step AND ms_stack.model = 'Stack'
WHERE ms.model != 'Stack' AND ms.test_score < ms_stack.test_score
GROUP BY p.name
HAVING COUNT(DISTINCT s.version) > (SELECT COUNT(*) FROM solution WHERE name = p.name);"
local263,"Identify the L1_model associated with each model (specified by name and version) that occurs most frequently for each status ('strong' or 'soft'), along with the number of times it occurs. A model has a 'strong' status if, for any of its steps, the maximum test score among non-'Stack' models is less than the 'Stack' model's test score. It has a 'soft' status if the maximum test score among non-'Stack' models equals the 'Stack' model's test score. Count how many times each L1_model is associated with a 'strong' or 'soft' status across all models, and determine which L1_model has the highest occurrence for each status.","WITH StatusScores AS (
    SELECT
        ms.name,
        ms.version,
        CASE
            WHEN MAX(CASE WHEN ms.model <> 'Stack' THEN ms.test_score ELSE NULL END) < MAX(CASE WHEN ms.model = 'Stack' THEN ms.test_score ELSE NULL END) THEN 'strong'
            WHEN MAX(CASE WHEN ms.model <> 'Stack' THEN ms.test_score ELSE NULL END) = MAX(CASE WHEN ms.model = 'Stack' THEN ms.test_score ELSE NULL END) THEN 'soft'
        END AS status
    FROM model_score ms
    GROUP BY ms.name, ms.version
),
L1ModelCounts AS (
    SELECT
        ss.name,
        ss.version,
        ss.status,
        m.L1_model,
        COUNT(*) AS count
    FROM StatusScores ss
    JOIN model m ON ss.name = m.name AND ss.version = m.version
    GROUP BY ss.status, m.L1_model
)
SELECT
    status,
    L1_model,
    MAX(count) AS max_count
FROM L1ModelCounts
GROUP BY status;"
local264,"Which model category (L1_model) appears the most frequently across all steps and versions when comparing traditional models to the Stack model, and what is the total count of its occurrences?","SELECT L1_model, COUNT(*) AS total_count
FROM model
WHERE name IN ('traditional', 'Stack')
GROUP BY L1_model
ORDER BY total_count DESC
LIMIT 1;"
local269,"What is the average total quantity across all final packaging combinations, considering only the leaf-level items within each combination after fully expanding any nested packaging relationships?","WITH RECURSIVE expanded_packaging AS (
    SELECT pr.packaging_id, pr.contains_id, pr.qty
    FROM packaging_relations pr
    LEFT JOIN packaging_relations sub ON pr.contains_id = sub.packaging_id
    WHERE sub.packaging_id IS NULL

    UNION ALL

    SELECT ep.packaging_id, pr.contains_id, ep.qty * pr.qty AS qty
    FROM expanded_packaging ep
    JOIN packaging_relations pr ON ep.contains_id = pr.packaging_id
)
SELECT AVG(total_qty) AS average_total_quantity
FROM (
    SELECT packaging_id, SUM(qty) AS total_qty
    FROM expanded_packaging
    GROUP BY packaging_id
) AS final_totals;"
local270,"Which top-level packaging containers, meaning those not contained within any other packaging, have any item for which the total quantity accumulated across all nested levels in the hierarchy exceeds 500, and what are the names of both these containers and the corresponding items?","WITH RECURSIVE PackagingHierarchy AS (
    SELECT 
        pr.packaging_id AS top_level_id, 
        pr.contains_id AS item_id, 
        pr.qty, 
        p.name AS top_level_name, 
        pi.name AS item_name
    FROM 
        packaging_relations pr
    JOIN 
        packaging p ON pr.packaging_id = p.id
    JOIN 
        packaging pi ON pr.contains_id = pi.id
    WHERE 
        pr.packaging_id NOT IN (SELECT contains_id FROM packaging_relations)
    
    UNION ALL
    
    SELECT 
        ph.top_level_id, 
        pr.contains_id AS item_id, 
        ph.qty * pr.qty AS qty, 
        ph.top_level_name, 
        pi.name AS item_name
    FROM 
        PackagingHierarchy ph
    JOIN 
        packaging_relations pr ON ph.item_id = pr.packaging_id
    JOIN 
        packaging pi ON pr.contains_id = pi.id
)
SELECT 
    top_level_id, 
    top_level_name, 
    item_id, 
    item_name, 
    SUM(qty) AS total_quantity
FROM 
    PackagingHierarchy
GROUP BY 
    top_level_id, top_level_name, item_id, item_name
HAVING 
    SUM(qty) > 500;"
local272,"For order 423, identify the product IDs, aisles, and positions from which to pick the exact quantities needed for each order line, ensuring that the total picked quantity for each product matches the cumulative quantities ordered without exceeding the available inventory in warehouse 1. Calculate the quantities to be picked from each location by prioritizing inventory with earlier purchased dates and smaller quantities, and ensure that picking respects the sequence and cumulative quantities of the order lines for products with multiple entries.","WITH OrderedProducts AS (
    SELECT ol.product_id, ol.qty AS ordered_qty
    FROM orderlines ol
    WHERE ol.order_id = 423
),
AvailableInventory AS (
    SELECT i.product_id, l.aisle, l.position, i.qty AS available_qty, i.purchase_id
    FROM inventory i
    JOIN locations l ON i.location_id = l.id
    WHERE l.warehouse = 1
),
PurchaseDates AS (
    SELECT purchase_id, MIN(purchase_date) AS earliest_date
    FROM purchases
    GROUP BY purchase_id
),
InventoryWithDates AS (
    SELECT ai.product_id, ai.aisle, ai.position, ai.available_qty, pd.earliest_date
    FROM AvailableInventory ai
    JOIN PurchaseDates pd ON ai.purchase_id = pd.purchase_id
),
OrderedInventory AS (
    SELECT op.product_id, iwd.aisle, iwd.position, 
           LEAST(op.ordered_qty, SUM(iwd.available_qty)) AS pick_qty
    FROM OrderedProducts op
    JOIN InventoryWithDates iwd ON op.product_id = iwd.product_id
    GROUP BY op.product_id, iwd.aisle, iwd.position
    ORDER BY iwd.earliest_date, iwd.available_qty
)
SELECT DISTINCT oinv.product_id, oinv.aisle, oinv.position, oinv.pick_qty
FROM OrderedInventory oinv
WHERE oinv.pick_qty > 0;"
local273,"Calculate the average pick percentage for each product name, using a first-in-first-out approach that selects from inventory locations based on the earliest purchase date and smallest available quantity, ensuring that the picked quantity reflects only the overlapping range between each order’s required quantity and the inventory’s available quantity, and then grouping and ordering the results by product name?","WITH OrderedInventory AS (
    SELECT 
        i.product_id,
        i.location_id,
        p.purchase_id,
        p.purchased,
        i.qty AS available_qty
    FROM inventory i
    JOIN purchases p ON i.purchase_id = p.id
    ORDER BY p.purchased ASC, i.qty ASC
),
FIFOInventory AS (
    SELECT 
        oi.product_id,
        oi.location_id,
        SUM(oi.available_qty) OVER (PARTITION BY oi.product_id ORDER BY oi.purchased ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_qty
    FROM OrderedInventory oi
),
PickingAnalysis AS (
    SELECT 
        pl.product_id,
        pl.qty AS required_qty,
        SUM(pl.qty) OVER (PARTITION BY pl.product_id ORDER BY pl.picklist_id, pl.line_no) AS cumulative_required,
        fi.cumulative_qty
    FROM picking_line pl
    JOIN FIFOInventory fi ON pl.product_id = fi.product_id AND pl.location_id = fi.location_id
),
PickMatches AS (
    SELECT 
        pa.product_id,
        LEAST(pa.cumulative_required, pa.cumulative_qty) - GREATEST(LEAST(pa.cumulative_required, pa.cumulative_qty) - pa.required_qty, 0) AS effective_pick_qty
    FROM PickingAnalysis pa
),
ProductPickPercentage AS (
    SELECT 
        p.productname,
        SUM(pm.effective_pick_qty) / SUM(pl.qty) * 100 AS average_pick_percentage
    FROM PickMatches pm
    JOIN picking_line pl ON pm.product_id = pl.product_id
    JOIN products p ON pm.product_id = p.productid
    GROUP BY p.productname
)
SELECT 
    productname,
    average_pick_percentage
FROM ProductPickPercentage
ORDER BY productname;"
local274,"Which products were picked for order 421, and what is the average number of units picked for each product, using FIFO (First-In, First-Out) method?",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
local275,"Based on monthly sales data starting in January 2016 and using a centered moving average to adjust for seasonality, which products had a seasonality-adjusted sales ratio that stayed consistently above 2 for every month in the year 2017?","WITH MonthlySales AS (
    SELECT 
        product_id,
        mth,
        qty AS actual_sales,
        AVG(qty) OVER (PARTITION BY product_id ORDER BY mth ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) AS moving_avg_sales
    FROM monthly_sales
    WHERE strftime('%Y', mth) BETWEEN '2016' AND '2017'
),
SeasonalityAdjusted AS (
    SELECT 
        product_id,
        mth,
        actual_sales / moving_avg_sales AS adjusted_ratio
    FROM MonthlySales
    WHERE strftime('%Y', mth) = '2017'
),
ConsistentHighPerformers AS (
    SELECT 
        product_id
    FROM SeasonalityAdjusted
    GROUP BY product_id
    HAVING MIN(adjusted_ratio) > 2
)
SELECT DISTINCT product_id
FROM ConsistentHighPerformers;"
local277,"What is the average forecasted annual sales for products 4160 and 7790 during 2018, using monthly sales data starting from January 2016 for the first 36 months, applying seasonality adjustments from time steps 7 through 30, and employing a weighted regression method to estimate sales?","The provided context is insufficient to generate the SQL query for several reasons:

1. The table structure lacks any explicit columns or data related to ""forecasted sales,"" which are required to calculate average forecasted annual sales.
2. There is no information or data on ""seasonality adjustments"" or how they should be applied to the data.
3. The term ""weighted regression method"" implies a statistical computation that typically cannot be directly implemented in SQL without additional functions or extensions that are not standard in basic SQL environments.

To proceed with generating a forecast using SQL, additional information or integration with statistical tools or extensions that support advanced analytics functions would be required."
local279,"Using a recursive monthly inventory adjustment model starting from December 2018 inventory levels, where we restock a product if its ending inventory drops below the minimum required level, determine for each product the month in 2019 where the absolute difference between its ending inventory and the minimum required level is the smallest, and return the product_id, that month, and the absolute difference.","The context provided lacks explicit information about initial inventory levels for December 2018, monthly sales data for 2019, and inventory replenishment data for 2019. These details are necessary to accurately simulate the inventory levels across the months of 2019 and to compute the absolute differences from the minimum required levels.

Please provide the initial inventory levels for December 2018, details on how inventory is replenished, and monthly sales data for 2019, or clarify if these should be assumed or approximated based on existing tables."
local283,"Analyze the soccer match dataset to determine the champion team for each season across all countries and leagues, awarding 3 points for every win, 1 point for every tie, and 0 points for every loss. For each season, return the champion’s team name, the league, the country, and the total points accumulated.","SELECT 
    season, 
    country_id,
    league_id,
    home_team_api_id AS team_id,
    SUM(points) AS total_points
FROM (
    SELECT 
        season, 
        country_id,
        league_id,
        home_team_api_id,
        CASE 
            WHEN home_team_goal > away_team_goal THEN 3
            WHEN home_team_goal = away_team_goal THEN 1
            ELSE 0 
        END AS points
    FROM Match
    UNION ALL
    SELECT 
        season, 
        country_id,
        league_id,
        away_team_api_id,
        CASE 
            WHEN home_team_goal < away_team_goal THEN 3
            WHEN home_team_goal = away_team_goal THEN 1
            ELSE 0 
        END AS points
    FROM Match
) AS results
GROUP BY season, country_id, league_id, team_id
ORDER BY season, country_id, league_id, total_points DESC;"
local284,"For veg whsle data, can you generate a summary of our items' loss rates? Include the average loss rate, and also break down the count of items that are below, above, and within one standard deviation from this average.","WITH LossRateStats AS (
    SELECT 
        AVG(""loss_rate_%"") AS avg_loss_rate, 
        STDDEV(""loss_rate_%"") AS stddev_loss_rate
    FROM veg_loss_rate_df
)

SELECT 
    (SELECT avg_loss_rate FROM LossRateStats) AS average_loss_rate,
    COUNT(CASE WHEN ""loss_rate_%"" < (SELECT avg_loss_rate - stddev_loss_rate FROM LossRateStats) THEN 1 END) AS below_one_stddev,
    COUNT(CASE WHEN ""loss_rate_%"" BETWEEN (SELECT avg_loss_rate - stddev_loss_rate FROM LossRateStats) AND (SELECT avg_loss_rate + stddev_loss_rate FROM LossRateStats) THEN 1 END) AS within_one_stddev,
    COUNT(CASE WHEN ""loss_rate_%"" > (SELECT avg_loss_rate + stddev_loss_rate FROM LossRateStats) THEN 1 END) AS above_one_stddev
FROM veg_loss_rate_df;"
local285,"For veg whsle data, can you analyze our financial performance over the years 2020 to 2023? I need insights into the average wholesale price, maximum wholesale price, minimum wholesale price, wholesale price difference, total wholesale price, total selling price, average loss rate, total loss, and profit for each category within each year. Round all calculated values to two decimal places.","SELECT 
  SUBSTR(whsle_date, 1, 4) AS year,
  v.item_code,
  v.item_name,
  ROUND(AVG(w.whsle_px_rmb_kg), 2) AS avg_wholesale_price,
  ROUND(MAX(w.whsle_px_rmb_kg), 2) AS max_wholesale_price,
  ROUND(MIN(w.whsle_px_rmb_kg), 2) AS min_wholesale_price,
  ROUND(MAX(w.whsle_px_rmb_kg) - MIN(w.whsle_px_rmb_kg), 2) AS wholesale_price_difference,
  ROUND(SUM(w.whsle_px_rmb_kg * t.qty_sold_kg), 2) AS total_wholesale_price,
  ROUND(SUM(t.unit_selling_px_rmb_kg * t.qty_sold_kg), 2) AS total_selling_price,
  ROUND(AVG(v.loss_rate_%), 2) AS average_loss_rate,
  ROUND(SUM(v.loss_rate_% / 100 * t.qty_sold_kg * t.unit_selling_px_rmb_kg), 2) AS total_loss,
  ROUND(SUM(t.unit_selling_px_rmb_kg * t.qty_sold_kg) - SUM(v.loss_rate_% / 100 * t.qty_sold_kg * t.unit_selling_px_rmb_kg) - SUM(w.whsle_px_rmb_kg * t.qty_sold_kg), 2) AS profit
FROM veg_whsle_df w
JOIN veg_txn_df t ON w.item_code = t.item_code AND SUBSTR(t.txn_date, 1, 4) = SUBSTR(w.whsle_date, 1, 4)
JOIN veg_loss_rate_df v ON v.item_code = t.item_code
WHERE SUBSTR(w.whsle_date, 1, 4) BETWEEN '2020' AND '2023'
GROUP BY year, v.item_code, v.item_name"
local286,"Prepare a comprehensive performance report on our sellers, focusing on total sales, average item price, average review scores, and packing times. Ensure that the report includes only those sellers who have sold a quantity of more than 100 products and highlight the product category names in English with the highest sales volume.","SELECT 
    o.seller_id,
    p.product_category_name,
    COUNT(DISTINCT o.order_id) AS total_sales,
    AVG(oi.price) AS average_item_price,
    AVG(pr.rating) AS average_review_score,
    AVG(o.order_metric_production_time) AS average_packing_time
FROM 
    order_items oi
JOIN 
    orders o ON oi.order_id = o.order_id
JOIN 
    products p ON oi.product_id = p.product_id
LEFT JOIN 
    productreview pr ON p.product_id = pr.productid
GROUP BY 
    o.seller_id, p.product_category_name
HAVING 
    SUM(oi.quantity) > 100
ORDER BY 
    total_sales DESC"
local301,"For weekly-sales data, I need an analysis of our sales performance around mid-June for the years 2018, 2019, and 2020. Specifically, calculate the percentage change in sales between the four weeks leading up to June 15 and the four weeks following June 15 for each year.","WITH DateRanges AS (
    SELECT 
        '2018-05-18' AS start_period_1, '2018-06-14' AS end_period_1, '2018-06-16' AS start_period_2, '2018-07-13' AS end_period_2, 2018 AS year
    UNION ALL
    SELECT 
        '2019-05-18', '2019-06-14', '2019-06-16', '2019-07-13', 2019
    UNION ALL
    SELECT 
        '2020-05-18', '2020-06-14', '2020-06-16', '2020-07-13', 2020
),
SalesPeriods AS (
    SELECT 
        dr.year,
        SUM(CASE WHEN w.week_date BETWEEN dr.start_period_1 AND dr.end_period_1 THEN w.sales ELSE 0 END) AS sales_pre,
        SUM(CASE WHEN w.week_date BETWEEN dr.start_period_2 AND dr.end_period_2 THEN w.sales ELSE 0 END) AS sales_post
    FROM 
        weekly_sales w
    INNER JOIN 
        DateRanges dr ON w.week_date BETWEEN dr.start_period_1 AND dr.end_period_2
    GROUP BY 
        dr.year
)
SELECT 
    year,
    sales_pre,
    sales_post,
    CASE WHEN sales_pre = 0 THEN NULL ELSE (CAST(sales_post AS REAL) - sales_pre) / sales_pre * 100 END AS percent_change
FROM 
    SalesPeriods;"
local302,"Analyze the average percentage change in sales between the 12 weeks before and after June 15, 2020, for each attribute type: region, platform, age band, demographic, and customer type. For each attribute type, calculate the average percentage change in sales across all its attribute values. Identify the attribute type with the highest negative impact on sales and provide its average percentage change in sales.","WITH PreAfterJune AS (
    SELECT 
        region, platform, age_band, demographic, customer_type,
        SUM(CASE WHEN week_date >= '2020-03-23' AND week_date < '2020-06-15' THEN sales ELSE 0 END) AS sales_pre,
        SUM(CASE WHEN week_date > '2020-06-15' AND week_date <= '2020-09-07' THEN sales ELSE 0 END) AS sales_post
    FROM cleaned_weekly_sales
    GROUP BY region, platform, age_band, demographic, customer_type
),
PercentageChange AS (
    SELECT 
        region, platform, age_band, demographic, customer_type,
        (CAST(sales_post AS REAL) - sales_pre) / sales_pre * 100 AS percent_change
    FROM PreAfterJune
    WHERE sales_pre > 0 AND sales_post > 0
),
AvgPercentageChange AS (
    SELECT 
        'Region' AS attribute_type, AVG(percent_change) AS avg_percent_change FROM PercentageChange GROUP BY region
    UNION ALL
    SELECT 
        'Platform' AS attribute_type, AVG(percent_change) AS avg_percent_change FROM PercentageChange GROUP BY platform
    UNION ALL
    SELECT 
        'Age Band' AS attribute_type, AVG(percent_change) AS avg_percent_change FROM PercentageChange GROUP BY age_band
    UNION ALL
    SELECT 
        'Demographic' AS attribute_type, AVG(percent_change) AS avg_percent_change FROM PercentageChange GROUP BY demographic
    UNION ALL
    SELECT 
        'Customer Type' AS attribute_type, AVG(percent_change) AS avg_percent_change FROM PercentageChange GROUP BY customer_type
),
MaxNegativeImpact AS (
    SELECT attribute_type, MIN(avg_percent_change) AS max_negative_impact
    FROM AvgPercentageChange
    GROUP BY attribute_type
    ORDER BY max_negative_impact ASC
    LIMIT 1
)
SELECT * FROM MaxNegativeImpact;"
local329,"How many unique sessions visited the /regist/input page and then the /regist/confirm page, in that order?",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
local330,"Using the activity log table, compute the total number of unique user sessions where each web page appears as either a landing page (the first page visited in a session based on timestamp) or an exit page (the last page visited in a session based on timestamp), or both. Count each session only once per page even if the page serves as both landing and exit for that session. ","WITH RankedSessions AS (
    SELECT
        session,
        path,
        RANK() OVER (PARTITION BY session ORDER BY stamp ASC) AS rank_asc,
        RANK() OVER (PARTITION BY session ORDER BY stamp DESC) AS rank_desc
    FROM activity_log
),
LandingExitPages AS (
    SELECT
        path,
        session
    FROM RankedSessions
    WHERE rank_asc = 1 OR rank_desc = 1
    GROUP BY path, session
)
SELECT 
    path AS webpage,
    COUNT(DISTINCT session) AS unique_sessions
FROM LandingExitPages
GROUP BY path;"
local331,"Which three distinct third-page visits are most frequently observed immediately after two consecutive visits to the '/detail' page, and how many times does each third-page visit occur?",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
local358,"How many users are there in each age category (20s, 30s, 40s, 50s, and others)?","SELECT
  CASE 
    WHEN age BETWEEN 20 AND 29 THEN '20s'
    WHEN age BETWEEN 30 AND 39 THEN '30s'
    WHEN age BETWEEN 40 AND 49 THEN '40s'
    WHEN age BETWEEN 50 AND 59 THEN '50s'
    ELSE 'Others'
  END AS age_category,
  COUNT(*) AS user_count
FROM alien_data
GROUP BY age_category;"
local360,"For each user session in the activity log table, identify the number of events that occurred before the first '/detail' click or '/complete' conversion, counting only events that have a non-empty search type. Find the sessions with the minimum count of such pre-click/pre-conversion events. If multiple sessions share this minimum count, include all of them in the results. Return each qualifying session along with the corresponding path and search type.","WITH FirstClickOrConversion AS (
    SELECT
        session,
        MIN(stamp) AS first_event_time
    FROM
        activity_log
    WHERE
        (path = '/detail' OR path = '/complete')
    GROUP BY
        session
),
CountBeforeFirstEvent AS (
    SELECT
        a.session,
        COUNT(*) AS count_events
    FROM
        activity_log a
    JOIN
        FirstClickOrConversion fc ON a.session = fc.session AND a.stamp < fc.first_event_time
    WHERE
        a.search_type <> ''
    GROUP BY
        a.session
),
MinEvents AS (
    SELECT
        MIN(count_events) AS min_events
    FROM
        CountBeforeFirstEvent
)
SELECT
    c.session,
    a.path,
    a.search_type
FROM
    CountBeforeFirstEvent c
JOIN
    activity_log a ON c.session = a.session
JOIN
    MinEvents m ON c.count_events = m.min_events
WHERE
    a.search_type <> ''
ORDER BY
    c.session;"
local344,"Considering all races where pit stop data is available, and focusing on instances when a driver was not behind another car on the previous lap but is behind on the current lap (accounting for retirements, pit-stop entries, pit-stop exits, and race starts), how many times has each type of overtake occurred in Formula 1?","WITH RelevantLaps AS (
    SELECT lp.race_id,
           lp.driver_id,
           lp.lap,
           lp.position AS current_position,
           LAG(lp.position) OVER (PARTITION BY lp.race_id, lp.driver_id ORDER BY lp.lap) AS previous_position,
           ps.lap AS pit_stop_lap
    FROM lap_positions lp
    LEFT JOIN pit_stops ps ON lp.race_id = ps.race_id AND lp.driver_id = ps.driver_id AND lp.lap = ps.lap
    WHERE EXISTS (SELECT 1 FROM races_ext re WHERE re.race_id = lp.race_id AND re.is_pit_data_available = 'true')
),

FilteredOvertakes AS (
    SELECT race_id, driver_id, lap, current_position, previous_position
    FROM RelevantLaps
    WHERE (previous_position IS NULL OR previous_position > current_position) 
          AND (current_position < previous_position OR (previous_position IS NULL AND current_position IS NOT NULL))
          AND pit_stop_lap IS NULL
),

OvertakeTypes AS (
    SELECT fo.race_id,
           fo.driver_id,
           fo.lap,
           CASE 
               WHEN previous_position IS NULL THEN 'Start'
               WHEN previous_position > current_position THEN 'On-track'
               ELSE 'Other'
           END AS overtake_type
    FROM FilteredOvertakes fo
    LEFT JOIN retirements r ON fo.race_id = r.race_id AND fo.driver_id = r.driver_id AND fo.lap = r.lap
    WHERE r.lap IS NULL
)

SELECT overtake_type, COUNT(*) AS overtake_count
FROM OvertakeTypes
GROUP BY overtake_type;"
local336,"In the first five laps of the race, how many overtakes occurred in each category—retirements, pit stops, start-related overtakes, and standard on-track passes?","WITH LapData AS (
  SELECT race_id, driver_id, lap, position
  FROM lap_positions
  WHERE lap BETWEEN 1 AND 5
),
Retirements AS (
  SELECT race_id, driver_id, COUNT(*) AS retirement_overtakes
  FROM retirements
  WHERE lap BETWEEN 1 AND 5
  GROUP BY race_id, driver_id
),
PitStops AS (
  SELECT race_id, driver_id, COUNT(*) AS pitstop_overtakes
  FROM pit_stops
  WHERE lap BETWEEN 1 AND 5
  GROUP BY race_id, driver_id
),
PositionChanges AS (
  SELECT 
    L1.race_id,
    L1.driver_id,
    SUM(CASE WHEN L1.position > L2.position THEN 1 ELSE 0 END) AS on_track_overtakes
  FROM LapData L1
  JOIN LapData L2 ON L1.race_id = L2.race_id AND L1.lap = L2.lap + 1 AND L1.driver_id = L2.driver_id
  GROUP BY L1.race_id, L1.driver_id
)
SELECT 
  L.race_id,
  L.driver_id,
  COALESCE(R.retirement_overtakes, 0) AS retirement_overtakes,
  COALESCE(P.pitstop_overtakes, 0) AS pitstop_overtakes,
  COALESCE(PC.on_track_overtakes, 0) AS on_track_overtakes
FROM LapData L
LEFT JOIN Retirements R ON L.race_id = R.race_id AND L.driver_id = R.driver_id
LEFT JOIN PitStops P ON L.race_id = P.race_id AND L.driver_id = P.driver_id
LEFT JOIN PositionChanges PC ON L.race_id = PC.race_id AND L.driver_id = PC.driver_id
GROUP BY L.race_id, L.driver_id"
local335,"In Formula 1 seasons since 2001, considering only drivers who scored points in a season, which five constructors have had the most seasons where their drivers scored the fewest total points among all point-scoring drivers in that season?","WITH SeasonDriverPoints AS (
    SELECT 
        ds.year,
        q.constructor_id,
        SUM(ds.points) AS total_points
    FROM driver_standings ds
    JOIN qualifying q ON ds.driver_id = q.driver_id AND ds.race_id = q.race_id
    WHERE ds.year >= 2001
    GROUP BY ds.year, q.constructor_id
    HAVING total_points > 0
),
RankedSeasons AS (
    SELECT 
        year,
        constructor_id,
        total_points,
        RANK() OVER (PARTITION BY year ORDER BY total_points ASC) AS rank
    FROM SeasonDriverPoints
),
LowestPointConstructors AS (
    SELECT 
        constructor_id,
        COUNT(*) AS seasons_with_lowest_points
    FROM RankedSeasons
    WHERE rank = 1
    GROUP BY constructor_id
)
SELECT 
    constructor_id,
    seasons_with_lowest_points
FROM LowestPointConstructors
ORDER BY seasons_with_lowest_points DESC
LIMIT 5;"
local309,"For each year, which driver and which constructor scored the most points? I want the full name of each driver.",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
local310,"Using only the data from the ‘results’ table, find the three years in which the sum of the highest total points earned by any driver and the highest total points earned by any constructor in that year (both calculated by summing up points from the ‘results’ table) is smallest, and list those three years in order of ascending total.","The provided context does not include a table named 'results'. Therefore, I cannot generate a SQL query to fulfill the request based on the available tables."
local311,"Which constructors had the top 3 combined points from their best driver and team, and in which years did they achieve them?","WITH BestDriverPoints AS (
    SELECT ds.year, ds.constructor_id, MAX(ds.points) AS max_driver_points
    FROM (
        SELECT dse.year, q.constructor_id, SUM(dse.points) AS points
        FROM driver_standings_ext dse
        JOIN qualifying q ON dse.driver_id = q.driver_id AND dse.race_id = q.race_id
        GROUP BY dse.year, q.constructor_id, dse.driver_id
    ) ds
    GROUP BY ds.year, ds.constructor_id
),
TeamPoints AS (
    SELECT r.year, cs.constructor_id, SUM(cs.points) AS total_team_points
    FROM constructor_standings cs
    JOIN races_ext r ON cs.race_id = r.race_id
    GROUP BY r.year, cs.constructor_id
),
CombinedPoints AS (
    SELECT bd.year, bd.constructor_id, (bd.max_driver_points + tp.total_team_points) AS combined_points
    FROM BestDriverPoints bd
    JOIN TeamPoints tp ON bd.year = tp.year AND bd.constructor_id = tp.constructor_id
)
SELECT year, constructor_id, combined_points
FROM CombinedPoints
QUALIFY RANK() OVER (PARTITION BY year ORDER BY combined_points DESC) <= 3;"
local354,"Among Formula 1 drivers who raced during the 1950s, which drivers completed a season in that decade with the same constructor in both the first and the last race they participated in, while also taking part in at least two distinct race rounds during that season?","SELECT DISTINCT d.year, d.driver_id 
FROM drives d 
JOIN (
    SELECT year, driver_id, MIN(first_round) AS first_race, MAX(last_round) AS last_race 
    FROM drives 
    WHERE year BETWEEN 1950 AND 1959 
    GROUP BY year, driver_id 
    HAVING COUNT(DISTINCT first_round) > 1 AND COUNT(DISTINCT last_round) > 1
) sub ON d.year = sub.year AND d.driver_id = sub.driver_id 
WHERE d.first_round = sub.first_race AND d.last_round = sub.last_race AND d.constructor_id = (
    SELECT constructor_id 
    FROM drives 
    WHERE year = d.year AND driver_id = d.driver_id AND first_round = sub.first_race
)"
local355,"Calculate the overall average first round and average last round of races missed by Formula 1 drivers across all years. Include only drivers who missed fewer than three races in a given year and who switched teams between their appearances immediately before and after their hiatus (i.e., the constructor ID for the race right before their first missed race must be different from the constructor ID for the race right after their last missed race in that year). Do not group results by year; return just the overall averages across the entire dataset.","WITH MissedRaceDetails AS (
    SELECT 
        d.year,
        d.driver_id,
        MIN(d.first_round) - 1 AS last_round_before_miss,
        MAX(d.last_round) + 1 AS first_round_after_miss
    FROM drives d
    WHERE d.is_first_drive_of_season = 0 OR d.is_final_drive_of_season = 0
    GROUP BY d.year, d.driver_id
    HAVING COUNT(*) < 3
),
TeamChanges AS (
    SELECT 
        m.year,
        m.driver_id,
        m.last_round_before_miss,
        m.first_round_after_miss,
        d1.constructor_id AS constructor_before,
        d2.constructor_id AS constructor_after
    FROM MissedRaceDetails m
    JOIN drives d1 ON m.year = d1.year AND m.driver_id = d1.driver_id AND d1.last_round = m.last_round_before_miss
    JOIN drives d2 ON m.year = d2.year AND m.driver_id = d2.driver_id AND d2.first_round = m.first_round_after_miss
    WHERE d1.constructor_id != d2.constructor_id
)
SELECT 
    AVG(last_round_before_miss) AS avg_last_round_before_miss,
    AVG(first_round_after_miss) AS avg_first_round_after_miss
FROM TeamChanges;"
local356,"Provide the full names of drivers who have been overtaken on track more times than they have overtaken others on track during race laps, excluding position changes due to pit stops (both at pit entry and exit), retirements, or position changes that occurred during the first lap of a race (considered as start movements).","WITH Overtakes AS (
    SELECT
        l1.race_id,
        l1.driver_id AS overtaker,
        l2.driver_id AS overtaken,
        l1.lap
    FROM
        lap_positions l1
    INNER JOIN lap_positions l2 ON l1.race_id = l2.race_id AND l1.lap = l2.lap
        AND l1.position = l2.position - 1
    WHERE
        l1.lap > 1 AND l1.lap_type NOT IN ('PitIn', 'PitOut') AND l2.lap_type NOT IN ('PitIn', 'PitOut')
)

, RetirementLaps AS (
    SELECT DISTINCT
        race_id,
        driver_id,
        lap
    FROM
        retirements
)

, PitStopLaps AS (
    SELECT DISTINCT
        race_id,
        driver_id,
        lap
    FROM
        pit_stops
)

, ValidOvertakes AS (
    SELECT
        o.race_id,
        o.overtaker,
        o.overtaken,
        o.lap
    FROM
        Overtakes o
    LEFT JOIN RetirementLaps r ON o.race_id = r.race_id AND o.overtaken = r.driver_id AND o.lap = r.lap
    LEFT JOIN PitStopLaps p ON o.race_id = p.race_id AND o.overtaken = p.driver_id AND o.lap = p.lap
    WHERE
        r.driver_id IS NULL AND p.driver_id IS NULL
)

, OvertakeCounts AS (
    SELECT
        driver_id,
        COUNT(*) AS OvertakesDone,
        0 AS OvertakesReceived
    FROM
        ValidOvertakes
    GROUP BY
        driver_id
    UNION ALL
    SELECT
        overtaken AS driver_id,
        0 AS OvertakesDone,
        COUNT(*) AS OvertakesReceived
    FROM
        ValidOvertakes
    GROUP BY
        overtaken
)

, NetOvertakes AS (
    SELECT
        driver_id,
        SUM(OvertakesDone) AS TotalOvertakesDone,
        SUM(OvertakesReceived) AS TotalOvertakesReceived
    FROM
        OvertakeCounts
    GROUP BY
        driver_id
    HAVING
        TotalOvertakesReceived > TotalOvertakesDone
)

SELECT
    d.full_name
FROM
    NetOvertakes no
INNER JOIN drivers d ON no.driver_id = d.driver_id"
sf001,"Assuming today is April 1, 2024, I would like to know the daily snowfall amounts greater than 6 inches for each U.S. postal code during the week ending after the first two full weeks of the previous year. Show the postal code, date, and snowfall amount.","The provided context is insufficient to generate the requested SQL query as there is no table containing data related to weather conditions such as daily snowfall amounts. Please provide a table that includes weather metrics, or specify the correct table if it exists within the current database schema."
sf003,"For each year from 2015 to 2020, which Census Zip Code Tabulation Area had the second-highest annual population growth rate? Include only areas with a population estimate of at least 25,000 people in that year (based on 5-Year American Community Survey estimates). For each year, provide the zip code, state abbreviation, and the annual growth rate percentage.","The provided context is insufficient to generate the requested SQL query. The tables given do not contain historical population data or annual growth rates for Census Zip Code Tabulation Areas, which are necessary to determine the second-highest annual population growth rate. Additionally, there are no tables with American Community Survey data or any fields that indicate the year of the data entries. Thus, the query cannot be generated based on the provided table structures."
sf002,"As of December 31, 2022, list the top 10 active banks with assets exceeding $10 billion, ranked by the highest percentage of uninsured assets, where the percentage is calculated as one minus the value of the '% Insured (Estimated)' variable from quarterly estimates. Provide the names of these banks and their respective percentages of uninsured assets.","The provided context is insufficient to generate the SQL query as there is no available table containing information about banks, their assets, or insured/uninsured asset estimates. Please provide the relevant table structure or data source to proceed."
sf044,"What was the percentage change in post-market close prices for the Magnificent 7 tech companies from January 1 to June 30, 2024?",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
sf006,"For each U.S. state, find how the number of active financial branch entities has changed from March 1, 2020, to December 31, 2021. An entity is considered active on a specific date if its start date is on or before that date and its end date is either null or on or after that date. For each state, calculate the number of entities active on March 1, 2020, the number of entities active on December 31, 2021, and the percentage change in these counts","The context provided is insufficient to generate the requested SQL query. The provided tables do not contain information regarding financial branch entities, their start dates, end dates, or active status. Additionally, there's no data linking branches or entities to specific U.S. states in the context of their operational timelines. To proceed, a table containing details about financial branch entities including their start and end dates, and their associated states, would be necessary."
sf008,"Determine the percentage change in gross income inflow and the seasonally-adjusted purchase-only home price index for the Phoenix-Mesa-Scottsdale, AZ Metro Area from January 1, 2023, to December 31, 2023. Gross income inflow refers to the total adjusted gross income from all financial entities within the specified metro area","The provided context is insufficient to generate the SQL query because there is no table containing data related to gross income inflow or home price indices, specifically for the Phoenix-Mesa-Scottsdale, AZ Metro Area or any other geographical location. Further, there is no data available on financial entities or any indices related to property values."
sf010,"What are the cumulative ratios of mortgages near default in California for each recorded date in 2023, including those that are 90 to 180 days past due, in forbearance, or undergoing foreclosure, bankruptcy, or deed-in-lieu processes?","The provided context is insufficient to generate a SQL query as there are no tables or columns described in the database schema related to mortgages, their statuses (such as past due, in forbearance, foreclosure, bankruptcy, or deed-in-lieu processes), or specific geographical data like states (e.g., California). A table containing mortgage details with relevant statuses and geographical identifiers would be necessary to create a SQL query for this request."
sf037,"How can we find the shortest straight-line distance in miles between each 'The Home Depot' store and its nearest 'Lowe's Home Improvement' location? Using the US_REAL_ESTATE.CYBERSYN database, join the point_of_interest_index table with point_of_interest_addresses_relationships and us_addresses tables to get geographic coordinates. For each 'The Home Depot' location (identified by its poi_id), calculate its distance to all 'Lowe's Home Improvement' stores using ST_DISTANCE and ST_MAKEPOINT functions, convert the distance from meters to miles (dividing by 1609), and return only the record with the minimum distance for each 'The Home Depot' store using QUALIFY with ROW_NUMBER().","The provided context does not include tables or a database schema related to US_REAL_ESTATE.CYBERSYN, point_of_interest_index, point_of_interest_addresses_relationships, or us_addresses. Therefore, it is not possible to generate a valid SQL query based on the current context. Please provide the relevant table structures and relationships or specify the correct database context to proceed."
sf012,"Using data from the FEMA National Flood Insurance Program Claim Index, for each year from 2010 through 2019, what were the total building damage amounts and total contents damage amounts reported under the National Flood Insurance Program for the NFIP community named 'City Of New York,' grouped by each year of loss?","The provided context is insufficient to generate the SQL query because there is no table or dataset related to the FEMA National Flood Insurance Program Claim Index or any data regarding building damage amounts, contents damage amounts, or NFIP community names provided in the schema descriptions."
sf018,"Examine user engagement with push notifications within a specified one-hour window on June 1, 2023.","The provided context is insufficient to generate a SQL query for examining user engagement with push notifications. There is no table or column in the provided schema that specifically relates to push notifications, user engagement data, or timestamps that would allow filtering by a specific hour on a specific day."
sf035,"How many unique users started sessions each day within each app group between June 1, 2023, and June 7, 2023? Also show the app group ID and the start day of the session.","SELECT COUNT(DISTINCT al.user_id) AS unique_users, al.session, DATE(al.stamp) AS session_start_day
FROM access_log al
WHERE DATE(al.stamp) BETWEEN '2023-06-01' AND '2023-06-07'
GROUP BY al.session, DATE(al.stamp);"
sf029,"Generate a daily detailed sales report for each product under the 'Manufacturing' distributor view, covering the 30 days leading up to February 6, 2022, by joining the sales, traffic, inventory, and net PPM data on date, ASIN, program, period, and distributor_view. The report must include total ordered units, ordered revenue, average selling price, glance views, conversion rate, shipped units, shipped revenue, average net PPM, average procurable product OOS, total on-hand units and value, net received units and value, open purchase order quantities, unfilled customer ordered units, and average vendor confirmation rate, receive fill rate, sell-through rate, and vendor lead time.","The provided context is insufficient to generate the requested SQL query because there are no tables or columns that match the descriptions provided in your request. Specifically, there are no tables related to traffic, inventory, net PPM, ASIN, program, period, distributor_view, glance views, conversion rate, procurable product OOS, vendor confirmation rate, receive fill rate, sell-through rate, or vendor lead time in the schema descriptions given. Without these tables or relevant fields, it is impossible to construct a SQL query that fulfills your request."
sf040,"Find the top 10 northernmost addresses in Florida's largest zip code area. What are their address numbers, street names, and types?",The LLM is not allowed to see the data in your database. Your question requires database introspection to generate the necessary SQL. Please set allow_llm_to_see_data=True to enable this.
sf009,"A real estate company needs a detailed side-by-side comparison of buildings in Amsterdam and Rotterdam. They require a report showing each building class and subclass, with the total surface area (in square meters) and the total number of buildings for each classification category in both cities. The data should be organized by building class and subclass in ascending order, with Amsterdam and Rotterdam statistics presented in parallel columns to facilitate direct comparison. Can you generate this comprehensive building classification comparison report?","The provided context is insufficient to generate the required SQL query. There are no tables containing information about buildings classified by city, building class, subclass, or surface area in square meters. Without the relevant data or relationships, it is not possible to create the requested report comparing buildings in Amsterdam and Rotterdam."
sf013,"Compare the total road lengths in Amsterdam and Rotterdam by creating a side-by-side analysis of both cities. For each combination of road class and subclass, calculate the total length of roads (in meters) specifically for QUADKEY segments '12020210' and '12020211'. Present the results with columns for class, subclass, Amsterdam's road lengths, and Rotterdam's road lengths.","The provided context is insufficient to generate a SQL query for comparing road lengths in Amsterdam and Rotterdam based on the provided tables. There are no tables or columns directly related to road lengths, road classes, subclasses, or specific city designations such as Amsterdam and Rotterdam. Additional data sources containing road information with geographic segmentation are required to fulfill the query's requirements."
sf041,"Produce a report for ERCOT on October 1, 2022, that combines hourly data on day-ahead and real-time prices from node ID 10000697078, load forecasts (datatypeid 19060) and actual loads, plus wind (forecast datatypeid 9285, actual datatypeid 16) and solar (forecast datatypeid 662, actual datatypeid 650) generation forecasts and actuals from object ID 10000712973. This report should include time zone alignments, peak classifications, and net load calculations, providing insights into daily operational dynamics and efficiency.","The provided context is insufficient to generate the requested SQL query. There are no tables or details in the schema related to ERCOT, node IDs, datatype IDs, object IDs, or any specifics about energy prices, load forecasts, wind, and solar generation data. Additionally, there's no information about time zone alignments, peak classifications, or net load calculations in the database schema provided."
sf011,"Determine the population distribution within each block group relative to its census tract in New York State using 2021 ACS data. Include block group ID, census value, state county tract ID, total tract population, and the population ratio of each block group.","The provided context is insufficient to generate the requested SQL query because there is no table containing information about block groups, census tracts, or ACS (American Community Survey) data specifically for New York State or any state. Furthermore, the necessary details to calculate population distribution relative to census tracts are missing. Please provide the relevant table structure or additional data sources that contain this information."
sf014,"What is the New York State ZIP code with the highest number of commuters traveling over one hour, according to 2021 ACS data? Include the zip code, the total commuters, state benchmark for this duration, and state population.","The provided context is insufficient to generate the SQL query as there is no table containing information about commuters, commuting duration, or specific 2021 ACS (American Community Survey) data related to New York State or any state benchmarks and populations. A suitable dataset or tables containing these specific details would be necessary to fulfill the query request."
