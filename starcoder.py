# -*- coding: utf-8 -*-
"""starcoder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dhpus9l2SwjQT1FZMOZJpokKqrgQXzS2
"""

!pip install transformers huggingface_hub pandas tqdm

from google.colab import drive
drive.mount('/content/drive')

GEO_FILE = "/content/drive/sample_data/path_to/cleaned_generated_sql_queries_geo_full_extracted.csv"
COSQL_FILE = "/content/drive/sample_data/path_to/cleaned_generated_sql_queries_cosql_full_extracted.csv"
OUTPUT_DIR = "/content/refined_queries"

token = ""

# --- Install required libraries ---
!pip install transformers huggingface_hub pandas tqdm

# --- Upload CSV Files ---
from google.colab import files
uploaded = files.upload()  # Upload your two CSVs when prompted

# --- HuggingFace Login ---
from huggingface_hub import login
from getpass import getpass

# Safer way to enter token
token = getpass("Enter your Hugging Face token: ")
login(token=token)

# --- Imports ---
import pandas as pd
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm
import os
import time

# --- Configuration ---
GEO_FILE = "cleaned_generated_sql_queries_geo_full_extracted.csv"
COSQL_FILE = "cleaned_generated_sql_queries_cosql_full_extracted.csv"
OUTPUT_DIR = "/content/refined_queries"
BATCH_SIZE = 5

ID_COLUMN = "instance_id"
NL_COLUMN = "nl_query"
SQL_COLUMN = "generated_sql"

os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- Load StarCoder Model ---
def load_quantized_starcoder():
    print("Loading StarCoder 1B model...")

    if torch.cuda.is_available():
        device = "cuda"
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        device = "mps"  # Apple Silicon
    else:
        device = "cpu"

    print(f"Using device: {device}")

    model = AutoModelForCausalLM.from_pretrained(
        "bigcode/starcoderbase-1b",
        device_map="auto" if device == "cuda" else None,
        trust_remote_code=True
    )

    if device != "cuda":
        model = model.to(device)

    tokenizer = AutoTokenizer.from_pretrained("bigcode/starcoderbase-1b")
    tokenizer.pad_token = tokenizer.eos_token

    return model, tokenizer, device

# --- Prompt Builder ---
def create_refinement_prompt(sql, nl_query, dataset_type, db=None, external_knowledge=None):
    nl_context = f"Natural language query: {nl_query}\n" if nl_query else ""
    db_context = f"Database: {db}\n" if db else ""
    external_context = f"External knowledge: {external_knowledge}\n" if external_knowledge else ""

    if dataset_type.lower() == "cosql":
        prompt = (
            f"# Refine the following SQL query from a conversational context\n"
            f"# Fix any syntax errors, optimize the query, and improve formatting\n\n"
            f"{nl_context}"
            f"Original query:\n"
            f"```sql\n{sql}\n```\n\n"
            f"Refined query:\n```sql\n"
        )
    elif dataset_type.lower() == "geo":
        prompt = (
            f"# Refine the following SQL query for a geographical database\n"
            f"# Fix any syntax errors, optimize the query, and improve formatting\n\n"
            f"{nl_context}"
            f"Original query:\n"
            f"```sql\n{sql}\n```\n\n"
            f"Refined query:\n```sql\n"
        )
    elif dataset_type.lower() == "spider_2":
        prompt = (
            f"# Refine the following SQL query using external knowledge and schema context\n"
            f"# Fix syntax errors and optimize for clarity and correctness\n\n"
            f"{db_context}{external_context}{nl_context}"
            f"Original query:\n"
            f"```sql\n{sql}\n```\n\n"
            f"Refined query:\n```sql\n"
        )
    elif dataset_type.lower() == "spider":
        prompt = (
            f"# Refine the following SQL query using knowledge of the database schema '{db}'\n"
            f"# Fix any syntax errors, optimize, and improve formatting\n\n"
            f"{nl_context}"
            f"Original query:\n"
            f"```sql\n{sql}\n```\n\n"
            f"Refined query:\n```sql\n"
        )
    else:
        prompt = (
            f"# Refine the following SQL query\n"
            f"# Fix any syntax errors, optimize the query, and improve formatting\n\n"
            f"{nl_context}"
            f"Original query:\n"
            f"```sql\n{sql}\n```\n\n"
            f"Refined query:\n```sql\n"
        )

    return prompt


# --- SQL Refinement ---
@torch.inference_mode()
def refine_query(model, tokenizer, sql, nl_query, dataset_type, device, db=None, external_knowledge=None):
    prompt = create_refinement_prompt(sql, nl_query, dataset_type, db=db, external_knowledge=external_knowledge)

    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=1024, return_attention_mask=True).to(device)

    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=200,
        temperature=0.2,
        top_p=0.95,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        num_return_sequences=1,
    )

    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    refined_query = generated_text.split("Refined query:\n```sql\n")[-1]
    if "```" in refined_query:
        refined_query = refined_query.split("```")[0].strip()

    return refined_query


# --- Process a CSV file ---
def process_file(file_path, model, tokenizer, device, dataset_type):
    if not os.path.exists(file_path):
        print(f"File not found: {file_path}")
        return None

    print(f"Loading {file_path}...")
    df = pd.read_csv(file_path)
    print(f"Loaded {len(df)} queries")

    required_columns = [ID_COLUMN, NL_COLUMN, SQL_COLUMN]
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        print(f"Error: Missing columns: {', '.join(missing_columns)}")
        print(f"Available columns: {', '.join(df.columns)}")
        return None

    print(f"Using fixed columns: ID={ID_COLUMN}, NL={NL_COLUMN}, SQL={SQL_COLUMN}")

    results = []
    processed = 0
    start_time = time.time()

    for i in tqdm(range(0, len(df), BATCH_SIZE)):
        batch = df.iloc[i:i+BATCH_SIZE]

        for _, row in batch.iterrows():
            sql = row[SQL_COLUMN]
            nl = row[NL_COLUMN]

            if not isinstance(sql, str) or sql.strip() == "":
                result_row = row.to_dict()
                result_row["refined_sql"] = ""
                results.append(result_row)
                continue

            try:
                refined_sql = refine_query(model, tokenizer, sql, nl, dataset_type, device)
                result_row = row.to_dict()
                result_row["refined_sql"] = refined_sql
                results.append(result_row)
                processed += 1

                if processed % 10 == 0:
                    avg_time = (time.time() - start_time) / processed
                    print(f"\nProcessed {processed} queries (avg {avg_time:.2f}s/query)")
                    print(f"Sample refinement:\nOriginal: {sql[:100]}...\nRefined: {refined_sql[:100]}...")

            except Exception as e:
                print(f"Error processing query: {e}")
                result_row = row.to_dict()
                result_row["refined_sql"] = f"ERROR: {str(e)}"
                results.append(result_row)

    results_df = pd.DataFrame(results)
    return results_df

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

def load_quantized_starcoder():
    print("Loading StarCoder 1B model...")

    if torch.cuda.is_available():
        device = "cuda"
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        device = "mps"
    else:
        device = "cpu"

    print(f"Using device: {device}")

    model = AutoModelForCausalLM.from_pretrained(
        "bigcode/starcoderbase-1b",
        device_map="auto" if device == "cuda" else None,
        trust_remote_code=True
    )

    if device != "cuda":
        model = model.to(device)

    tokenizer = AutoTokenizer.from_pretrained("bigcode/starcoderbase-1b")
    tokenizer.pad_token = tokenizer.eos_token

    return model, tokenizer, device

# Load the model now
model, tokenizer, device = load_quantized_starcoder()

def process_file(file_path, model, tokenizer, device, dataset_type, save_every=50, output_filename=None):
    if not os.path.exists(file_path):
        print(f"File not found: {file_path}")
        return None

    print(f"Loading {file_path}...")
    df = pd.read_csv(file_path)
    print(f"Loaded {len(df)} queries")

    required_columns = [ID_COLUMN, NL_COLUMN, SQL_COLUMN]
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        print(f"Error: Missing columns: {', '.join(missing_columns)}")
        print(f"Available columns: {', '.join(df.columns)}")
        return None

    print(f"Using fixed columns: ID={ID_COLUMN}, NL={NL_COLUMN}, SQL={SQL_COLUMN}")

    results = []
    processed = 0
    start_time = time.time()

    for i in tqdm(range(0, len(df), BATCH_SIZE)):
        batch = df.iloc[i:i + BATCH_SIZE]

        for _, row in batch.iterrows():
            sql = row[SQL_COLUMN]
            nl = row[NL_COLUMN]
            db = row.get("db", None)
            external_knowledge = row.get("external_knowledge", None)

            if not isinstance(sql, str) or sql.strip() == "":
                result_row = row.to_dict()
                result_row["refined_sql"] = ""
                results.append(result_row)
                continue

            try:
                refined_sql = refine_query(
                    model=model,
                    tokenizer=tokenizer,
                    sql=sql,
                    nl_query=nl,
                    dataset_type=dataset_type,
                    device=device,
                    db=db,
                    external_knowledge=external_knowledge
                )
                result_row = row.to_dict()
                result_row["refined_sql"] = refined_sql
                results.append(result_row)
                processed += 1

                if processed % 10 == 0:
                    avg_time = (time.time() - start_time) / processed
                    print(f"\nProcessed {processed} queries (avg {avg_time:.2f}s/query)")
                    print(f"Sample refinement:\nOriginal: {sql[:100]}...\nRefined: {refined_sql[:100]}...")

                # Save at 50, 100, etc. (overwrite same file)
                if processed % save_every == 0 and output_filename:
                    temp_df = pd.DataFrame(results)
                    temp_df.to_csv(output_filename, index=False)
                    print(f"✅ Auto-saved {processed} queries to {output_filename}")

            except Exception as e:
                print(f"Error processing query: {e}")
                result_row = row.to_dict()
                result_row["refined_sql"] = f"ERROR: {str(e)}"
                results.append(result_row)

    results_df = pd.DataFrame(results)

    if output_filename:
        results_df.to_csv(output_filename, index=False)
        print(f"✅ Final save completed to {output_filename}")

    return results_df

!pip install datasets

from datasets import load_dataset
import pandas as pd
from google.colab import files

# --- Step 1: Load Spider dataset from Hugging Face ---
spider_raw = load_dataset("xlangai/spider", split="train")
spider_df = spider_raw.to_pandas()

# Rename columns for consistency with your format
spider_df = spider_df.rename(columns={"question": "nl_query", "db_id": "db"})

# --- Step 2: Load your generated SQLs ---
generated_df = pd.read_csv("cleaned_generated_sql_queries_spider_full_extracted.csv")

# --- Step 3: Merge using 'nl_query' as join key ---
merged_df = pd.merge(generated_df, spider_df[["nl_query", "db"]], on="nl_query", how="left")

# --- Step 4: Show missing values ---
missing_count = merged_df["db"].isnull().sum()
print(f"⚠️ {missing_count} queries missing db info after merge.")

# --- Step 5: Save the merged output ---
output_file = "cleaned_generated_sql_queries_spider_full_merged.csv"
merged_df.to_csv(output_file, index=False)
print(f"✅ Merged file saved as: {output_file}")

# Preview the first few rows
display(merged_df.head())

# --- Step 6: Download the merged file ---
files.download(output_file)

import json
import pandas as pd
from google.colab import files

# --- Step 1: Read spider2-lite.jsonl and build mapping ---
spider_questions = {}

with open("spider2-lite.jsonl", "r") as f:
    for line in f:
        item = json.loads(line)
        spider_questions[item['instance_id']] = {
            "nl_query": item.get("question", ""),
            "db": item.get("db", ""),
            "external_knowledge": item.get("external_knowledge", "")
        }

print(f"✅ Loaded {len(spider_questions)} entries from spider2-lite.jsonl")

# --- Step 2: Load generated SQL file ---
generated_df = pd.read_csv("cleaned_generated_sql_queries_spider2.csv")

# --- Step 3: Map nl_query, db, external_knowledge into new columns ---
generated_df["nl_query"] = generated_df["instance_id"].map(lambda x: spider_questions.get(x, {}).get("nl_query", ""))
generated_df["db"] = generated_df["instance_id"].map(lambda x: spider_questions.get(x, {}).get("db", ""))
generated_df["external_knowledge"] = generated_df["instance_id"].map(lambda x: spider_questions.get(x, {}).get("external_knowledge", ""))

# --- Step 4: Handle any missing values (optional safety) ---
generated_df["nl_query"] = generated_df["nl_query"].fillna("")
generated_df["db"] = generated_df["db"].fillna("")
generated_df["external_knowledge"] = generated_df["external_knowledge"].fillna("")

# --- Step 5: Preview and save ---
print("✅ Preview of merged data:")
display(generated_df.head())

output_file = "cleaned_generated_sql_queries_spider2_merged.csv"
generated_df.to_csv(output_file, index=False)
print(f"✅ Saved merged file as {output_file}")

# --- Step 6: Download the file ---
files.download(output_file)

# from google.colab import files
# # --- Process Spider2 ---
# SPIDER2_FILE = "cleaned_generated_sql_queries_spider2_merged.csv"
# print("\n=== Processing Spider 2 Lite Dataset ===")
# spider2_output = os.path.join(OUTPUT_DIR, "refined_spider2_queries.csv")
# spider2_results = process_file(SPIDER2_FILE, model, tokenizer, device, "spider_2", save_every=50, output_filename=spider2_output)

# (Optional) Download at the end

files.download(spider2_output)
# --- Process Spider ---
SPIDER_FILE = "cleaned_generated_sql_queries_spider_full_merged.csv"
print("\n=== Processing Spider Dataset ===")
spider_output = os.path.join(OUTPUT_DIR, "refined_spider_queries.csv")
spider_results = process_file(SPIDER_FILE, model, tokenizer, device, "spider", save_every=50, output_filename=spider_output)

# --- Download outputs (at end) ---

files.download(spider_output)

spider_results.shape[0]
df = pd.read_csv("/content/refined_queries/refined_spider2_queries.csv")

# Show number of rows
print(f"📦 Number of rows: {len(df)}")

import os
import pandas as pd
import re

import re

def basic_clean_sql(query):
    if not isinstance(query, str):
        return query

    # Remove triple quotes
    query = query.replace("'''", "").replace('"""', '')

    # Remove full comment lines starting with -- or ##
    query = re.sub(r'(?m)^(--|##).*$', '', query)

    # Remove triple backticks ``` anywhere
    query = query.replace('```', '')

    # Remove single leading/trailing backticks if somehow left
    query = query.strip('`')

    # Remove leading/trailing extra whitespace
    query = query.strip()

    # Collapse multiple spaces into one
    query = re.sub(r'\s+', ' ', query)

    return query


# --- Make sure clean folder exists ---
clean_folder = "/content/clean"
os.makedirs(clean_folder, exist_ok=True)

# --- List of all your 4 files ---
input_files = [
    # ("/content/refined_cosql_queries (1).csv", "refined_cosql_queries_cleaned_basic.csv"),
    # ("/content/refined_geo_queries (1).csv", "refined_geo_queries_cleaned_basic.csv"),
  ("/content/refined_queries/refined_spider_queries.csv", "refined_spider_queries_cleaned_basic.csv")
    # ("/content/refined_queries/refined_spider2_queries.csv", "refined_spider2_queries_cleaned_basic.csv")
]


# --- Process each file ---
for input_path, output_filename in input_files:
    try:
        df = pd.read_csv(input_path)
        df['refined_sql_cleaned'] = df['refined_sql'].apply(basic_clean_sql)

        output_path = os.path.join(clean_folder, output_filename)
        df.to_csv(output_path, index=False)
        print(f"✅ Saved cleaned version to {output_path}")

    except Exception as e:
        print(f"⚠️ Error processing {input_path}: {e}")





